# -*- coding: utf-8 -*-
"""main_eeg_(2)_(3)_(2)_(1)_(2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ByU-nmwgKmn5UhQDnckyeUgf-0iNWjt1

# EEG MITACS Project 2022
Libraries and data
"""

from google.colab import drive
drive.mount("/content/gdrive/")

import numpy as np
import pandas as pd
#!pip install mne
import tensorflow as tf
import gc
import keras
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from keras.layers import LSTM, GRU, Flatten
!pip install mne
import mne



!pip install pyedflib
import numpy as np
import pandas as pd
import csv
import mne
import tensorflow as tf
import gc
import keras
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
# Importing dependencies numpy and keras
import numpy
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import LSTM
from keras.utils import np_utils
from sklearn.metrics import mean_absolute_error as mae
from sklearn.metrics import explained_variance_score
import numpy as np
import matplotlib.pyplot as plt
from os import listdir
import pyedflib
#from edf_preprocessing import PreProcessing
from os.path import isfile, join
import random
import os
import glob

# Importing dependencies numpy and keras
import numpy
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import LSTM, GRU, Flatten
from keras.utils import np_utils
from keras import backend as K
from sklearn.metrics import mean_absolute_error as mae
from sklearn.metrics import explained_variance_score
import numpy as np
import matplotlib.pyplot as plt

def read_edf(filepath):
    data = mne.io.read_raw_edf(filepath, exclude = ['A1', 'A2', 'AUX1', 
        'AUX2', 'AUX3', 'AUX4', 'AUX5', 'AUX6', 'AUX7', 'AUX8', 'Cz', 
        'DC1', 'DC2', 'DC3', 'DC4', 'DIF1', 'DIF2', 'DIF3', 'DIF4', 
        'ECG1', 'ECG2', 'EKG1', 'EKG2', 'EOG 1', 'EOG 2', 'EOG1', 'EOG2', 
        'Fp1', 'Fp2', 'Fpz', 'Fz', 'PG1', 'PG2', 'Patient Event', 'Photic', 
        'Pz', 'Trigger Event', 'X1', 'X2', 'aux1', 'phoic', 'photic'], verbose='warning', preload=True)
    
    target_channels = set(["FP1", "FPZ", "FP2", "F3", "F4", "F7", "F8", "FZ", "T3", "T4", "T5", "T6", "C3", "C4", "CZ", "P3", "P4", "PZ", "O1", "O2"])
    current_channels = set(data.ch_names)
    
    if (target_channels == current_channels) and not  np.any(np.isnan(np.array(data.get_data()))):
      print("done")
      return  data.get_data()
x=read_edf("/content/gdrive/MyDrive/data/00209732-e4f3-42b8-9758-2423605f1fbf.edf")

x[10]=[np.nan*x[10].shape]

"""# EEG_Slicing"""

import os
import pandas as pd
import numpy as np
filepath = "/home/vpa20/projects/rpp-doesburg/databases/eeg_fha/release_001/edf/Burnaby"
output_path = "data10sec/"
edf_names = os.listdir(filepath)

final_df = pd.DataFrame()
chunk_size = 500
count = 0
for i in range(len(edf_names)):
    file_name = edf_names[i]
    path = "/home/vpa20/projects/rpp-doesburg/databases/eeg_fha/release_001/edf/Burnaby/" + file_name
    print('file number:', i)
    try:
        df = process1(path)
        if str(type(df)) == "<class 'pandas.core.frame.DataFrame'>":
            final_df = pd.concat([final_df, df], axis=0, ignore_index=True)
            count += 1
    except:
        print('wrong file', i, 'error')
    if np.abs(count - i)!=0:
        print(i,"after error index")
        count=i

    if i % 100 == 0:
        print(i, 'EDF extracted')

    if i % chunk_size == 0 and i != 0:
        final_df = pd.concat([final_df, df], axis=0, ignore_index=True)
        final_df.to_csv(output_path + 'edf_data_10sec.csv')

        edf_npdata = np.stack(final_df['data'])
        np.save(output_path + 'edf_data_10sec_nparray', edf_npdata)

final_df.to_csv(output_path + 'edf_data_10sec.csv')

edf_npdata = np.stack(final_df['data'])
np.save(output_path + 'edf_data_10sec_nparray', edf_npdata)

"""# windowing"""

def dataset(xdata,ydata, frequency=5000):
  """
  input: x and y
  frequency: its window size
  output: return will be x and y with windowing
  """
  mydata=[]
  myydata=[]
  for sample in range(len(xdata)):
    start=0
    while(start+frequency)<=90000:
      mydata.append(xdata[sample][:,start:start+frequency])
      start=start+frequency
      #print(sample)
      myydata.append(ydata[sample])
  return mydata, myydata
def show_metrics(y_test, y_predict):
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()

n=int(input("enter no of samples, if LSTM,n<60 to avoid memory clash     >>"))

data_path=r"/content/gdrive/MyDrive/mitacs/edf_data_3min_nparray.npy"
xdata= np.load(data_path , allow_pickle= True)[0]

main_path=r"/content/gdrive/MyDrive/mitacs/age_ScanID.csv"
main_csv=pd.read_csv(main_path).iloc[:,[1,4,5]]
main_csv.head()
main_csv.columns= ['scan_id', 'AgeYears','AgeDays']
main_csv.head()



eeg3_csv=pd.read_csv(r"/content/gdrive/MyDrive/mitacs/edf_data_3min_final").iloc[:,[1,3]]
eeg3_csv.head()

#break

"""# train : x100,y100"""

y10=np.array(pd.merge(eeg3_csv,main_csv, on="scan_id",how="inner").iloc[:,2])[:n]
#y100=tf.convert_to_tensor(y10)
x1=xdata.tolist()[0:n]
#x2=tf.convert_to_tensor(x1)

x,y=dataset(x1,y10,10000)
del x1

"""
msg=1 #int(input("Are you testing PCA-NN model press eithe 1(y)/0(n)"))
if msg==1:
  check=pcaf(x)
  x100=tf.convert_to_tensor(check)
else:
  x100=x1

"""

break

"""# train test split

from sklearn.model_selection import train_test_split
xtrain1,xtest1,ytrain1,ytest1=train_test_split(x1,y10,test_size=0.2,shuffle=True)
xtrain=tf.convert_to_tensor(xtrain1)
xtest=tf.convert_to_tensor(xtest1)
ytrain=tf.convert_to_tensor(ytrain1)
ytest=tf.convert_to_tensor(ytest1)
"""

#break

"""# testing pca oncept start:"""

def pcaf(data,n_components=20):
  pca=PCA(n_components=1)
  out=[]
  for i in range(len(data)):
    pred=pca.fit_transform(data[i])
    out.append(pred)
  return out

"""from sklearn.decomposition import PCA
pca=PCA(n_components=3)
pred=pca.fit_transform(x[1])
out=pca.explained_variance_ratio_
plt.plot(out)
plt.grid()
plt.plot( np.cumsum(out))
plt.legend(["var_ratio","cum"])
print(np.cumsum(out))

#Test : xtest, ytest
"""

#ytest1=np.array(pd.merge(eeg3_csv,main_csv, on="scan_id",how="inner").iloc[:,2])[n:]
#ytest=tf.convert_to_tensor(ytest1)

#x1=xdata.tolist()
#x2=tf.convert_to_tensor(x1[n:])

#for pca only
"""
msg=int(input("Are you testing PCA-NN model press eithe 1(y)/0(n)"))
if msg==1:
  check=pcaf(x2)
  xtest=tf.convert_to_tensor(check)
else:
  xtest=x2
  """

#x1=np.transpose(x)
#x1

#len(x1)

"""# Memory health"""

del eeg3_csv 
del main_csv
# to free up memory

import time
time.sleep(15)

del xdata
gc.collect()
#time.sleep(15)

!pip install psutil
import os, psutil
process = psutil.Process(os.getpid())
print((process.memory_info().rss)/1024**2)

break

"""# BLSTM paper code : lstm-1"""

# defining the LSTM model
from keras.layers import Bidirectional
model = Sequential()
model.add(Bidirectional(LSTM(256,return_sequences=True), input_shape=(20,30000))) # 10 is ouput of lstm block and input is 20 timesteps with 90000 inpiut shape  
model.add(Dropout(0.2))
model.add(tf.keras.layers.BatchNormalization())
model.add(LSTM(128, return_sequences=True))
model.add(tf.keras.layers.BatchNormalization())
model.add(LSTM(64, return_sequences=False))
model.add(tf.keras.layers.BatchNormalization())
#model.add(LSTM(300))
#model.add(Dropout(0.2))
model.add(Dense(32, activation='relu'))
model.add(Dense(6, activation='relu'))
model.compile(loss="MeanAbsoluteError", optimizer='adam')
model.summary()

model.fit(tf.convert_to_tensor(xxx),tf.convert_to_tensor(y), steps_per_epoch=10 ,epochs=500, batch_size=n,use_multiprocessing=True  )

xxx=[]
xt=np.array(x)
for i in range(len(xt)):
  xxx.append(xt[i].T)
  #print(len(xxx[0]))

tf.convert_to_tensor(xxx).shape

ypred=model.predict(xtest)

ytest.shape

ypred1=tf.reshape(ypred,ypred.shape[0])
show_metrics(ytest, ypred1)
#ytest1.shape

ypred2=ypred1.numpy()
ypred2=np.insert(ypred2,0,-1)
ypred2

a=[1]
b

ypred1.numpy

from numpy import savetxt
from numpy import asarray
savetxt('/content/gdrive/MyDrive/saved_models/LSTM_nn/lstm_ytest.csv', asarray(ytest), delimiter
=',')
savetxt('/content/gdrive/MyDrive/saved_models/LSTM_nn/lstm_ypred.csv', asarray(ypred2), delimiter
=',')

pd.read_csv(r"/content/gdrive/MyDrive/saved_models/LSTM_nn/lstm_ypred.csv")

ypred1[0]

"""# LSTM-2

"""

# defining the LSTM model
model = Sequential()
model.add(LSTM(20, input_shape=(20,90000), return_sequences=True)) # 10 is ouput of lstm block and input is 20 timesteps with 90000 inpiut shape
model.add(LSTM(20, input_shape=(20,1), return_sequences=False))  
model.add(Dropout(0.5))
#model.add(LSTM(10, input_shape=(20,1), return_sequences=False))  
#model.add(LSTM(300))
#model.add(Dropout(0.2))
model.add(Dense(20, activation='relu'))
model.add(Dense(1, activation='relu'))

model.compile(loss="MeanSquaredError", optimizer='adam')
model.summary()

"""#lstm-3
https://github.com/xiangzhang1015/EEG-based-Control
"""

# defining the LSTM model
model = Sequential()
model.add(Dense(20,  input_shape=(20,90000),activation='relu'))
model.add(Dense(20,  activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(20,  activation='relu'))
model.add(Dense(20,  activation='relu'))
model.add(LSTM(20, input_shape=(20,20), return_sequences=True)) # 10 is ouput of lstm block and input is 20 timesteps with 90000 inpiut shape
model.add(LSTM(20, input_shape=(20,20), return_sequences=False))  
#model.add(Dropout(0.5))
#model.add(LSTM(10, input_shape=(20,1), return_sequences=False))  
#model.add(LSTM(300))
#model.add(Dense(20, activation='relu'))
model.add(Dense(1, activation='relu'))

model.compile(loss="MeanSquaredError", optimizer='adam')
model.summary()

"""#LSTM_4

"""

# defining the LSTM model
model = Sequential()

model.add(LSTM(20, input_shape=(20,90000), return_sequences=True)) # 10 is ouput of lstm block and input is 20 timesteps with 90000 inpiut shape
#model.add(LSTM(20, input_shape=(20,20), return_sequences=False))  
#model.add(Dropout(0.5))
#model.add(LSTM(10, input_shape=(20,1), return_sequences=False))  
#model.add(LSTM(300))
model.add(Dense(10, activation='relu'))
model.add(tf.keras.layers.GlobalAveragePooling1D(data_format="channels_first"))
model.add(Dense(1, activation='relu'))

model.compile(loss="MeanSquaredError", optimizer='adam')
model.summary()

"""#LSTM_5

"""

# defining the LSTM model
model = Sequential()

model.add(GRU(10, input_shape=(20,90000), return_sequences=True)) # 10 is ouput of lstm block and input is 20 timesteps with 90000 inpiut shape
#model.add(LSTM(20, input_shape=(20,20), return_sequences=False))  
#model.add(Dropout(0.5))
#model.add(LSTM(10, input_shape=(20,1), return_sequences=False))  
#model.add(LSTM(300))

model.add(Dense(10, activation='relu'))
model.add(GRU(10, return_sequences=True))
#model.add(tf.keras.layers.GlobalAveragePooling1D(data_format="channels_first"))
model.add(Flatten())
model.add(Dense(1, activation='relu'))

model.compile(loss="MeanSquaredError", optimizer='adam')
model.summary()

"""# LSTM-6"""

# defining the LSTM model
model = Sequential()

model.add(LSTM(20, input_shape=(20,90000), return_sequences=True)) # 10 is ouput of lstm block and input is 20 timesteps with 90000 inpiut shape
model.add(Dropout(0.5))
model.add(LSTM(20, input_shape=(20,20), return_sequences=False))  
model.add(Dropout(0.5))
#model.add(LSTM(10, input_shape=(20,1), return_sequences=False))  
#model.add(LSTM(300))
model.add(Dense(10, activation='relu'))
#model.add(tf.keras.layers.GlobalAveragePooling1D(data_format="channels_first"))
model.add(Dense(1, activation='relu'))

model.compile(loss="MeanSquaredError", optimizer='adam')
model.summary()

"""# LSTM -NN
train: x100,y100
<br> test :xtest, ytest
"""

newy=y100[:4]
new.shape

# defining the LSTM model
model = Sequential()
model.add(LSTM(10, input_shape=(20,5000), return_sequences=False)) # 10 is ouput of lstm block and input is 20 timesteps with 90000 inpiut shape  
#model.add(Dropout(0.2))
#model.add(LSTM(300))
#model.add(Dropout(0.2))
model.add(Dense(90, activation='relu'))
model.add(Dense(1, activation='relu'))

model.compile(loss="MeanAbsoluteError", optimizer='adam')
model.summary()
# fitting the model
model.fit(new, y1data, steps_per_epoch=10 ,epochs=500, batch_size=n,use_multiprocessing=True  )

pred=model.predict(x100)

from keras import backend as K
def layer_prediction(model,layer_index,xdata):
  get_3rd_layer_output = K.function([model.layers[0].input] ,[model.layers[0].output])
  return get_3rd_layer_output(xdata)

layerPred=layer_prediction(model,1,new)
len(layerPred[0][0])
#plt.plot(get_3rd_layer_output([new])[0][19])

import matplotlib.pyplot as plt
plt.subplots(1,3, figsize=(10,5))
plt.subplot(1,3,1)
plt.plot(y100)
plt.title("real data for 50 sample")
plt.subplot(1,3,2)
plt.plot(pred,"r")
plt.title("predicted data for 50 sample")
plt.subplot(1,3,3)
plt.plot(y100,".b")
plt.plot(pred,"*r")

print(pred.max())

break

"""# Model Buidilng - PCA_NeuralNetwork
data set is : x100,y100




"""

check=pcaf(x100)

check[0].shape

x100.shape

x10=tf.convert_to_tensor(check)
x10.shape

# defining the deep nn model

model = Sequential()
model.add(Dense(units=20,input_shape=(20,)))
#model.add(LSTM(10, input_shape=(20,90000), return_sequences=False)) #input(batch, timestep, faeture)
#model.add(Dropout(0.2))
#model.add(LSTM(300))
#model.add(Dropout(0.2))
model.add(Dense(50, activation='tanh'))
model.add(Dense(20, activation='relu'))
model.add(Dense(1, activation='relu'))

model.compile(loss="MeanSquaredError", optimizer=tf.keras.optimizers.Adam(learning_rate=0.03))
#model.summary()
# fitting the model
model.fit(x10, y100, steps_per_epoch=5 ,epochs=500, batch_size=n,use_multiprocessing=True  )

# fitting the model

history=model.fit(x10, y100, steps_per_epoch=5 ,epochs=500, batch_size=n,use_multiprocessing=True  )
history

"""result=[]
for i in np.arange(0.009,0.1,0.001):
  model.compile(loss="MeanSquaredError", optimizer=tf.keras.optimizers.Adam(learning_rate=i));
  out=model.fit(x10, y100, steps_per_epoch=5 ,epochs=500, batch_size=n,use_multiprocessing=True  );
  c=out.history["loss"][-1];
  result.append(c)

import matplotlib.pyplot as plt
plt.plot(np.arange(0.009,0.1,0.001),result)
plt.savefig("/content/gdrive/MyDrive/colabs/nn_3min")
"""

pred=model.predict(x10)
pred.shape

model.history?

import matplotlib.pyplot as plt
plt.plot(y100)
plt.plot(pred)
plt.legend(["real","pred"])

break

"""# Random_NN"""

from numpy.ma.core import outer

class random_nn :
  def __init__(self, xtrain, ytrain,xtest,ytest,epochs=1,steps_per_epoch=10,n_estimators=200):
    self.xtrain=xtrain
    self.ytrain=ytrain
    self.xtest=xtest
    self.ytest=ytest
    self.epochs=epochs
    self.steps_per_epoch=steps_per_epoch
    self.n_estimators=n_estimators

  def dmodel(self):
    model=keras.models.Sequential([
        keras.layers.Dense(20, input_shape=(20,90000), activation="relu"),   
        keras.layers.Flatten(),
        #keras.layers.Dense(30, activation="relu"),
        keras.layers.Dense(30, activation="relu"),
        #keras.layers.Dense(100, activation="relu"), 
        #keras.layers.Dense(50, activation="sigmoid"),
        #keras.layers.Reshape((10, 5))
        #keras.layers.Dense(1)
        keras.layers.Dense(1)])
    model.summary()
    model.compile(loss="MeanSquaredError", optimizer=tf.keras.optimizers.Adam(learning_rate=0.003))
    #model.summary()
    # fitting the model
    return model
  def plotting(self,pred,y,xlabel="Sample",ylabel="Days",sub="testing"):
    plt.figure()
    plt.plot(pred)
    plt.plot(y,"*g")
    plt.legend(["predicted","actual"])
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.title(sub)
    #plt.subplot()
    pass
  def root_mean_squared_error(self,x,y):
      return np.mean(np.square(x - y)) 
  def rmodel(self):
    from sklearn.ensemble import RandomForestRegressor
    m=RandomForestRegressor(n_estimators=self.n_estimators)
    return m
  def train(self):
    deepmodel=self.dmodel()
    modeld=deepmodel.fit(self.xtrain, self.ytrain, self.steps_per_epoch ,self.epochs,use_multiprocessing=True)
    pred1=deepmodel.predict(self.xtrain)
    self.ddmodel=deepmodel
    self.rmodel=self.rmodel().fit(pred1,self.ytrain)
    out1=self.rmodel.predict(pred1)
    acc=self.root_mean_squared_error(out1,self.ytrain)
    self.plotting(out1,self.ytrain)
    plt.figure()
    plt.plot(modeld.history["loss"])
    print(acc," training accuracy is ----------------------------@ ")
    return self.ddmodel,self.rmodel,acc
    
  def predictions(self):
    """dmodel  ka output alag hai, model sahi karo"""
    deepmodel,rmodel,acc=self.train()
    pred1=deepmodel.predict(self.xtest)
    pred=rmodel.predict(pred1)
    self.plotting(pred,self.ytest)
    acc=self.root_mean_squared_error(pred,self.ytest)
    print(acc," TESTING accuracy is ***********************************@ ")
    return acc
check=random_nn(x100,y100,x100,y100,epochs=3,steps_per_epoch=100,n_estimators=200)
check.predictions()

class random_nn :
    def __init__(self, xtrain, ytrain,xtest,ytest,epochs=1,steps_per_epoch=10,n_estimators=200):
        self.xtrain=xtrain
        self.ytrain=ytrain
        self.xtest=xtest
        self.ytest=ytest
        self.epochs=epochs
        self.steps_per_epoch=steps_per_epoch
        self.n_estimators=n_estimators
        self.x=None
        self.y=None
    def dmodel(self):
        model=keras.models.Sequential([
        keras.layers.Dense(20, input_shape=(20,90000), activation="relu"),   
        keras.layers.Flatten(),
        #keras.layers.Dense(30, activation="relu"),
        keras.layers.Dense(30, activation="relu"),
        #keras.layers.Dense(100, activation="relu"), 
        #keras.layers.Dense(50, activation="sigmoid"),
        #keras.layers.Reshape((10, 5))
        #keras.layers.Dense(1)
        keras.layers.Dense(1)])
        model.summary()
        model.compile(loss="MeanSquaredError", optimizer=tf.keras.optimizers.Adam(learning_rate=0.003))
        #model.summary()
        # fitting the model
        return model
    def plotting(self,pred,y,xlabel="Sample",ylabel="Days",sub="testing"):
        plt.figure()
        plt.plot(pred)
        plt.plot(y,"*g")
        plt.legend(["predicted","actual"])
        plt.xlabel(xlabel)
        plt.ylabel(ylabel)
        plt.title(sub)
        #plt.subplot()
        pass
    def root_mean_squared_error(self,x,y):
        return np.mean(np.square(x - y)) 
    def rmodel(self):
        from sklearn.ensemble import RandomForestRegressor
        m=RandomForestRegressor(n_estimators=self.n_estimators)
        return m
    def train(self):
        deepmodel=self.dmodel()
        modeld=deepmodel.fit(self.xtrain, self.ytrain, self.steps_per_epoch      ,self.epochs,use_multiprocessing=True)
        pred1=deepmodel.predict(self.xtrain)
        self.ddmodel=deepmodel
        self.rmodel=self.rmodel().fit(pred1,self.ytrain)
        out1=self.rmodel.predict(pred1)
        acc=self.root_mean_squared_error(out1,self.ytrain)
        self.plotting(out1,self.ytrain)
        plt.figure()
        plt.plot(modeld.history["loss"])
        print(acc," training accuracy is ----------------------------@ ")
        return self.ddmodel,self.rmodel,acc,modeld
    
    def predictions(self):
        """dmodel  ka output alag hai, model sahi karo"""
        deepmodel,rmodel,acc,modeld=self.train()
        x=self.xtest
        y=self.ytest
        pred1=deepmodel.predict(x)
        pred=rmodel.predict(pred1)
        self.plotting(pred,y)
        #acc=self.root_mean_squared_error(pred,self.ytest)
        #print(acc," TESTING accuracy is ***********************************@ ")
        return pred,modeld,deepmodel,rmodel
check=random_nn(x100,y100,x100,y100,epochs=3,steps_per_epoch=100,n_estimators=200)
pred,modeld,deepmodel,rmodel=check.predictions()
a=deepmodel.predict(x100)
out=rmodel.predict(a)
check.root_mean_squared_error(out,y100)

import keras
model = keras.models.Sequential([
        keras.layers.Dense(20, input_shape=(20,90000), activation="relu"),   
        keras.layers.Flatten(),
        #keras.layers.Dense(30, activation="relu"),
        keras.layers.Dense(30, activation="relu"),
        #keras.layers.Dense(100, activation="relu"), 
        #keras.layers.Dense(50, activation="sigmoid"),
        #keras.layers.Reshape((10, 5))
        #keras.layers.Dense(1)
        keras.layers.Dense(1)
])
model.summary()

model.compile(loss="MeanSquaredError", optimizer=tf.keras.optimizers.Adam(learning_rate=0.003))
#model.summary()
# fitting the model
out=model.fit(x100, y100, steps_per_epoch=5 ,epochs=100, batch_size=n,use_multiprocessing=True)
from sklearn.ensemble import RandomForestRegressor
m=RandomForestRegressor(n_estimators=200)
#

pred=model.predict(x100)

m.fit(pred,y100)
final=m.predict(pred)

pred.shape

plt.plot(final,"*r")
plt.plot(pred)
plt.plot(y100)
plt.legend(["random","nn","actual"])

"""# RF with mean enenrgy rms variance"""

def show_metrics(y_test, y_predict):
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()

final=[]
x1=x
for i in range(len(x1)):
  current=[]
  for j in range(len(x1[0])):
    current.append(np.mean(x1[i][j]))
    current.append(np.var(x1[i][j]))
    current.append((np.var(x1[i][j]))**(1/2))
    current.append(np.sum(np.power(x1[i][j],2)))
    current.append(np.mean(x1[i][j])*(1/np.sqrt(  len(x1[1][1])  )  ) )
    #print(current)
  final.append(current)
#del x1
from sklearn.model_selection import train_test_split
xtrain1,xtest1,ytrain1,ytest1=train_test_split(final,y,test_size=0.2,shuffle=True)
xtrain=tf.convert_to_tensor(xtrain1)
xtest=tf.convert_to_tensor(xtest1)
ytrain=tf.convert_to_tensor(ytrain1)
ytest=tf.convert_to_tensor(ytest1)

from sklearn.ensemble import RandomForestRegressor
model=RandomForestRegressor(n_estimators=120)
model.fit(xtrain,ytrain)
pred=model.predict(xtest)
plt.plot(ytest,pred,".")
show_metrics(ytest,pred)

plt.plot(ytest,".r")
plt.plot(pred,"*g")



"""#DEEP_NN"""

import keras
model = keras.models.Sequential([
        keras.layers.Dense(20, input_shape=(20,90000), activation="relu"),   
        keras.layers.Flatten(),
        #keras.layers.Dense(30, activation="relu"),
        keras.layers.Dense(40, activation="relu"),
        #keras.layers.Dense(100, activation="relu"), 
        #keras.layers.Dense(50, activation="sigmoid"),
        #keras.layers.Reshape((10, 5))
        #keras.layers.Dense(1)
      
])
model.summary()

model.compile(loss="MeanSquaredError", optimizer=tf.keras.optimizers.Adam(learning_rate=0.003))
#model.summary()
# fitting the model
model.fit(x100, y100, steps_per_epoch=5 ,epochs=500, batch_size=n,use_multiprocessing=True  )

pred=model.predict(x100)
y100.shape

import matplotlib.pyplot as plt
plt.plot(y100)
plt.plot(pred)
plt.legend(["real","pred"])

x100.shape

break

"""#MY Resnet"""

from keras import Sequential, Model, Input
from keras.layers import Dense
from tensorflow.keras import activations
model=Sequential
x=Input(shape=(20)) # put pca inputs here
y=Dense(20)(x)
y=Dense(40,activation="relu")(y+x)
y=Dense(1,activation="relu")(y)
model=Model(x,y)
model.summary()

model.compile(loss="MeanSquaredError", optimizer=tf.keras.optimizers.Adam(learning_rate=0.003))
#model.summary()
#fitting the model
check=pcaf(x100)
x100=tf.convert_to_tensor(check)
model.fit(x100, y100, steps_per_epoch=5 ,epochs=500, batch_size=n,use_multiprocessing=True  )

break

final=[]
x1=x
for i in range(len(x1)):
  current=[]
  for j in range(len(x1[0])):
    current.append(np.mean(x1[i][j]))
    current.append(np.var(x1[i][j]))
    current.append((np.var(x1[i][j]))**(1/2))
    current.append(np.sum(np.power(x1[i][j],2)))
    current.append(np.mean(x1[i][j])*(1/np.sqrt(  len(x1[1][1])  )  ) )
    #print(current)
  final.append(current)
#del x1

"""#channel wise analysis -LSTM"""

from keras.layers import Bidirectional
model = Sequential()
model.add(Bidirectional(LSTM(120,return_sequences=True), input_shape=(10000,1))) # 10 is ouput of lstm block and input is 20 timesteps with 90000 inpiut shape  
model.add(Dropout(0.2))
model.add(tf.keras.layers.BatchNormalization())
model.add(LSTM(60, return_sequences=True))
model.add(tf.keras.layers.BatchNormalization())
model.add(LSTM(20, return_sequences=False))
model.add(tf.keras.layers.BatchNormalization())
#model.add(LSTM(300))
#model.add(Dropout(0.2))
model.add(Dense(5, activation='relu'))
model.add(Dense(1, activation='relu'))
model.compile(loss="MeanAbsoluteError", optimizer='adam')
#model.summary()


channel=0

from sklearn.model_selection import train_test_split
xtrain1,xtest1,ytrain1,ytest1=train_test_split(np.array(x)[:,channel,:],y,test_size=0.2,shuffle=True)
xtrain=tf.convert_to_tensor(xtrain1)
xtest=tf.convert_to_tensor(xtest1)
ytrain=tf.convert_to_tensor(ytrain1)
ytest=tf.convert_to_tensor(ytest1)
model.fit(xtrain,ytrain, steps_per_epoch=1 ,epochs=100, batch_size=n,use_multiprocessing=True  )
ypred=model.predict(xtest)
ypred1=tf.reshape(ypred,ypred.shape[0])
show_metrics(ytest, ypred1)

np.array(x)[:,1,:].shape

import track_memory
from track_memory import track_memory_use, plot_memory_use

#np.array(x).shape
xtrain[0]

np.array(x)[:,19,:].shape

"""# model Save"""

path=r"/content/gdrive/MyDrive/saved_models/deepmodel/"  # where u want to save your model
deepmodel.save(path)

break

"""# Load Model"""

# path is where your model is saved 
path=r"/content/gdrive/MyDrive/saved_models/deepmodel"
m1=tf.keras.models.load_model(path)

pred=m1.predict(x100)
print(y100.shape,pred.shape)

plt.plot(ytest)
plt.plot(pred)
plt.legend(["real","pred"])

Pkl_Filename = "/content/gdrive/MyDrive/saved_models/random123"  
with open(Pkl_Filename, 'wb') as file:  
    pickle.dump(rmodel, file)

path=r"/content/gdrive/MyDrive/saved_models/random123"
with open(path, 'rb') as file:  
    Pickled_LR_Model = pickle.load(file)
Pickled_LR_Model.predict(a)

"""# 24 June code"""

import numpy as np
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.preprocessing import FunctionTransformer
x=[1,2,4]

def w(s):
  print("hii")
  x=1
  return np.sqrt(s)+1000000

def w1(s):
  print("swati")
  return s/12 #np.square(x)

#p2=make_pipeline((f.f1()))

transformer=FunctionTransformer(w)
transformer1=FunctionTransformer(w1)
p1=Pipeline(steps=[("transformer1",transformer1)])
p2=Pipeline(steps=[("transformer",transformer),("transformer1",transformer1)]) #("p1",p1)])
z=p2.transform(x)
z

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/MyDrive

from os import listdir
!pip install pyedflib
!pip install mne 
import mne
from edf_preprocessing import PreProcessing
from os.path import isfile, join
dataPath=r"/content/gdrive/MyDrive/data"
onlyfiles = [f for f in listdir(dataPath) ]  ## my datastore

import pandas as pd
df=pd.DataFrame({"a":[1,2]})

type(df)==("pandas.core.frame.DataFrame") #type(type(df))

#file_name = "81c0c60a-8fcc-4aae-beed-87931e582c45.edf"
#path = "/home/mykolakl/projects/rpp-doesburg/databases/eeg_fha/release_001/edf/Burnaby/" + file_name
#output_path = "your_folder"

# Initiate the preprocessing object, resample and filter the data
def process1(path,file_name,output_path="/content/sample_data"):
  p = PreProcessing(path)
  # This calls internal functions to detect 'bad intervals' and define 5 'good' ones 60 seconds each
  p.extract_good(target_length=60, target_slices=5)

  # Calling the function saves new EDF files to output_folder. In case there are more than 1, it adds suffix "_n" to the file name 
  p.save_clean_part(folder=output_path, filename=file_name)

  # Extract and convert data to Numpy arrays
  p.create_intervals_data()
  df = p.intervals_df
  return df
finallist=[]

out=process1(path)
if type(out)=="pandas.core.frame.DataFrame":
  finallist.append(out)
else:
  continue


path="/content/gdrive/MyDrive/data/"
for file1 in onlyfiles:
  #print(file1)
  process1(path=path+onlyfiles[1],file_name=onlyfiles[1])

try:
  #df_new=pd.DataFrame({"a":["acb"]})
  df_new=None
  print(type(df_new)=='pandas.core.frame.DataFrame')
  df = pd.concat([df,df_new], axis=0, ignore_index=True)
  print(df)
  #df.concat([123])
except:
  print("hii")

onlyfiles

#xx=mne.io.read_raw_edf(r"/content/sample_data/00209732-e4f3-42b8-9758-2423605f1fbf.edf")
#x=xx.get_data()

sample=r"/content/sample_data"
mnefile=[i for i in listdir(sample) if(i[-3:])=="edf"]
mnefile

main_path=(r"/content/gdrive/MyDrive/data/age_ScanID.csv")
main_csv=pd.read_csv(main_path).iloc[:,[1,4,5]]
main_csv.head()
main_csv.columns= ['scan_id', 'AgeYears','AgeDays']
main_csv.head()

finalarray=[]
finaly=[]
for i in mnefile:
  d1=mne.io.read_raw_edf(i) #.get_data()
  #plt.plot(d1[19])
  finalarray.append(d1.get_data())

  id=d1.filenames[0][-40:-4]
  #print(d1.info)
  id1=pd.DataFrame([id],columns=["scan_id"])
  age=pd.merge(id1,main_csv,how="inner").iloc[:,1].tolist()[0]
  finaly.append(age)

len(finalarray) #[0].shape
#finalarray[0].shap

"""
y==finaly
x=finalarray

"""

finalarray[1].shape

from keras.layers import Bidirectional
model = Sequential()
model.add(Bidirectional(LSTM(256,return_sequences=True), input_shape=(20,30500))) # 10 is ouput of lstm block and input is 20 timesteps with 90000 inpiut shape  
model.add(Dropout(0.2))
model.add(tf.keras.layers.BatchNormalization())
model.add(LSTM(128, return_sequences=True))
model.add(tf.keras.layers.BatchNormalization())
model.add(LSTM(64, return_sequences=False))
model.add(tf.keras.layers.BatchNormalization())
#model.add(LSTM(300))
#model.add(Dropout(0.2))
model.add(Dense(32, activation='relu'))
model.add(Dense(6, activation='relu'))
model.compile(loss="MeanAbsoluteError", optimizer='adam')
model.summary()

model.fit(tf.convert_to_tensor(finalarray),tf.convert_to_tensor(finaly),steps_per_epoch=1 ,epochs=5)

path=r"/content/gdrive/MyDrive/saved_models/deepmodel"  # where u want to save your model
model.save(path)

# path is where your model is saved 
path=r"/content/gdrive/MyDrive/saved_models/deepmodel"
m1=tf.keras.models.load_model(path)

m1.fit(tf.convert_to_tensor(finalarray),tf.convert_to_tensor(finaly),steps_per_epoch=1 ,epochs=5)

m1.fit(tf.convert_to_tensor(finalarray),tf.convert_to_tensor(finaly),steps_per_epoch=1 ,epochs=5)

m1.fit(tf.convert_to_tensor(finalarray),tf.convert_to_tensor(finaly),steps_per_epoch=1 ,epochs=5)

m1.fit(tf.convert_to_tensor(finalarray),tf.convert_to_tensor(finaly),steps_per_epoch=1 ,epochs=5)

import random
r=random.sample(range(0,700),50)

n=189
c=np.int(n*0.08)

overall=np.arange(0,701,1)
test=[i for i in overall if(i not in r)]
test

len(r)

r1=np.random.shuffle(r)
r[:10]

import glob
p1=r"/content/sample_data/"
files = os.listdir(r"/content/sample_data")
for f in files:
    os.remove(os.path.join(p1,f))
    #print(f)

n=700
a=[1]
b=[11,2,3,4]
a.extend(b)
a

#random.sample(overall.tolist(),10)
x=random.sample(overall.tolist(),np.int(n*0.80))
len(x)

#type(overall)
#overall.tolist()
train_index

np.array(random.sample(overall.tolist(),int(n*0.80)))

a=[1,2,3]
for i in a:
  if i==2:
    continue
  print(i)

"""#Dask"""

from google.colab import drive
drive.mount("/content/gdrive/")

cd /content/gdrive/MyDrive

!pip install dask
import dask as dd
from os import listdir
!pip install pyedflib
!pip install mne 
import mne
!pip install dask[dataframe]
from edf_preprocessing import PreProcessing
from os.path import isfile, join
dataPath=r"/content/gdrive/MyDrive/data"
onlyfiles = [f for f in listdir(dataPath) ]  ## my datastore

data=r"/content/gdrive/MyDrive/00209732-e4f3-42b8-9758-2423605f1fbf.edf"
eeg=mne.io.read_raw_edf(data)

ch=set(eeg.ch_names)
ch1=set(["FP1", "FPZ", "FP2", "F3", "F4", "F7", "F8", "FZ", "T3", "T4", "T5", "T6", "C3", "C4", "CZ", "P3", "P4", "PZ", "O1", "O2"])
x=(ch1==ch)
x

pwd

import dask
import dask.array as darray
x=dask.array.from_array([1,2,3])
x

a=[[1,2,3],[22,33,44],[33,44,55]]
a1=dask.array.from_array(a)
a1

del a
a1

l=a1[:,1].compute()
#type(l)
l

!pip install dask
#import dask.dataframe as dd
!pip install dask[dataframe]

path=r"/content/gdrive/MyDrive/data"

from dask import dataframe
import pandas as pd

!pip install "dask[complete]"
import dask
from dask import dataframe
import dask.dataframe as dd

cd "/content/gdrive/MyDrive"

"""#dask dataframe"""

!pip install "dask[complete]"
import dask
from dask import dataframe
from dask.dataframe import DataFrame as dff

from edf_preprocessing import PreProcessing

path=r"/content/gdrive/MyDrive/data/00282340-afbc-4c4e-8c07-4d489c3a3edd.edf"
# Initiate the preprocessing object, resample and filter the data
p = PreProcessing(path)

# This calls internal functions to detect 'bad intervals' and define 5 'good' ones 60 seconds each
p.extract_good(target_length=60, target_slices=5)

# Calling the function saves new EDF files to output_folder. In case there are more than 1, it adds suffix "_n" to the file name 
#p.save_clean_part(folder=output_folder, filename=file_name)

# Extract and convert data to Numpy arrays
p.create_intervals_data()
df = p.intervals_df
df

df.iloc[3,-1].shape

a=[1,2,3]
a.reverse()
for i in a:
  print(i)

df

import dask
from dask.dataframe import from_pandas
d22=dask.dataframe.from_pandas(df,chunksize=1)
d22

d22.loc[4,:].compute()

d23=dask.dataframe.concat([d22,d22])
d23.compute()

"""age collection"""

import pandas as pd
pdd=pd.read_csv(r"/content/gdrive/MyDrive/data/age_ScanID.csv").iloc[:,[1,4,5]]
pdd.columns= ['scan_id', 'AgeYears','AgeDays']
pdd  #df
pd.merge(pdd,df,how="inner")

id=dask.dataframe.read_csv(r"/content/gdrive/MyDrive/data/age_ScanID.csv",assume_missing=True).iloc[:,[1,4,5]]
id.columns= ['scan_id', 'AgeYears','AgeDays']
id.compute()

age=dataframe.DataFrame.merge(id,d23,how="inner")
age.compute()
# reindexing dataframe

age1=age.copy()
age1.reset_index()

age1=age1.set_index(age1.scan_id)

age1

a=age1.compute().iloc[1,-1]#.head()
len(a)
a

x=age["data"]
y=age["AgeYears"]
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout
tf.keras.backend.set_floatx('float32')

class KerasWrapper:
    def __init__(self, model ):
        self.model = model
        #self.feat_mean = feat_mean
        #self.feat_std = feat_std
        
    def predict_proba(self, X):
        
        preds = self.model.predict((X - self.feat_mean)/self.feat_std)
        return np.c_[preds, preds]
        
# using a function so we can track memory usage
#@track_memory_use(close=False, return_history=True)
def dask_read_and_incrementally_fit_keras(blocksize):
    
    # reading df with dask
    #df_train = dd.read_csv('./train.csv', blocksize=blocksize)
    df_train=age
    
    # creating keras model
    from keras.layers import Bidirectional
    model = Sequential()
    model.add(Bidirectional(LSTM(256,return_sequences=True), input_shape=(20,30000))) # 10 is ouput of lstm block and input is 20 timesteps with 90000 inpiut shape  
    model.add(Dropout(0.2))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(LSTM(128, return_sequences=True))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(LSTM(64, return_sequences=False))
    model.add(tf.keras.layers.BatchNormalization())
    #model.add(LSTM(300))
    #model.add(Dropout(0.2))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(6, activation='relu'))
    model.compile(loss="MeanAbsoluteError", optimizer='adam')
    #model.summary()

    
    # getting mean and std for dataset to normalize features
    #feat_mean = df_train.drop('isFraud', axis=1).mean().compute(scheduler='synchronous')
    #feat_std = df_train.drop('isFraud', axis=1).std().compute(scheduler='synchronous')
    
    # loop for number of partitions
    for i in range(df_train.npartitions):
        print("hii,..........................................00",i)
        # getting one partition
        part = df_train.get_partition(i).compute(scheduler='synchronous')
        print(part)
        # splitting
        #X_part = (part.drop('isFraud', axis=1) - feat_mean)/feat_std
        #y_part = part['isFraud']
        #print(np.array(part["data"]))
        X_part=tf.convert_to_tensor(np.array(part["data"])[0] )
        X_part_1=tf.expand_dims(X_part,axis=0)
        print(X_part_1.shape,X_part_1.dtype)
        y_part=tf.convert_to_tensor(part["AgeYears"])
        
        if i==3:
          print(part["AgeYears"])
          print(X_part_1,y_part)
        
        model.fit(X_part_1, y_part) 
        
    return KerasWrapper(model)

model = dask_read_and_incrementally_fit_keras(blocksize=5e6)

import joblib 
import dask.distributed
c=dask.distributed.Client()
c

set([6,1,2,3])==set([3,2,1])

data=mne.io.read_raw_edf("/content/gdrive/MyDrive/data/00282340-afbc-4c4e-8c07-4d489c3a3edd.edf",verbose='warning', preload=True)

main_path=r"/content/gdrive/MyDrive/mitacs/age_ScanID.csv"
main_csv=pd.read_csv(main_path).iloc[:,[1,4,5]]
main_csv.head()
main_csv.columns= ['scan_id', 'AgeYears','AgeDays']
main_csv.head()
x=data.get_data()
id=data.filenames[0][-40:-4]
id1=pd.DataFrame([id],columns=["scan_id"])
age=pd.merge(id1,main_csv,how="inner").iloc[:,1].tolist()[0]

xfinal=[]
yfinal=[]

xfinal.append(x)

len(xfinal)

#len(xfinal)
np.stack(np.array(xfinal),axis=0).shape

xfinal=np.stack([xfinal,np.expand_dims(x,axis=0)],axis=1)

np.asarray(xfinal,dtype=np.float32)

np.expand_dims(x,axis=0).shape

"""## Python RNN- 10 sec 100HZ PKL fresh model"""

from google.colab import drive
drive.mount("/content/gdrive/")
#task-1 libraries import
#!update-alternatives --install /usr/bin/python python /usr/bin/python3.8 1
print("Update is :             ---------------------------------LIbraries is importing")
import numpy as np
#!pip install --upgrade pandas==1.4.1
import pandas as pd
import csv
#import mne
import numpy
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import LSTM, GRU, Flatten
from keras.utils import np_utils
from keras import backend as K
from sklearn.metrics import mean_absolute_error as mae
from sklearn.metrics import explained_variance_score
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import gc
import keras
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
# Importing dependencies numpy and keras
import numpy
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import LSTM, Conv1D
from keras.utils import np_utils
from sklearn.metrics import mean_absolute_error as mae
from sklearn.metrics import explained_variance_score
import numpy as np
import matplotlib.pyplot as plt
#import pickle
!pip3 install pickle5
import pickle5 as pickle
#from edf_preprocessing import PreProcessing
# data path

def show_metrics(y_test, y_predict):
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import LSTM, Conv1D, MaxPool1D,AveragePooling1D, Conv3D, Conv2D
from keras.utils import np_utils
from sklearn.metrics import mean_absolute_error as mae
from sklearn.metrics import explained_variance_score
from keras import Input

def layerscheck(model):
  from keras import backend as K
  inp = model.input                                           # input placeholder
  outputs = [layer.output for layer in model.layers]          # all layer outputs
  functors = [K.function([inp], [out]) for out in outputs]    # evaluation functions
  layer_outs = [func([np.expand_dims(xtest[0],0)]) for func in functors]
  print(model.summary())
  return layer_outs

ypath=r"/content/gdrive/MyDrive/data/edf_data_10sec_50HZ_y.npy"
xpath=r"/content/gdrive/MyDrive/data/edf_data_10sec_50HZ_x.npy"
xdata=np.load(xpath,allow_pickle=True).tolist()
ydata=np.load(ypath,allow_pickle=True).tolist()

from sklearn.model_selection import train_test_split
# xtrain,xval,ytrain,yval=train_test_split(xdata,ydata,shuffle=True,train_size=0.8)
# xtest,xval,ytest,yval=train_test_split(xval,yval,shuffle=True,train_size=0.5)

xtrain,xtest,ytrain,ytest=train_test_split(np.transpose(xdata,(0,2,1)),ydata,shuffle=True,train_size=0.8) #np.transpose(x,(0,2,1))
    
# main_path=r"/content/gdrive/MyDrive/mitacs/age_ScanID.csv"
# age_ScanID=pd.read_csv(main_path).iloc[:,[1,4,5]]
# age_ScanID.columns= ['scan_id', 'AgeYears','AgeDays']

# y=np.array(pd.merge(data,age_ScanID,how="inner").loc[:,"AgeYears"]) 
# ydata=numpy.nan_to_num(y,nan=np.nanmean(y))
# # np.argwhere(np.isnan(ydata1))
# #xdata,ydata
# #x0,y0=dataset(xdata,ydata)

# xdata_T=[]
# #xt=np.array(x0)
# for i in range(len(xdata)):
#   xdata_T.append(xdata[i].T)

# print(" train test data split is 80-20 % ----------------------------------------")
# xtrain1,xtest1, ytrain1, ytest1= train_test_split(xdata.tolist(),ydata, test_size=0.20, shuffle=False)
xtrain=tf.convert_to_tensor(xtrain)
xtest=tf.convert_to_tensor(xtest)
#yval=tf.convert_to_tensor(yval)
#xtest=tf.convert_to_tensor(xtest)
ytrain=tf.convert_to_tensor(ytrain)
ytest=tf.convert_to_tensor(ytest)
print("Update is :             --------------------------------- Model Building")
del xdata
# del xdata
del ydata
# del data
# del age_ScanID
gc.collect()
print("xtrain shape is : ", xtrain.shape)

"""## CNN model with 1d conv2d for individual channesl"""

#a1
model2 = Sequential()
model2.add(Conv1D(4000,2))
model2.add(AveragePooling1D(2))
model2.add(Conv1D(1500,3))
model2.add(MaxPool1D(2))
model2.add(Conv1D(800,3))
model2.add(AveragePooling1D(2))
model2.add(Conv1D(500,3))
model2.add(AveragePooling1D(3))
#model2.add(Conv1D(50,2))
#model2.add(MaxPool1D(2))
#model2.add(Conv1D(400,2))
#model2.add(Conv1D(200,2))
#model2.add(Conv1D(100,2))
#model2.add(Conv1D(500,2))
#print()
model2.add(Flatten())
#model2.add(Bidirectional(LSTM(700,return_sequences=False), input_shape=(350,20))) # 10 is ouput of lstm block and input is 20 timesteps with 90000 inpiut shape  
#model2.add(Dropout(0.2))
#model2.add(tf.keras.layers.BatchNormalization())
#model2.add(LSTM(700, return_sequences=False))
#model2.add(tf.keras.layers.BatchNormalization())
#model2.add(LSTM(425, return_sequences=False))
#model2.add(Dropout(0.2))
#model2.add(tf.keras.layers.BatchNormalization())
#model.add(LSTM(300))
#model.add(Dropout(0.2))
#model2.add(Dense(750, activation='relu'))
#model2.add(Dense(35, activation='relu'))
#model2.add(Dropout(0.2))
#model2.add(Dense(200, activation='relu'))
#model2.add(Dense(100, activation='relu'))
model2.add(Dense(100, activation='relu'))
model2.add(Dense(100, activation='relu'))
model2.add(Dense(1, activation='relu'))
model2.compile(loss="MeanAbsoluteError", optimizer='adam')
tf.keras.optimizers.Adam(learning_rate=0.001 )#, beta_1=0.09, beta_2=0.009, epsilon=0.01, decay=0.001, amsgrad=False)
#model.compile(loss="MeanAbsoluteError", optimizer='adam')

from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='loss', mode='auto', verbose=1,min_delta=0.001, patience=5)
hist=model2.fit(xtrain,ytrain, steps_per_epoch=5,validation_split=0.3 ,epochs=50,batch_size=64,use_multiprocessing=True,callbacks=[es]  )

pred=model2.predict(xtest)
ypred1=tf.reshape(pred,pred.shape[0])

show_metrics(ytest, ypred1)
model2.build(input_shape=(38,350,20))
model2.summary()
plt.plot(hist.history["val_loss"])
plt.plot(hist.history["loss"])
plt.legend(["va loss"," training loss"])

#a1
model2 = Sequential()
model2.add(Conv1D(4000,2))
model2.add(AveragePooling1D(2))
model2.add(Conv1D(1500,3))
model2.add(MaxPool1D(2))
model2.add(Conv1D(800,3))
model2.add(AveragePooling1D(2))
model2.add(Conv1D(500,3))
model2.add(AveragePooling1D(3))
#model2.add(Conv1D(50,2))
#model2.add(MaxPool1D(2))
#model2.add(Conv1D(400,2))
#model2.add(Conv1D(200,2))
#model2.add(Conv1D(100,2))
#model2.add(Conv1D(500,2))
#print()
model2.add(Flatten())
#model2.add(Bidirectional(LSTM(700,return_sequences=False), input_shape=(350,20))) # 10 is ouput of lstm block and input is 20 timesteps with 90000 inpiut shape  
#model2.add(Dropout(0.2))
#model2.add(tf.keras.layers.BatchNormalization())
#model2.add(LSTM(700, return_sequences=False))
#model2.add(tf.keras.layers.BatchNormalization())
#model2.add(LSTM(425, return_sequences=False))
#model2.add(Dropout(0.2))
#model2.add(tf.keras.layers.BatchNormalization())
#model.add(LSTM(300))
#model.add(Dropout(0.2))
#model2.add(Dense(750, activation='relu'))
#model2.add(Dense(35, activation='relu'))
#model2.add(Dropout(0.2))
#model2.add(Dense(200, activation='relu'))
#model2.add(Dense(200, activation='relu'))
model2.add(Dense(200, activation='relu'))
model2.add(Dense(100, activation='relu'))
model2.add(Dense(1, activation='relu'))
model2.compile(loss="MeanAbsoluteError", optimizer='adam')
tf.keras.optimizers.Adam(learning_rate=0.001 )#, beta_1=0.09, beta_2=0.009, epsilon=0.01, decay=0.001, amsgrad=False)
#model.compile(loss="MeanAbsoluteError", optimizer='adam')

from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='loss', mode='auto', verbose=1,min_delta=0.001, patience=5)
hist=model2.fit(xtrain,ytrain, steps_per_epoch=5,validation_split=0.3 ,epochs=50,batch_size=64,use_multiprocessing=True,callbacks=[es]  )

pred=model2.predict(xtest)
ypred1=tf.reshape(pred,pred.shape[0])

show_metrics(ytest, ypred1)
model2.build(input_shape=(38,350,20))
model2.summary()
plt.plot(hist.history["val_loss"])
plt.plot(hist.history["loss"])
plt.legend(["va loss"," training loss"])

#a1
model2 = Sequential()
model2.add(Conv1D(4000,2))
model2.add(AveragePooling1D(2))
model2.add(Conv1D(2000,2))
model2.add(MaxPool1D(2))
model2.add(Conv1D(1000,3))
model2.add(AveragePooling1D(3))
model2.add(Conv1D(500,3))
model2.add(AveragePooling1D(3))
model2.add(Conv1D(200,3))
model2.add(AveragePooling1D(3))
#model2.add(Conv1D(50,2))
#model2.add(MaxPool1D(2))
#model2.add(Conv1D(400,2))
#model2.add(Conv1D(200,2))
#model2.add(Conv1D(100,2))
#model2.add(Conv1D(500,2))
#print()
model2.add(Flatten())
#model2.add(Bidirectional(LSTM(700,return_sequences=False), input_shape=(350,20))) # 10 is ouput of lstm block and input is 20 timesteps with 90000 inpiut shape  
#model2.add(Dropout(0.2))
#model2.add(tf.keras.layers.BatchNormalization())
#model2.add(LSTM(700, return_sequences=False))
#model2.add(tf.keras.layers.BatchNormalization())
#model2.add(LSTM(425, return_sequences=False))
#model2.add(Dropout(0.2))
#model2.add(tf.keras.layers.BatchNormalization())
#model.add(LSTM(300))
#model.add(Dropout(0.2))
#model2.add(Dense(750, activation='relu'))
#model2.add(Dense(35, activation='relu'))
#model2.add(Dropout(0.2))
#model2.add(Dense(200, activation='relu'))
#model2.add(Dense(100, activation='relu'))
model2.add(Dense(100, activation='relu'))
model2.add(Dense(100, activation='relu'))
model2.add(Dense(1, activation='relu'))
model2.compile(loss="MeanAbsoluteError", optimizer='adam')
tf.keras.optimizers.Adam(learning_rate=0.001 )#, beta_1=0.09, beta_2=0.009, epsilon=0.01, decay=0.001, amsgrad=False)
#model.compile(loss="MeanAbsoluteError", optimizer='adam')

from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='loss', mode='auto', verbose=1,min_delta=0.001, patience=5)
hist=model2.fit(xtrain,ytrain, steps_per_epoch=5,validation_split=0.3 ,epochs=50,batch_size=64,use_multiprocessing=True,callbacks=[es]  )

pred=model2.predict(xtest)
ypred1=tf.reshape(pred,pred.shape[0])

show_metrics(ytest, ypred1)
model2.build(input_shape=(38,350,20))
model2.summary()
plt.plot(hist.history["val_loss"])
plt.plot(hist.history["loss"])
plt.legend(["va loss"," training loss"])

model2 = Sequential()
model2.add(Conv1D(4000,2))
model2.add(MaxPool1D(2))
#model2.add(Conv1D(200,3))
model2.add(Conv1D(1500,3))
model2.add(MaxPool1D(2))
model2.add(Conv1D(800,3))
model2.add(AveragePooling1D(3))
model2.add(Conv1D(500,3))
model2.add(AveragePooling1D(3))
#model2.add(Conv1D(100,2))
#print()
model2.add(Flatten())
#model2.add(Bidirectional(LSTM(700,return_sequences=False), input_shape=(350,20))) # 10 is ouput of lstm block and input is 20 timesteps with 90000 inpiut shape  
#model2.add(Dropout(0.2))
#model2.add(tf.keras.layers.BatchNormalization())
#model2.add(LSTM(700, return_sequences=False))
#model2.add(tf.keras.layers.BatchNormalization())
#model2.add(LSTM(425, return_sequences=False))
#model2.add(Dropout(0.2))
#model2.add(tf.keras.layers.BatchNormalization())
#model.add(LSTM(300))
#model.add(Dropout(0.2))
#model2.add(Dense(750, activation='relu'))
#model2.add(Dense(35, activation='relu'))
#model2.add(Dropout(0.2))
#model2.add(Dense(350, activation='relu'))
#model2.add(Dense(350, activation='relu'))
#model2.add(Dense(100, activation='relu'))
model2.add(Dense(100, activation='relu'))
model2.add(Dense(10, activation='relu'))
model2.add(Dense(1, activation='relu'))
model2.compile(loss="MeanAbsoluteError", optimizer='adam')
tf.keras.optimizers.Adam(learning_rate=0.001 )#, beta_1=0.09, beta_2=0.009, epsilon=0.01, decay=0.001, amsgrad=False)
#model.compile(loss="MeanAbsoluteError", optimizer='adam')

from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='loss', mode='auto', verbose=0,min_delta=0.001, patience=5)
hist=model2.fit(xtrain,ytrain, steps_per_epoch=5,validation_split=0.3 ,epochs=50,batch_size=64,use_multiprocessing=True,callbacks=[es]  )

pred=model2.predict(xtest)
ypred1=tf.reshape(pred,pred.shape[0])

show_metrics(ytest, ypred1)
model2.build(input_shape=(38,350,20))
model2.summary()
plt.plot(hist.history["val_loss"])
plt.plot(hist.history["loss"])
plt.legend(["va loss"," training loss"])

model2 = Sequential()
model2.add(Conv1D(3000,2))
model2.add(MaxPool1D(2))
#model2.add(Conv1D(200,3))
model2.add(Conv1D(800,2))
model2.add(MaxPool1D(2))
model2.add(Conv1D(500,2))
model2.add(AveragePooling1D(2))
#model2.add(Conv1D(100,2))
#print()
model2.add(Flatten())
#model2.add(Bidirectional(LSTM(700,return_sequences=False), input_shape=(350,20))) # 10 is ouput of lstm block and input is 20 timesteps with 90000 inpiut shape  
#model2.add(Dropout(0.2))
#model2.add(tf.keras.layers.BatchNormalization())
#model2.add(LSTM(700, return_sequences=False))
#model2.add(tf.keras.layers.BatchNormalization())
#model2.add(LSTM(425, return_sequences=False))
#model2.add(Dropout(0.2))
#model2.add(tf.keras.layers.BatchNormalization())
#model.add(LSTM(300))
#model.add(Dropout(0.2))
#model2.add(Dense(750, activation='relu'))
#model2.add(Dense(35, activation='relu'))
#model2.add(Dropout(0.2))
#model2.add(Dense(350, activation='relu'))
#model2.add(Dense(350, activation='relu'))
#model2.add(Dense(100, activation='relu'))
model2.add(Dense(100, activation='relu'))
model2.add(Dense(10, activation='relu'))
model2.add(Dense(1, activation='relu'))
model2.compile(loss="MeanAbsoluteError", optimizer='adam')
tf.keras.optimizers.Adam(learning_rate=0.001 )#, beta_1=0.09, beta_2=0.009, epsilon=0.01, decay=0.001, amsgrad=False)
#model.compile(loss="MeanAbsoluteError", optimizer='adam')

from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='loss', mode='auto', verbose=0,min_delta=0.001, patience=5)
hist=model2.fit(xtrain,ytrain, steps_per_epoch=5,validation_split=0.3 ,epochs=50,batch_size=64,use_multiprocessing=True,callbacks=[es]  )

pred=model2.predict(xtest)
ypred1=tf.reshape(pred,pred.shape[0])

show_metrics(ytest, ypred1)
model2.build(input_shape=(38,350,20))
model2.summary()
plt.plot(hist.history["val_loss"])
plt.plot(hist.history["loss"])
plt.legend(["va loss"," training loss"])

model2 = Sequential()
model2.add(Conv1D(2000,3))
model2.add(MaxPool1D(2))
#model2.add(Conv1D(200,3))
model2.add(Conv1D(800,2))
model2.add(MaxPool1D(2))
model2.add(Conv1D(500,2))
model2.add(AveragePooling1D(2))
#model2.add(Conv1D(100,2))
#print()
model2.add(Flatten())
#model2.add(Bidirectional(LSTM(700,return_sequences=False), input_shape=(350,20))) # 10 is ouput of lstm block and input is 20 timesteps with 90000 inpiut shape  
#model2.add(Dropout(0.2))
#model2.add(tf.keras.layers.BatchNormalization())
#model2.add(LSTM(700, return_sequences=False))
#model2.add(tf.keras.layers.BatchNormalization())
#model2.add(LSTM(425, return_sequences=False))
#model2.add(Dropout(0.2))
#model2.add(tf.keras.layers.BatchNormalization())
#model.add(LSTM(300))
#model.add(Dropout(0.2))
#model2.add(Dense(750, activation='relu'))
#model2.add(Dense(35, activation='relu'))
#model2.add(Dropout(0.2))
#model2.add(Dense(350, activation='relu'))
#model2.add(Dense(350, activation='relu'))
#model2.add(Dense(100, activation='relu'))
model2.add(Dense(100, activation='relu'))
model2.add(Dense(10, activation='relu'))
model2.add(Dense(1, activation='relu'))
model2.compile(loss="MeanAbsoluteError", optimizer='adam')
tf.keras.optimizers.Adam(learning_rate=0.001 )#, beta_1=0.09, beta_2=0.009, epsilon=0.01, decay=0.001, amsgrad=False)
#model.compile(loss="MeanAbsoluteError", optimizer='adam')

from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='loss', mode='auto', verbose=0,min_delta=0.001, patience=5)
hist=model2.fit(xtrain,ytrain, steps_per_epoch=5,validation_split=0.3 ,epochs=50,batch_size=64,use_multiprocessing=True,callbacks=[es]  )

pred=model2.predict(xtest)
ypred1=tf.reshape(pred,pred.shape[0])

show_metrics(ytest, ypred1)
model2.build(input_shape=(38,350,20))
model2.summary()
plt.plot(hist.history["val_loss"])
plt.plot(hist.history["loss"])
plt.legend(["va loss"," training loss"])

"""## DNN model only"""

xtrain1=tf.expand_dims(xtrain,axis=1)
xtrain1.shape

model2 = Sequential()
#model2.add(Conv1D(2000,3))
#model2.add(MaxPool1D(2))
#model2.add(Conv1D(200,3))
#model2.add(Conv1D(800,2))
#model2.add(MaxPool1D(2))
#model2.add(Conv1D(500,2))
#model2.add(AveragePooling1D(2))
#model2.add(Conv1D(100,2))
#print()
#model2.add(Flatten())
#model2.add(Bidirectional(LSTM(700,return_sequences=False), input_shape=(350,20))) # 10 is ouput of lstm block and input is 20 timesteps with 90000 inpiut shape  
#model2.add(Dropout(0.2))
#model2.add(tf.keras.layers.BatchNormalization())
#model2.add(LSTM(700, return_sequences=False))
#model2.add(tf.keras.layers.BatchNormalization())
#model2.add(LSTM(425, return_sequences=False))
#model2.add(Dropout(0.2))
#model2.add(tf.keras.layers.BatchNormalization())
#model.add(LSTM(300))
#model.add(Dropout(0.2))
#model2.add(Dense(750, activation='relu'))
#model2.add(Dense(35, activation='relu'))
#model2.add(Dropout(0.2))
#model2.add(Dense(350, activation='relu'))
#model2.add(Dense(350, activation='relu'))
#model2.add(Dense(100, activation='relu'))
model2.add(Dense(2000, activation='relu'))
model2.add(Dense(1050, activation='relu'))

model2.add(Flatten())
model2.add(Dense(550, activation='relu'))
model2.add(Dense(500, activation='relu'))
model2.add(Dense(10, activation='relu'))
model2.add(Dense(1, activation='relu'))
model2.compile(loss="MeanAbsoluteError", optimizer='adam')
tf.keras.optimizers.Adam(learning_rate=0.001 )#, beta_1=0.09, beta_2=0.009, epsilon=0.01, decay=0.001, amsgrad=False)
#model.compile(loss="MeanAbsoluteError", optimizer='adam')

from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='loss', mode='auto', verbose=0,min_delta=0.001, patience=5)
hist=model2.fit(xtrain,ytrain, steps_per_epoch=5,validation_split=0.3 ,epochs=50,batch_size=64,use_multiprocessing=True,callbacks=[es]  )

pred=model2.predict(xtest)
ypred1=tf.reshape(pred,pred.shape[0])

show_metrics(ytest, ypred1)
model2.build(input_shape=(38,350,20))
model2.summary()
plt.plot(hist.history["val_loss"])
plt.plot(hist.history["loss"])
plt.legend(["va loss"," training loss"])

"""## CNN-3D N,1,360,20 (3D IMAGE)"""

tf.expand_dims(tf.expand_dims(xtrain,axis=1),axis=4)
g=xtrain1.numpy()[1][0][:,3,0] #.shape #.shape
plt.plot(g)

model2 = Sequential()
model2.add(Conv3D(200,(1,2,20),input_shape=(1,350,20,1))) # conv3d(no_3Dfilters=2, filter_shape=(x=1,y=3,z=1) , input_shape=(x=1,y=350,z=20, no of chaneels=1# only this))
model2.add(Conv3D(200,(1,2,1)))
model2.add(Conv3D(200,(1,2,1)))
#model2.add(MaxPool1D(2))
#model2.add(Conv3D(10,3))
#model2.add(Conv1D(800,2))
#model2.add(MaxPool1D(2))
#model2.add(Conv1D(500,2))
#model2.add(AveragePooling1D(2))
#model2.add(Conv1D(100,2))
#print()
model2.add(Flatten())

#model2.add(Bidirectional(LSTM(700,return_sequences=False), input_shape=(350,20))) # 10 is ouput of lstm block and input is 20 timesteps with 90000 inpiut shape  
#model2.add(Dropout(0.2))
#model2.add(tf.keras.layers.BatchNormalization())
#model2.add(LSTM(700, return_sequences=False))
#model2.add(tf.keras.layers.BatchNormalization())
#model2.add(LSTM(425, return_sequences=False))
#model2.add(Dropout(0.2))
#model2.add(tf.keras.layers.BatchNormalization())
#model.add(LSTM(300))
#model.add(Dropout(0.2))
#model2.add(Dense(750, activation='relu'))
#model2.add(Dense(35, activation='relu'))
#model2.add(Dropout(0.2))
#model2.add(Dense(350, activation='relu'))
#model2.add(Dense(350, activation='relu'))
#model2.add(Dense(100, activation='relu'))
#model2.add(Dense(2000, activation='relu'))
#model2.add(Dense(1050, activation='relu'))

#model2.add(Flatten())
model2.add(Dense(550, activation='relu'))
model2.add(Dense(500, activation='relu'))
model2.add(Dense(10, activation='relu'))
print("check1")
model2.add(Dense(1, activation='relu'))
model2.compile(loss="MeanAbsoluteError", optimizer='adam')

tf.keras.optimizers.Adam(learning_rate=0.001 )#, beta_1=0.09, beta_2=0.009, epsilon=0.01, decay=0.001, amsgrad=False)
model2.compile(loss="MeanAbsoluteError", optimizer='adam')

from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='loss', mode='auto', verbose=0,min_delta=0.001, patience=5)
hist=model2.fit(tf.expand_dims(tf.expand_dims(xtrain,axis=1),axis=4),       
                ytrain, steps_per_epoch=5,validation_split=0.3 ,epochs=50,batch_size=64,use_multiprocessing=True,callbacks=[es]  )

pred=model2.predict(tf.expand_dims(tf.expand_dims(xtest,axis=1),axis=4))
ypred1=tf.reshape(pred,pred.shape[0])

show_metrics(ytest, ypred1)
model2.summary()
plt.plot(hist.history["val_loss"])
plt.plot(hist.history["loss"])
plt.legend(["va loss"," training loss"])

"""## B-LSTM+CNN.2D+ CNN.1D+DNN"""

# state of art

from keras.layers import Bidirectional
model = Sequential()
model.add(Bidirectional(LSTM(50,return_sequences=True), input_shape=(350,20))) # 10 is ouput of lstm block and input is 20 timesteps with 90000 inpiut shape  
#model.add(tf.keras.layers.BatchNormalization())
#model.add(LSTM(100, return_sequences=True))

model.add(tf.keras.layers.Reshape((350,100,1)))
#model.add(Conv2D(64,2))
model.add(tf.keras.layers.MaxPool2D(2))
model.add(Conv1D(200,2))
model.add(tf.keras.layers.MaxPool2D(2))
model.add(Conv1D(100,2))
model.add(tf.keras.layers.AveragePooling2D(2))
model.add(Conv1D(50,2))

#model.add(tf.keras.layers.AveragePooling2D(3))

model.add(Flatten())
model.add(Dense(2000, activation='relu'))
# model.add(Dense(100, activation='relu'))
model.add(Dense(2000, activation='relu'))
model.add(Dense(1000, activation='relu'))
model.add(Dense(50, activation='relu'))
model.add(Dense(1, activation='relu'))

##############################################################################
model.compile(loss="MeanAbsoluteError", optimizer='adam')

tf.keras.optimizers.Adam(learning_rate=0.0001 )#, beta_1=0.09, beta_2=0.009, epsilon=0.01, decay=0.001, amsgrad=False)
model.compile(loss="MeanAbsoluteError", optimizer='adam')

from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='loss', mode='auto', verbose=0,min_delta=0.0001, patience=5)
hist=model.fit(xtrain,ytrain, steps_per_epoch=5,validation_split=0.2 ,epochs=50,batch_size=32,use_multiprocessing=True,callbacks=[es]  )

pred=model.predict(xtest)
ypred1=tf.reshape(pred,pred.shape[0])

show_metrics(ytest, ypred1)
model.summary()
plt.plot(hist.history["val_loss"])
plt.plot(hist.history["loss"])
plt.legend(["va loss"," training loss"])

from keras.layers import Bidirectional
model = Sequential()
model.add(Bidirectional(LSTM(50,return_sequences=True), input_shape=(350,20))) # 10 is ouput of lstm block and input is 20 timesteps with 90000 inpiut shape  
#model.add(tf.keras.layers.BatchNormalization())
#model.add(LSTM(100, return_sequences=True))

model.add(tf.keras.layers.Reshape((350,100,1)))
model.add(Conv2D(200,2))
#model.add(tf.keras.layers.MaxPool2D(2))
model.add(Conv1D(200,2))
model.add(tf.keras.layers.MaxPool2D(2))
model.add(Conv1D(100,2))
model.add(tf.keras.layers.MaxPool2D(2))
model.add(Conv1D(50,2))
model.add(tf.keras.layers.MaxPool2D(2))
model.add(Conv1D(25,2))
model.add(tf.keras.layers.MaxPool2D(2))
#model.add(tf.keras.layers.AveragePooling2D(3))

model.add(Flatten())
model.add(Dense(3000, activation='relu'))
# model.add(Dense(100, activation='relu'))
model.add(Dense(2000, activation='relu'))
model.add(Dense(1000, activation='relu'))
model.add(Dense(50, activation='relu'))
model.add(Dense(1, activation='relu'))

##############################################################################
model.compile(loss="MeanAbsoluteError", optimizer='adam')

tf.keras.optimizers.Adam(learning_rate=0.0001 )#, beta_1=0.09, beta_2=0.009, epsilon=0.01, decay=0.001, amsgrad=False)
model.compile(loss="MeanAbsoluteError", optimizer='adam')

from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='loss', mode='auto', verbose=0,min_delta=0.0001, patience=5)
hist=model.fit(xtrain,ytrain, steps_per_epoch=5,validation_split=0.2 ,epochs=50,batch_size=32,use_multiprocessing=True,callbacks=[es]  )

pred=model.predict(xtest)
ypred1=tf.reshape(pred,pred.shape[0])

show_metrics(ytest, ypred1)
model.summary()
plt.plot(hist.history["val_loss"])
plt.plot(hist.history["loss"])
plt.legend(["va loss"," training loss"])

from keras.layers import Bidirectional
model = Sequential()
model.add(Bidirectional(LSTM(50,return_sequences=True), input_shape=(350,20))) # 10 is ouput of lstm block and input is 20 timesteps with 90000 inpiut shape  
#model.add(tf.keras.layers.BatchNormalization())
#model.add(LSTM(100, return_sequences=True))

model.add(tf.keras.layers.Reshape((350,100,1)))
model.add(Conv2D(200,2))
#model.add(tf.keras.layers.MaxPool2D(2))
model.add(Conv1D(200,2))
model.add(tf.keras.layers.MaxPool2D(2))
model.add(Conv1D(100,2))
model.add(tf.keras.layers.MaxPool2D(2))
model.add(Conv1D(50,2))
model.add(tf.keras.layers.MaxPool2D(2))
model.add(Conv1D(25,2))
model.add(tf.keras.layers.MaxPool2D(2))
#model.add(tf.keras.layers.AveragePooling2D(3))

model.add(Flatten())
model.add(Dense(5000, activation='relu'))
# model.add(Dense(100, activation='relu'))
model.add(Dense(2000, activation='relu'))
model.add(Dense(1000, activation='relu'))
model.add(Dense(50, activation='relu'))
model.add(Dense(1, activation='relu'))

##############################################################################
model.compile(loss="MeanAbsoluteError", optimizer='adam')

tf.keras.optimizers.Adam(learning_rate=0.0001 )#, beta_1=0.09, beta_2=0.009, epsilon=0.01, decay=0.001, amsgrad=False)
model.compile(loss="MeanAbsoluteError", optimizer='adam')

from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='loss', mode='auto', verbose=0,min_delta=0.0001, patience=5)
hist=model.fit(xtrain,ytrain, steps_per_epoch=5,validation_split=0.2 ,epochs=50,batch_size=32,use_multiprocessing=True,callbacks=[es]  )

pred=model.predict(xtest)
ypred1=tf.reshape(pred,pred.shape[0])

show_metrics(ytest, ypred1)
model.summary()
plt.plot(hist.history["val_loss"])
plt.plot(hist.history["loss"])
plt.legend(["va loss"," training loss"])

from keras.layers import Bidirectional
model = Sequential()
model.add(Bidirectional(LSTM(20,return_sequences=True), input_shape=(350,20))) # 10 is ouput of lstm block and input is 20 timesteps with 90000 inpiut shape  
#model.add(tf.keras.layers.BatchNormalization())
#model.add(LSTM(100, return_sequences=True))
#model.add(tf.keras.layers.BatchNormalization())
n1=2
#model.add(LSTM(n1, return_sequences=True))
#model.add(LSTM(n1, return_sequences=False))
#model.add(tf.keras.layers.BatchNormalization())
#model.add(Dropout(0.2))

model.add(tf.keras.layers.Reshape((200,70,1)))
model.add(Conv2D(40,2))
model.add(tf.keras.layers.MaxPool2D(2))
#model.add(Conv1D(20,2))
#model.add(tf.keras.layers.MaxPool2D(2))
#model.add(Conv1D(60,2))
# model.add(Conv2D(64,2))
# model.add(tf.keras.layers.MaxPool2D (2))
# model.add(Conv2D(32,3))
#model.add(tf.keras.layers.MaxPool2D(2))
#model.add(Conv1D(40,2))
# n2=32
# model.add(Conv2D(n2,3))
# #model.add(tf.keras.layers.AveragePooling2D (2))
# #model.add(tf.keras.layers.Reshape((26,13*32)))
# #model2.add(Conv1D(100,2))

model.add(Flatten())
#model.add(Dense(2000, activation='relu'))
# model.add(Dense(100, activation='relu'))
model.add(Dense(3000, activation='relu'))
model.add(Dense(100, activation='relu'))
#model.add(Dense(100, activation='relu'))
model.add(Dense(10, activation='relu'))
model.add(Dense(1, activation='relu'))

##############################################################################
model.compile(loss="MeanAbsoluteError", optimizer='adam')

tf.keras.optimizers.Adam(learning_rate=0.0001 )#, beta_1=0.09, beta_2=0.009, epsilon=0.01, decay=0.001, amsgrad=False)
model.compile(loss="MeanAbsoluteError", optimizer='adam')

from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='loss', mode='auto', verbose=0,min_delta=0.0001, patience=5)
hist=model.fit(xtrain,ytrain, steps_per_epoch=5,validation_split=0.3 ,epochs=50,batch_size=64,use_multiprocessing=True,callbacks=[es]  )

pred=model.predict(xtest)
ypred1=tf.reshape(pred,pred.shape[0])

show_metrics(ytest, ypred1)
model.summary()
plt.plot(hist.history["val_loss"])
plt.plot(hist.history["loss"])
plt.legend(["va loss"," training loss"])

"""## saving and testing"""

path=r"/content/gdrive/MyDrive/saved_models/BLSTM_CNN_DNN_12%/"  # where u want to save your model
model.save(path)

path=r"/content/gdrive/MyDrive/saved_models/BLSTM_CNN_DNN"
m1=tf.keras.models.load_model(path)
pred=m1.predict(xtest)
ypred1=tf.reshape(pred,pred.shape[0])

show_metrics(ytest, ypred1)
m1.summary()

data=layerscheck(m1)

plt.subplots(4,4,figsize=(30,10))

plt.subplot(4,4,1)
plt.plot(data[0][0][0][0])
plt.title("1")

plt.subplot(4,4,2)
plt.plot(data[1][0][0][0])
plt.title("2")

plt.subplot(4,4,3)
plt.plot(data[2][0][0][0])
plt.title("3")

plt.subplot(4,4,4)
plt.plot(data[3][0][0][0][0])
plt.title("4")

plt.subplot(4,4,5)
plt.plot(data[4][0][0][0][0])
plt.title("5")

plt.subplot(4,4,6)
plt.plot(data[5][0][0][0][0])
plt.title("6")

plt.subplot(4,4,7)
plt.plot(data[6][0][0][0][0])
plt.title("7")

plt.subplot(4,4,8)
plt.plot(data[7][0][0])
plt.title("8")

plt.subplot(4,4,9)
plt.plot(data[8][0][0])
plt.title("9")

plt.subplot(4,4,10)
plt.plot(data[9][0][0])
plt.title("10")

plt.subplot(4,4,11)
plt.plot(data[10][0][0])
plt.title("11")

plt.subplot(4,4,12)
plt.plot(data[11][0][0])
plt.title("12")

#plt.subplot(4,4,8)
plt.plot(data[0][0][0],".")
plt.title("8")
#None, 349, 99, 64)

"""## <h1>  BLSTM model</h1>

"""

#print(len(xtrain))
from keras.layers import Bidirectional
model2 = Sequential()
model2.add(Dense(750,input_shape=(20,350)))
model2.add(Flatten())
#model2.add(Bidirectional(LSTM(700,return_sequences=False), input_shape=(350,20))) # 10 is ouput of lstm block and input is 20 timesteps with 90000 inpiut shape  
#model2.add(Dropout(0.2))
#model2.add(tf.keras.layers.BatchNormalization())
#model2.add(LSTM(700, return_sequences=False))
#model2.add(tf.keras.layers.BatchNormalization())
#model2.add(LSTM(425, return_sequences=False))
#model2.add(Dropout(0.2))
#model2.add(tf.keras.layers.BatchNormalization())
#model.add(LSTM(300))
#model.add(Dropout(0.2))
model2.add(Dense(750, activation='relu'))
#model2.add(Dense(35, activation='relu'))
#model2.add(Dropout(0.2))
#model2.add(Dense(350, activation='relu'))
#model2.add(Dense(350, activation='relu'))
model2.add(Dense(1, activation='relu'))
model2.compile(loss="MeanAbsoluteError", optimizer='adam')

model=model2

tf.keras.optimizers.Adam(learning_rate=0.001 )#, beta_1=0.09, beta_2=0.009, epsilon=0.01, decay=0.001, amsgrad=False)
model.compile(loss="MeanAbsoluteError", optimizer='adam')
model.summary()

from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='loss', mode='auto', verbose=1,min_delta=0.001, patience=5)
hist=model.fit(xtrain,ytrain, steps_per_epoch=5,validation_split=0.3 ,epochs=50,batch_size=64,use_multiprocessing=True,callbacks=[es]  )

pred=model.predict(xtest)
ypred1=tf.reshape(pred,pred.shape[0])

show_metrics(ytest, ypred1)
model.summary()

"""## ** PYTORCH 20 JULY New-CUSTOM DATASET AND DATALOADER**
<br> https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-create-a-neural-network-for-regression-with-pytorch.md
"""

from google.colab import drive
drive.mount("/content/gdrive/")
import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
!pip install mne
import mne
# Ignore warnings
import warnings
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import torchsummary
from torch.autograd import Variable
import random
!pip install torchinfo
import  torchinfo
from torchinfo import summary
import tensorflow as tf
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
from keras.metrics import mean_absolute_error  as mae
from sklearn.metrics import explained_variance_score
print("stage: libraries importing done")

X_path =  '/content/gdrive/MyDrive/edf'
Y_path =  '/content/gdrive/MyDrive/data/age_ScanID.csv'

#NPY_CustomEEGData : use it  when you load data as 
class NPY_CustomEEGData:
    def __init__(self,x,y):
      """
      inputs:
      x: npy araay of all edf outputs
      y: age of all data in  npy array in x 
      """
      self.x = torch.from_numpy(np.asarray(x ,dtype=np.float32))#.double()
      self.y = torch.from_numpy(np.asarray(y,dtype=np.float32))#.double()
        
    def __len__(self):
      return len(self.x)

    def __getitem__(self, idx):
      """
      output: X and Y
      """
      return self.x[idx],self.y[idx]
    
# when you pass path of edf files and y :
class PATH_CustomEEGData:
    def __init__(self,edf_file_directory_path,main_age_metadata):
      """
      inputs:
      edf_files_directory_path: directory of all filtered edf  files 
      main_age_metadata_: main metadata containg id and corresponding age
      """
      self.edf_file_directory = edf_file_directory_path
      self.main_age_metadata = main_age_metadata
      pass
        
    def __len__(self):
      n=os.listdir(self.edf_file_directory)
      return len(n)

    def __getitem__(self, idx):
      """
      output: X and Y
      """
      file_i = os.listdir(self.edf_file_directory)[idx]
      x = mne.io.read_raw_edf(os.path.join(self.edf_file_directory,file_i), verbose=False)
      #print(x.info)
      freq = int(dict(x.info)['sfreq'])

      xdata1 = np.array(x.get_data()[:,:-freq]).astype(np.float32)  #
      #print(xdata.shape)  
      xdata=np.transpose(xdata1,(1,0))
      id = x.filenames[0][-40:-4]
      id1 = pd.DataFrame([id],columns=["ScanID"])
      age1 = pd.merge(id1,pd.read_csv(self.main_age_metadata),how="inner").iloc[:,4].tolist()[0]
      age= np.array(age1).astype(np.float32)
      return xdata,age

dataset=PATH_CustomEEGData(X_path, Y_path)
#xx1=dataset.__getitem__(10)

# for i in range(7020):
#      x,y=dataset.__getitem__(i)
#      if np.any(np.isnan(x)):
#            print("yes  x")
#      if np.any(np.isnan(y)):
#            print("yes  y")

dataset=PATH_CustomEEGData(X_path, Y_path)   # contian x nd y data
"""
dataset.__getitem__(10)
batch_size = 32
dataset=PATH_CustomEEGData(X_path, Y_path)   # contian x nd y data



validation_split = .2
shuffle_dataset = True
dataset_size = len(dataset)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))
np.random.shuffle(indices)

train_indices, val_indices,test_indices = indices[split:], indices[:int(split/2)],indices[int(split/2):split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)
test_sampler= SubsetRandomSampler(test_indices)

train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,sampler=valid_sampler)
test_loader= torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)
# for x,y in train_loader:
#     print(x.shape)
#     break

# class ConvNet(nn.Module):
#     def __init__(self):
#         super(ConvNet, self).__init__()

#         self.layer1 = nn.Conv1d(20,5,1,1)   #17*347 
#         self.maxpool1= nn.MaxPool1d(4)    
#         self.layer2 = nn.Conv1d(1,1,2)     #16*346
#         self.maxpool2= nn.MaxPool1d(2)
#         self.relu=  nn.ReLU()
#         self.flatten=nn.Flatten()
#         self.fc1 = nn.Linear(1280, 100)  # 5*5 from image dimension
#         self.fc2 = nn.Linear(215, 10)
#         self.fc3 = nn.Linear(100, 1)

#     def forward(self, x):
#         #print(x.shape,"I miss u swati ")
#         out=self.layer1(x)
#         #print(out.shape)
#         out=self.maxpool1(out)
#         #print(out.shape)
#         #out=self.layer2(out)
#         out=self.maxpool2(out)
#         #print(out.shape)
#         #print(out.shape,"----------------")
#         #print(out.shape,"flatten check ")
#         out=self.flatten(out)
#         #print(out.shape)
#         #print(out.shape,"flatten check  after flatten")
#         out=self.relu(out)
#         #print(out.shape)
#         #out=self.fc1(out)
#         out=self.fc1(out)
#         #print(out.shape)
#         out=self.fc3(out)
#         #print(out.shape)
#         return out
print("stage: data loader testing done! ")

# class ConvNet(nn.Module):
#     def __init__(self):
#         super(ConvNet, self).__init__()

#         self.layer1 = nn.Conv1d(20,5,1,1)   #17*347 
#         self.maxpool1= nn.MaxPool1d(4)    
#         self.layer2 = nn.Conv1d(1,1,2)     #16*346
#         self.maxpool2= nn.MaxPool1d(2)
#         self.relu=  nn.ReLU()
#         self.flatten=nn.Flatten()
#         self.fc1 = nn.Linear(1280, 100)  # 5*5 from image dimension
#         self.fc2 = nn.Linear(215, 10)
#         self.fc3 = nn.Linear(100, 1)

#     def forward(self, x):
#         #print(x.shape,"I miss u swati ")
#         out=self.layer1(x)
#         #print(out.shape)
#         out=self.maxpool1(out)
#         #print(out.shape)
#         #out=self.layer2(out)
#         out=self.maxpool2(out)
#         #print(out.shape)
#         #print(out.shape,"----------------")
#         #print(out.shape,"flatten check ")
#         out=self.flatten(out)
#         #print(out.shape)
#         #print(out.shape,"flatten check  after flatten")
#         out=self.relu(out)
#         #print(out.shape)
#         #out=self.fc1(out)
#         out=self.fc1(out)
#         #print(out.shape)
#         out=self.fc3(out)
#         #print(out.shape)
#         return out
# model = ConvNet()
"""

class LSTM_sub(nn.Module):

    def __init__(self, input_size, hidden_size, num_layers=5):
        super(LSTM_sub, self).__init__()
        
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size
        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size,num_layers=num_layers, batch_first=True)
        #self.lstm2 = nn.LSTM(input_size=2048*3, hidden_size=2048,num_layers=num_layers, batch_first=True)
        #self.lstm3 = nn.LSTM(input_size=2048, hidden_size=1024,num_layers=num_layers, batch_first=True)
        #self.lstm4 = nn.LSTM(input_size=1024, hidden_size=512,num_layers=num_layers, batch_first=True)
        #self.lstm5 = nn.LSTM(input_size=512, hidden_size=256,num_layers=num_layers, batch_first=True)
        #self.lstm6 = nn.LSTM(input_size=256, hidden_size=128,num_layers=num_layers, batch_first=True)
        #self.lstm7 = nn.LSTM(input_size=128, hidden_size=64,num_layers=num_layers, batch_first=True)
        #self.lstm8 = nn.LSTM(input_size=64, hidden_size=32,num_layers=num_layers, batch_first=True)
        #self.conv1=nn.Conv1d(2048,2048,2)
        #self.conv2=nn.Conv1d(2048,1024,2)
        #self.conv3=nn.Conv1d(1024,512,3)
        #self.conv4=nn.Conv1d(512,256,3)
        #self.conv5=nn.Conv1d(256,128,5)
        #self.conv6=nn.Conv1d(128,64,5)
        #self.flat=torch.nn.flatten()
        
        self.l1=nn.Linear(2048*50, 300)
        self.l2=nn.Linear(300,100)
        self.l3=nn.Linear(100,10)
        self.l4=nn.Linear(10,1)
       
    def forward(self, x):
        print(x.shape,"forward x shape",self.hidden_size )

        h_0 = Variable(torch.zeros(self.num_layers, x.shape[0],self.hidden_size)) #x.shape[0],
        #print(h_0.shape)
        c_0 = Variable(torch.zeros(self.num_layers,x.shape[0], self.hidden_size)) #x.shape[0],
        #print(c_0.shape,"forward c0 shape")
        # Propagate input through LSTM
        a,(h,c)= self.lstm1(x, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        # h_0 = Variable(torch.zeros(1, x.shape[0],2048)) #x.shape[0],
        # #print(h_0.shape)
        # c_0 = Variable(torch.zeros(1,x.shape[0], 2048)) #x.shape[0],
        # #print(c_0.shape,"forward c0 shape")
        # # Propagate input through LSTM
        # a1,(h1,c1)= self.lstm2(a, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        # h_0 = Variable(torch.zeros(1, x.shape[0],1024)) #x.shape[0],
        # #print(h_0.shape)
        # c_0 = Variable(torch.zeros(1,x.shape[0], 1024)) #x.shape[0],
        # #print(c_0.shape,"forward c0 shape")
        # # Propagate input through LSTM
        # a2,(h1,c1)= self.lstm3(a1, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        # h_0 = Variable(torch.zeros(1, x.shape[0],512)) #x.shape[0],
        # #print(h_0.shape)
        # c_0 = Variable(torch.zeros(1,x.shape[0], 512)) #x.shape[0],
        # #print(c_0.shape,"forward c0 shape")
        # # Propagate input through LSTM
        # a3,(h1,c1)= self.lstm4(a2, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        # h_0 = Variable(torch.zeros(1, x.shape[0],256)) #x.shape[0],
        # #print(h_0.shape)
        # c_0 = Variable(torch.zeros(1,x.shape[0], 256)) #x.shape[0],
        # #print(c_0.shape,"forward c0 shape")
        # # Propagate input through LSTM
        # a4,(h1,c1)= self.lstm5(a3, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        # h_0 = Variable(torch.zeros(1, x.shape[0],128)) #x.shape[0],
        # #print(h_0.shape)
        # c_0 = Variable(torch.zeros(1,x.shape[0], 128)) #x.shape[0],
        # #print(c_0.shape,"forward c0 shape")
        # # Propagate input through LSTM
        # a5,(h1,c1)= self.lstm6(a4, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        # h_0 = Variable(torch.zeros(1, x.shape[0],64)) #x.shape[0],
        # #print(h_0.shape)
        # c_0 = Variable(torch.zeros(1,x.shape[0], 64)) #x.shape[0],
        # #print(c_0.shape,"forward c0 shape")
        # # Propagate input through LSTM
        # a6,(h1,c1)= self.lstm7(a5, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 32,2048,7

        # h_0 = Variable(torch.zeros(1, x.shape[0],32)) #x.shape[0],
        # #print(h_0.shape)
        # c_0 = Variable(torch.zeros(1,x.shape[0], 32)) #x.shape[0],
        # #print(c_0.shape,"forward c0 shape")
        # # Propagate input through LSTM
        # a7,(h1,c1)= self.lstm8(a6, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 32,2048,7


        #print(a2.shape,h.shape,c.shape,"forard a,b,c shape")
        #self.conv=nn.Conv1d(a.shape[1],a.shape[1]/2,2)
        #out=self.conv1(a2)
        #out=self.conv2(out)
        #print(out.shape,"conv2")
        #out=self.conv3(out)
        #out=self.conv4(out)
        #print(out.shape)
        #out=self.conv5(out)
        #print(out.shape)
        #out=self.conv6(out)
        #print(out.shape,"hhhhhhhhhhhhhoooooooooooooooooooooooooooooopppppppppppppppppooooooooooooooooooooooooooooooohhhhhhhhhhh")
        #s=sel.flat()(out)
        #print(a.shape[1],"jbkhvbhv")
        #out=nn.Conv1d(a.shape[1],a.shape[1]/2,2)(a)
        #print(a6.shape)
        out=nn.Flatten()(a)
        print(out.shape)
        #print(out.shape[1],"hhhhhhhhhhhhhhhhhhhhhhhh")
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
       
        #print(out.shape) ##160
        return out
model=LSTM_sub(20,50)
import  torchinfo
from torchinfo import summary
print("hii")
summary(model, (1,2048,20))

"""## channel wise lstm"""

class LSTM(nn.Module):
     # only lstm network
    def __init__(self, input_size, hidden_size, num_layers=1):
        super(LSTM, self).__init__()
        
        self.num_layers = num_layers
        self.input_size = input_size    #input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size
        
        
        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size,num_layers=num_layers, batch_first=True)
        self.lstm2 = nn.LSTM(input_size=input_size, hidden_size=hidden_size,num_layers=num_layers, batch_first=True)
        self.lstm3 = nn.LSTM(input_size=200, hidden_size=100,num_layers=num_layers, batch_first=True)
        
        self.l1=nn.Linear(100,1)   
       
    def forward(self, x):
        print(torch.unsqueeze(x[:,1,:],1).shape,"forward x shape" )

        h_0 = Variable(torch.zeros(self.num_layers, x.shape[0],self.hidden_size)) #x.shape[0],
        c_0 = Variable(torch.zeros(self.num_layers,x.shape[0], self.hidden_size)) #x.shape[0],
        a,(h,c)= self.lstm1(torch.unsqueeze(x[:,1,:],1), (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        h_0 = Variable(torch.zeros(1, x.shape[0],self.hidden_size)) #x.shape[0],
        c_0 = Variable(torch.zeros(1,x.shape[0], self.hidden_size)) #x.shape[0],
        a1,(h1,c1)= self.lstm2(torch.unsqueeze(x[:,2,:],1), (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50
        
        afinal=torch.squeeze(torch.stack((a,a1),1),2)

        h_0 = Variable(torch.zeros(1, x.shape[0],100)) #
        c_0 = Variable(torch.zeros(1,x.shape[0], 100)) #x.shape[0],
        
        a1,(h1,c1)= self.lstm3(afinal, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50
        
        print(a1.shape,h1.shape,c1.shape,"a1,h1,c1---")
        out=nn.Flatten()(h1)
        out=self.l1(out)
        return out


model=LSTM(2048,200)
summary(model, (1,20, 2048))

class LSTM(nn.Module):
     # only lstm network
    def __init__(self, input_size, hidden_size, num_layers=1):
        super(LSTM, self).__init__()
        
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size
        
        
        self.lstm1 = nn.LSTM(input_size=20, hidden_size=200,num_layers=num_layers, batch_first=True)
        self.lstm2 = nn.LSTM(input_size=200, hidden_size=400,num_layers=num_layers, batch_first=True)
        self.lstm3 = nn.LSTM(input_size=400, hidden_size=200,num_layers=num_layers, batch_first=True)
        self.lstm4 = nn.LSTM(input_size=200, hidden_size=100,num_layers=num_layers, batch_first=True)
        self.lstm5 = nn.LSTM(input_size=100, hidden_size=50,num_layers=num_layers, batch_first=True)
        #self.lstm6 = nn.LSTM(input_size=50, hidden_size=25,num_layers=num_layers, batch_first=True)
        #self.lstm7 = nn.LSTM(input_size=25, hidden_size=7,num_layers=num_layers, batch_first=True)
        
        
        
        self.conv1=nn.Conv1d(2048,4096,2)
        self.conv2=nn.Conv1d(4096,8192,2)
        self.conv3=nn.Conv1d(8192,2048,3)
        self.conv4=nn.Conv1d(2048,1024,3)
        self.conv5=nn.Conv1d(1024,512,5)
        self.conv6=nn.Conv1d(512,128,5)
        #self.flat=torch.nn.flatten()
        
        self.pool1=torch.nn.MaxPool1d(2)  # input: 128*36, 128*18

        self.l1=nn.Linear(128*9, 2000)
        self.l2=nn.Linear(2000,1)
        #self.l3=nn.Linear(800,100)
        #self.l4=nn.Linear(100,1)
       
    def forward(self, x):
        #print(x.shape,"forward x shape" )
        h_0 = Variable(torch.zeros(1, x.shape[0],self.hidden_size)) #x.shape[0],
        #print(h_0.shape)
        c_0 = Variable(torch.zeros(1,x.shape[0], self.hidden_size)) #x.shape[0],
        #print(c_0.shape,"forward c0 shape")
        # Propagate input through LSTM
        a,(h,c)= self.lstm1(x, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        h_0 = Variable(torch.zeros(1, x.shape[0],400)) #x.shape[0],
        #print(h_0.shape)
        c_0 = Variable(torch.zeros(1,x.shape[0], 400)) #x.shape[0],
        #print(c_0.shape,"forward c0 shape")
        # Propagate input through LSTM
        a1,(h1,c1)= self.lstm2(a, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        h_0 = Variable(torch.zeros(1, x.shape[0],200)) #x.shape[0],
        #print(h_0.shape)
        c_0 = Variable(torch.zeros(1,x.shape[0], 200)) #x.shape[0],
        #print(c_0.shape,"forward c0 shape")
        # Propagate input through LSTM
        a2,(h1,c1)= self.lstm3(a1, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        h_0 = Variable(torch.zeros(1, x.shape[0],100)) #x.shape[0],
        #print(h_0.shape)
        c_0 = Variable(torch.zeros(1,x.shape[0], 100)) #x.shape[0],
        #print(c_0.shape,"forward c0 shape")
        # Propagate input through LSTM
        a3,(h1,c1)= self.lstm4(a2, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        h_0 = Variable(torch.zeros(1, x.shape[0],50)) #x.shape[0],
        #print(h_0.shape)
        c_0 = Variable(torch.zeros(1,x.shape[0], 50)) #x.shape[0],
        #print(c_0.shape,"forward c0 shape")
        # Propagate input through LSTM
        a4,(h1,c1)= self.lstm5(a3, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

#         h_0 = Variable(torch.zeros(1, x.shape[0],25)) #x.shape[0],
#         #print(h_0.shape)
#         c_0 = Variable(torch.zeros(1,x.shape[0], 25)) #x.shape[0],
#         #print(c_0.shape,"forward c0 shape")
#         # Propagate input through LSTM
#         a5,(h1,c1)= self.lstm6(a4, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

#         h_0 = Variable(torch.zeros(1, x.shape[0],7)) #x.shape[0],
#         #print(h_0.shape)
#         c_0 = Variable(torch.zeros(1,x.shape[0], 7)) #x.shape[0],
#         #print(c_0.shape,"forward c0 shape")
#         # Propagate input through LSTM
#         a6,(h1,c1)= self.lstm7(a5, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 32,2048,7


        #print(a2.shape,h.shape,c.shape,"forard a,b,c shape")
        #self.conv=nn.Conv1d(a.shape[1],a.shape[1]/2,2)
        out=self.conv1(a4)
        out=self.conv2(out)
        #print(out.shape,"conv2")
        out=self.conv3(out)
        out=self.conv4(out)
        #print(out.shape)
        out=self.conv5(out)
        #print(out.shape)
        out=self.conv6(out)
        #print(out.shape,"hhhhhhhhhhhhhoooooooooooooooooooooooooooooopppppppppppppppppooooooooooooooooooooooooooooooohhhhhhhhhhh")
        #s=sel.flat()(out)
        #print(a.shape[1],"jbkhvbhv")
        #out=nn.Conv1d(a.shape[1],a.shape[1]/2,2)(a)
        #print(a6.shape)
        out=self.pool1(out)
        out=self.pool1(out)
        out=nn.Flatten()(out)
        #print(out.shape)
        #print(out.shape[1],"hhhhhhhhhhhhhhhhhhhhhhhh")
        out=self.l1(out)
        out=self.l2(out)
        #out=self.l3(out)
        #out=self.l4(out)
       
        #print(out.shape) ##160
        return out
model=LSTM(20,200)
import  torchinfo
from torchinfo import summary
print("hii")
summary(model, (1,2048, 20))  # sample, timeseries, len of one timeseries

class LSTM(nn.Module):
     # only lstm network
    def __init__(self, input_size, hidden_size, num_layers=1):
        super(LSTM, self).__init__()
        
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size
        
        
        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size,num_layers=num_layers, batch_first=True)
        self.lstm2 = nn.LSTM(input_size=50, hidden_size=100,num_layers=num_layers, batch_first=True)
        self.lstm3 = nn.LSTM(input_size=100, hidden_size=200,num_layers=num_layers, batch_first=True)
        self.lstm4 = nn.LSTM(input_size=200, hidden_size=100,num_layers=num_layers, batch_first=True)
        self.lstm5 = nn.LSTM(input_size=100, hidden_size=50,num_layers=num_layers, batch_first=True)
        self.lstm6 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers, batch_first=True)
        #self.lstm7 = nn.LSTM(input_size=25, hidden_size=7,num_layers=num_layers, batch_first=True)
        
        
        #self.conv1=nn.Conv1d(2048,2048,2)
        #self.conv2=nn.Conv1d(2048,1024,2)
        #self.conv3=nn.Conv1d(1024,512,3)
        #self.conv4=nn.Conv1d(512,256,3)
        #self.conv5=nn.Conv1d(256,128,5)
        #self.conv6=nn.Conv1d(128,64,5)
        self.flat=torch.nn.Flatten()
        
        self.l1=nn.Linear(2048*20, 2048)
        self.l2=nn.Linear(2048,516)
        self.l3=nn.Linear(516,1)
        #self.l4=nn.Linear(2048*20/10/10/10,1)
       
    def forward(self, x):
        #print(x.shape,"forward x shape" )
        h_0 = Variable(torch.zeros(1, x.shape[0],self.hidden_size)) #x.shape[0],
        #print(h_0.shape)
        c_0 = Variable(torch.zeros(1,x.shape[0], self.hidden_size)) #x.shape[0],
        #print(c_0.shape,"forward c0 shape")
        # Propagate input through LSTM
        a,(h,c)= self.lstm1(x, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        h_0 = Variable(torch.zeros(1, x.shape[0],100)) #x.shape[0],
        #print(h_0.shape)
        c_0 = Variable(torch.zeros(1,x.shape[0], 100)) #x.shape[0],
        #print(c_0.shape,"forward c0 shape")
        # Propagate input through LSTM
        a1,(h1,c1)= self.lstm2(a, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        h_0 = Variable(torch.zeros(1, x.shape[0],200)) #x.shape[0],
        #print(h_0.shape)
        c_0 = Variable(torch.zeros(1,x.shape[0], 200)) #x.shape[0],
        #print(c_0.shape,"forward c0 shape")
        # Propagate input through LSTM
        a2,(h1,c1)= self.lstm3(a1, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        h_0 = Variable(torch.zeros(1, x.shape[0],100)) #x.shape[0],
        #print(h_0.shape)
        c_0 = Variable(torch.zeros(1,x.shape[0], 100)) #x.shape[0],
        #print(c_0.shape,"forward c0 shape")
        # Propagate input through LSTM
        a3,(h1,c1)= self.lstm4(a2, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        h_0 = Variable(torch.zeros(1, x.shape[0],50)) #x.shape[0],
        #print(h_0.shape)
        c_0 = Variable(torch.zeros(1,x.shape[0], 50)) #x.shape[0],
        #print(c_0.shape,"forward c0 shape")
        # Propagate input through LSTM
        a4,(h1,c1)= self.lstm5(a3, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        h_0 = Variable(torch.zeros(1, x.shape[0],20)) #x.shape[0],
        #print(h_0.shape)
        c_0 = Variable(torch.zeros(1,x.shape[0], 20)) #x.shape[0],
        #print(c_0.shape,"forward c0 shape")
        # Propagate input through LSTM
        a5,(h1,c1)= self.lstm6(a4, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

#         h_0 = Variable(torch.zeros(1, x.shape[0],7)) #x.shape[0],
#         #print(h_0.shape)
#         c_0 = Variable(torch.zeros(1,x.shape[0], 7)) #x.shape[0],
#         #print(c_0.shape,"forward c0 shape")
#         # Propagate input through LSTM
#         a6,(h1,c1)= self.lstm7(a5, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 32,2048,7


        #print(a2.shape,h.shape,c.shape,"forard a,b,c shape")
        #self.conv=nn.Conv1d(a.shape[1],a.shape[1]/2,2)
        #out=self.conv1(a2)
        #out=self.conv2(out)
        #print(out.shape,"conv2")
        #out=self.conv3(out)
        #out=self.conv4(out)
        #print(out.shape)
        #out=self.conv5(out)
        #print(out.shape)
        #out=self.conv6(out)
        #print(out.shape,"hhhhhhhhhhhhhoooooooooooooooooooooooooooooopppppppppppppppppooooooooooooooooooooooooooooooohhhhhhhhhhh")
        #s=sel.flat()(out)
        #print(a.shape[1],"jbkhvbhv")
        #out=nn.Conv1d(a.shape[1],a.shape[1]/2,2)(a)
        print(a5.shape)
        out=nn.Flatten()(a5)
        
        print(out.shape)
        #print(out.shape[1],"hhhhhhhhhhhhhhhhhhhhhhhh")
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        #out=self.l4(out)
       
        #print(out.shape) ##160
        return out
model=LSTM(20,50)
import  torchinfo
from torchinfo import summary
print("hii")
summary(model, (1,2048, 20))  # sample, timeseries, len of one timeseries

class LSTM(nn.Module):
     # only lstm network
    def __init__(self, input_size, hidden_size, num_layers=1):
        super(LSTM, self).__init__()
        
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size
        
        
        self.lstm1 = nn.LSTM(input_size=20, hidden_size=200,num_layers=num_layers, batch_first=True)
        self.lstm2 = nn.LSTM(input_size=200, hidden_size=400,num_layers=num_layers, batch_first=True)
        self.lstm3 = nn.LSTM(input_size=400, hidden_size=200,num_layers=num_layers, batch_first=True)
        self.lstm4 = nn.LSTM(input_size=200, hidden_size=100,num_layers=num_layers, batch_first=True)
        #self.lstm5 = nn.LSTM(input_size=100, hidden_size=50,num_layers=num_layers, batch_first=True)
        #self.lstm6 = nn.LSTM(input_size=50, hidden_size=25,num_layers=num_layers, batch_first=True)
        #self.lstm7 = nn.LSTM(input_size=25, hidden_size=7,num_layers=num_layers, batch_first=True)
        
        
        #self.conv1=nn.Conv1d(2048,2048,2)
        #self.conv2=nn.Conv1d(2048,1024,2)
        #self.conv3=nn.Conv1d(1024,512,3)
        #self.conv4=nn.Conv1d(512,256,3)
        #self.conv5=nn.Conv1d(256,128,5)
        #self.conv6=nn.Conv1d(128,64,5)
        #self.flat=torch.nn.flatten()
        
        #self.l1=nn.Linear(-, 100)
        self.l2=nn.Linear(100,1)
        #self.l3=nn.Linear(800,100)
        #self.l4=nn.Linear(100,1)
       
    def forward(self, x):
        #print(x.shape,"forward x shape" )
        h_0 = Variable(torch.zeros(1, x.shape[0],self.hidden_size)) #x.shape[0],
        #print(h_0.shape)
        c_0 = Variable(torch.zeros(1,x.shape[0], self.hidden_size)) #x.shape[0],
        #print(c_0.shape,"forward c0 shape")
        # Propagate input through LSTM
        a,(h,c)= self.lstm1(x, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        h_0 = Variable(torch.zeros(1, x.shape[0],400)) #x.shape[0],
        #print(h_0.shape)
        c_0 = Variable(torch.zeros(1,x.shape[0], 400)) #x.shape[0],
        #print(c_0.shape,"forward c0 shape")
        # Propagate input through LSTM
        a1,(h1,c1)= self.lstm2(a, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        h_0 = Variable(torch.zeros(1, x.shape[0],200)) #x.shape[0],
        #print(h_0.shape)
        c_0 = Variable(torch.zeros(1,x.shape[0], 200)) #x.shape[0],
        #print(c_0.shape,"forward c0 shape")
        # Propagate input through LSTM
        a2,(h1,c1)= self.lstm3(a1, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        h_0 = Variable(torch.zeros(1, x.shape[0],100)) #x.shape[0],
        #print(h_0.shape)
        c_0 = Variable(torch.zeros(1,x.shape[0], 100)) #x.shape[0],
        #print(c_0.shape,"forward c0 shape")
        # Propagate input through LSTM
        a3,(h1,c1)= self.lstm4(a2, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

#         h_0 = Variable(torch.zeros(1, x.shape[0],50)) #x.shape[0],
#         #print(h_0.shape)
#         c_0 = Variable(torch.zeros(1,x.shape[0], 50)) #x.shape[0],
#         #print(c_0.shape,"forward c0 shape")
#         # Propagate input through LSTM
#         a4,(h1,c1)= self.lstm5(a3, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

#         h_0 = Variable(torch.zeros(1, x.shape[0],25)) #x.shape[0],
#         #print(h_0.shape)
#         c_0 = Variable(torch.zeros(1,x.shape[0], 25)) #x.shape[0],
#         #print(c_0.shape,"forward c0 shape")
#         # Propagate input through LSTM
#         a5,(h1,c1)= self.lstm6(a4, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

#         h_0 = Variable(torch.zeros(1, x.shape[0],7)) #x.shape[0],
#         #print(h_0.shape)
#         c_0 = Variable(torch.zeros(1,x.shape[0], 7)) #x.shape[0],
#         #print(c_0.shape,"forward c0 shape")
#         # Propagate input through LSTM
#         a6,(h1,c1)= self.lstm7(a5, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 32,2048,7


        #print(a2.shape,h.shape,c.shape,"forard a,b,c shape")
        #self.conv=nn.Conv1d(a.shape[1],a.shape[1]/2,2)
        #out=self.conv1(a2)
        #out=self.conv2(out)
        #print(out.shape,"conv2")
        #out=self.conv3(out)
        #out=self.conv4(out)
        #print(out.shape)
        #out=self.conv5(out)
        #print(out.shape)
        #out=self.conv6(out)
        #print(out.shape,"hhhhhhhhhhhhhoooooooooooooooooooooooooooooopppppppppppppppppooooooooooooooooooooooooooooooohhhhhhhhhhh")
        #s=sel.flat()(out)
        #print(a.shape[1],"jbkhvbhv")
        #out=nn.Conv1d(a.shape[1],a.shape[1]/2,2)(a)
        #print(a6.shape)
        out=nn.Flatten()(h1)
        #print(out.shape)
        #print(out.shape[1],"hhhhhhhhhhhhhhhhhhhhhhhh")
        #out=self.l1(h1)
        out=self.l2(out)
        #out=self.l3(out)
        #out=self.l4(out)
       
        #print(out.shape) ##160
        return out
model=LSTM(20,200)
import  torchinfo
from torchinfo import summary
print("hii")
summary(model, (1,2048, 20))  # sample, timeseries, len of one timeseries

"""## <h1> pytorch 1 - CNN model
  </h1>
"""

from __future__ import print_function, division
import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils

# Ignore warnings
import warnings
warnings.filterwarnings("ignore")

"""User input : path of x and y np array"""

#kindly pass these 4 inputs and run notebook like amul makkan.
ypath=r"/content/gdrive/MyDrive/data/edf_data_10sec_50HZ_y.npy"
xpath=r"/content/gdrive/MyDrive/data/edf_data_10sec_50HZ_x.npy"
batch_size=32
epochs=50

xdata=np.load(xpath,allow_pickle=True).tolist()
ydata=np.load(ypath,allow_pickle=True).tolist()

from sklearn.model_selection import train_test_split
xtrain,xval,ytrain,yval=train_test_split(xdata,ydata,shuffle=True,train_size=0.8)
xtest,xval,ytest,yval=train_test_split(xval,yval,shuffle=True,train_size=0.5)

#NPY_CustomEEGData : use it  when you load data as 
class NPY_CustomEEGData:
    def __init__(self,x,y):
      """
      inputs:
      x: npy araay of all edf outputs
      y: age of all data in  npy array in x 
      """
      self.x=torch.from_numpy(np.asarray(x ,dtype=np.float32))#.double()
      self.y=torch.from_numpy(np.asarray(y,dtype=np.float32))#.double()
        
    def __len__(self):
      return len(self.x)

    def __getitem__(self, idx):
      """
      output: X and Y
      """
      return self.x[idx],self.y[idx]
# when you pass path of edf files and y :
class PATH_CustomEEGData:
  
    def __init__(self,edf_files_directory_path,main_age_metadata_):
      """
      inputs:
      edf_files_directory_path: directory of all filtered edf  files 
      main_age_metadata_: main metadata containg id and corresponding age
      """
      self.edf_file_directory=edf_file_directory_path
      self.main_age_metadata=main_age_metadata
      pass
        
    def __len__(self):
      n=os.listdir(self.edf_file_directory)
      return len(n)

    def __getitem__(self, idx):
      """
      output: X and Y
      """
      x=mne.io.read_raw_edf(os.listdir(self.edf_file_directory)[idx])
      xdata=np.array(x.get_data())

      id=x.filenames[0][-40:-4]
      id1=pd.DataFrame([id],columns=["scan_id"])
      age=pd.merge(id1,pd.read_csv(self.main_age_metadata),how="inner").iloc[:,1].tolist()[0]

      return xdata,age

def show_metrics(y_test, y_predict):
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()

Custom_Data = CustomEEGData(xtrain,ytrain)
x1,y1=Custom_Data.__getitem__(10)

x1.dtype

from torch.utils.data import DataLoader

train_dataloader = DataLoader(dataset=CustomEEGData(xtrain,ytrain), batch_size=batch_size, shuffle=True,num_workers =3)
validation_dataloader = DataLoader(dataset=CustomEEGData(xval,yval), batch_size=batch_size, shuffle=True,num_workers =3)
test_dataloader = DataLoader(dataset=CustomEEGData(xtest,ytest), batch_size=batch_size, shuffle=True,num_workers =3)

for x,y in train_dataloader:
  print(x.shape,x.dtype)
  break

#testing the iter dataloader:happy
dataiter=iter(train_dataloader)  # iterate data one  by one
data=dataiter.next()
x,y=data
print(x.shape)

import torch
import torch.nn as nn
import torch.nn.functional as F
!pip install torchinfo
from torchinfo import summary

"""# CNN model"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
  
    def __init__(self):
        super(Net, self).__init__()

        self.layer1 = nn.Conv2d(1,1,4,1)   #17*347 
        self.maxpool1= nn.MaxPool2d(2)    
        self.layer2 = nn.Conv2d(1,1,2)     #16*346
        self.maxpool2= nn.MaxPool2d(2)
        self.relu=  nn.ReLU()
        self.flatten=nn.Flatten()
        self.fc1 = nn.Linear(1384, 120)  # 5*5 from image dimension
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 1)

    def forward(self, x):
      #print(x.shape,"I miss u swati ")
      out=self.layer1(x)
      #out=self.maxpool1(out)
      out=self.layer2(out)
      out=self.maxpool2(out)
      #print(out.shape,"----------------")
      out=self.flatten(out)
      out=self.relu(out)
      out=self.fc1(out)
      out=self.fc2(out)
      out=self.fc3(out)
      return out
model = Net()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
criterion = nn.L1Loss()
optimizer=torch.optim.Adam(model.parameters(),lr=0.001)

check=1
@torch.no_grad()  # disable gradient calculation.
def evaluation(model,validation_loader):
  global check
  """
  it will return loss and prediction on validation data

  Help: use validation_epoch_end to get collet all the loss and predictions across different gpu and collect all in 1 place
  """
  output=[]
  yvalid=[]
  for batch in validation_loader:
    batch[0]=torch.unsqueeze(batch[0],1)  # use it for cnn model 
    out=model(batch[0])
    if check:
      x=batch[0][0][0][0]
      plt.plot(x,*"")
      check=0
    loss=criterion(out,batch[1])

    output.extend(out[0].numpy()[0])
    yvalid.extend(batch[1])
    mae_=round(mae(yvalid, output), 3)

  return mae_ 

epochs=epochs
hist={"Validation":[],"training":[]}
for epoch in range(epochs):
  model.train()
  training_loss = []
  for batch in train_dataloader:
    batch[0]=torch.unsqueeze(batch[0],1)  # use it for cnn model

    pred=model(batch[0])

    loss=criterion(pred,batch[1])
    loss.backward()
    
    training_loss.append(loss.item())

    optimizer.step() # update optimizer paramerters
    optimizer.zero_grad() # zero all gradient
    
  output=evaluation(model,train_dataloader)
  print("Epoch_{i}        ".format(i=epoch),"validation MAE {:3f}         ".format(np.mean(output)),"training MAE {:3f}".format(np.mean(training_loss)))
  hist["Validation"].append(np.mean(output))
  hist["training"].append(np.mean(training_loss))

print('Finished Training')

"""# LSTM model"""

class ShallowRegressionLSTM(nn.Module):
    def __init__(self, num_sensors, hidden_units,bidirectional=True):
        super().__init__()
        self.num_sensors = num_sensors  # this is the number of features
        self.hidden_units = hidden_units
        self.num_layers = 1
        self.fc1 = nn.Linear(20, 84)
        self.fc2 = nn.Linear(84, 1)
        self.flat=torch.flatten
        self.maxpool2= nn.MaxPool2d(2)
        self.relu=  nn.ReLU()
        if bidirectional:
          self.d=2
        else:
          self.d=1

        self.lstm = nn.LSTM(
            input_size=num_sensors, #timeseries
            hidden_size=hidden_units, # features in hidden output layers
            batch_first=True,
            num_layers=self.num_layers,
            bidirectional=True
        )

        self.linear = nn.Linear(in_features=self.hidden_units, out_features=1)

    def forward(self, x):
        #print(x.shape,"shape of x input")
        batch_size = x.shape[0]
        h0 = torch.zeros(self.num_layers*self.d, batch_size, self.hidden_units).requires_grad_()  #requires_grad_: require grad tells variable is trainable
        c0 = torch.zeros(self.num_layers*self.d, batch_size, self.hidden_units).requires_grad_()
        #print(h0.shape,c0.shape, "shape of h0 and c0 respectively")
        output, (hn, Cn) = self.lstm(x, (h0, c0)) #  output, (h_n, c_n)
        #print(output.shape,hn.shape,Cn.shape,"output,hn,cn shapes from final output ")
        out=self.relu(hn[0])
        out = self.fc1(out)# .flatten()  # First dim of Hn is num_layers, which is set to 1 above., hn[0] says 1 layer h0 and c0
        out=self.fc2(out)
        
        return out
model2 = ShallowRegressionLSTM(350,20)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
criterion = nn.L1Loss()
optimizer=torch.optim.Adam(model.parameters(),lr=0.001)
model2



model=model2
check=1
@torch.no_grad()  # disable gradient calculation.
def evaluation(model,validation_loader):
  global check
  """
  it will return loss and prediction on validation data

  Help: use validation_epoch_end to get collet all the loss and predictions across different gpu and collect all in 1 place
  """
  output=[]
  yvalid=[]
  for batch in validation_loader:
    #batch[0]=torch.unsqueeze(batch[0],1)  # use it for cnn model 
    out=model(batch[0])
    yvalid.extend(batch[1].numpy())
    #print(yvalid)
    if check:
      x=batch[0][0][0][0]
      plt.plot(x,*"")
      check=0
    loss=criterion(out,batch[1])
    #print(out.shape)
    output.extend(out.numpy())
    #print(out,out.numpy())
    #print(len(output)==len(yvalid))
    mae_=round(mae(yvalid, output), 3)
  return mae_




epochs=epochs
hist={"Validation":[],"training":[]}
for epoch in range(epochs):
  model.train()
  training_loss = []
  for batch in train_dataloader:
    #batch[0]=torch.unsqueeze(batch[0],1)  # use it for cnn model

    pred=model(batch[0])

    loss=criterion(pred,batch[1])
    loss.backward()
    
    training_loss.append(loss.item())

    optimizer.step() # update optimizer paramerters
    optimizer.zero_grad() # zero all gradient
    
  output=evaluation(model,train_dataloader)
  print("Epoch_{i}        ".format(i=epoch),"validation MAE {:3f}         ".format(np.mean(output)),"training MAE {:3f}".format(np.mean(training_loss)))
  hist["Validation"].append(np.mean(output))
  hist["training"].append(np.mean(training_loss))

print('Finished Training')

"""<h2>Xtra </h2>"""

for x,y in train_dataloader:
  print(type(x))
  model2(x)
  break

model2(x)

print(model)

params = list(model.parameters())
print(len(params))
print(params[-1].size())  # conv1's .weight

"""1- validatio_step: use it when you need validation <br>
2-training_step: return training loss of give tensors as in dataloader               <br>
3- predict_step: it calls forward propagtion by default

<h1>Analysis of rnn models</h1>
"""

plt.subplots(figsize=[30,10])
plt.plot(hist["Validation"])
plt.plot(hist["training"],"r")
plt.legend(["Validation loss","training loss"])
plt.xlabel("No of epochs")
plt.title("LOSS vS ePOCHS pYCHAM")

PATH = '/content/gdrive/MyDrive/cnn_pytorch_6july.pth'
torch.save(model.state_dict(), PATH)

@torch.no_grad()  # disable gradient calculation.
def prediction(model,test_loader):
  """
  1-it will return loss and prediction on validation data
  2-Help: use validation_epoch_end to get collet all the loss and predictions across different gpu and collect all in 1 place
  """
  output=[]
  y=[]
  for batch in test_loader:
    #batch[0]=torch.unsqueeze(batch[0],1) use it for cnn model
    out=model(batch[0])
    output.extend(out.numpy())
    y.extend(batch[1].numpy())
  #print(out.numpy(),batch[1].numpy())
  return np.squeeze(np.array(output)) ,np.array(y)
ypred,ytest=prediction(model,test_dataloader)

show_metrics(ytest,ypred)

## Histry model for help

class LSTM(nn.Module):
     # only lstm network
    def __init__(self, input_size, hidden_size, num_layers=1):
        super(LSTM, self).__init__()
        
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size
        
        
        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size,num_layers=num_layers, batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=50, hidden_size=100,num_layers=num_layers, batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=100, hidden_size=50,num_layers=num_layers, batch_first=True).to(device)
        #self.lstm4 = nn.LSTM(input_size=20, hidden_size=2,num_layers=num_layers, batch_first=True)
        #self.lstm5 = nn.LSTM(input_size=100, hidden_size=50,num_layers=num_layers, batch_first=True)
        #self.lstm6 = nn.LSTM(input_size=100, hidden_size=20,num_layers=num_layers, batch_first=True)
        #self.lstm7 = nn.LSTM(input_size=25, hidden_size=7,num_layers=num_layers, batch_first=True)
        
        
        self.conv1=nn.Conv1d(2048,4096,3)
        
        self.conv2=nn.Conv1d(4096,2048,3)
        self.conv3=nn.Conv1d(2048,1024,3)
        self.conv4=nn.Conv1d(1024,256,3)
        #self.conv5=nn.Conv1d(256,128,5)
        #self.conv6=nn.Conv1d(128,64,5)

        self.GrpNorm=nn.GroupNorm(1, 516)  # put 516 channels in  1 groupand normalize it as one piece
        self.channelDropout=nn.Dropout2d(p=0.2) # input:n,channels-1028 timesteps, sequence_length-20    , it will mute zero to some channels 20%  from 1028.

        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(2048).to(device)
        self.batchNorm=torch.nn.BatchNorm1d(2048).to(device)
        
        self.l1=nn.Linear(256*18, 2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)

        self.l4=nn.Linear(500,1)
        self.l5=nn.Linear(20,1)
       
    def forward(self, x):
        h_0 = Variable(torch.zeros(1, x.shape[0],self.hidden_size)).to(device) #x.shape[0],
        c_0 = Variable(torch.zeros(1,x.shape[0], self.hidden_size)).to(device) #x.shape[0],
        #xx=torch.nn.Dropout(0.2)(x).to(device)
        a,(h,c)= self.lstm1(x, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        out=self.batchNorm(a)
        out=self.relu(out)

        h_0 = Variable(torch.zeros(1, x.shape[0],100)).to(device) #x.shape[0],
        c_0 = Variable(torch.zeros(1,x.shape[0], 100)).to(device) #x.shape[0],
        aa=self.batchNorm(a).to(device)
        #aa1=torch.nn.Dropout(0.2)(aa).to(device)
        a1,(h1,c1)= self.lstm2(aa, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        out=self.batchNorm(a1)

        h_0 = Variable(torch.zeros(1, x.shape[0],50)).to(device) #x.shape[0],
        c_0 = Variable(torch.zeros(1,x.shape[0], 50)).to(device) #x.shape[0],
        #aaa=self.relu(a1)
        a5,(h1,c1)= self.lstm3(self.relu(out), (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        # h_0 = Variable(torch.zeros(1, x.shape[0],2)).to(device) #x.shape[0],
        # c_0 = Variable(torch.zeros(1,x.shape[0], 2)).to(device) #x.shape[0],
        aaa=self.relu(a5)
        # a5,(h1,c1)= self.lstm4(a5, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50
        out=self.batchNorm(aaa)
        
        out=self.conv1(out)
        #out=torch.nn.MaxPool1d(2)(out)
        out=torch.nn.AvgPool1d(2)(out)
        
        out=self.conv2(out)
        #out=self.batchNorm(out)
        out=self.conv3(out)
        out=self.conv4(out)
        #out=self.conv5(out)
        #out=self.conv6(out)
        #s=sel.flat()(out)
        #out=nn.Conv1d(a.shape[1],a.shape[1]/2,2)(a)
        
        out=nn.Flatten()(out)
        #out=self.dropout(out)
        #print(out.shape)
        out=self.l1(out)
        out=self.relu(out)
        #out=self.relu(out)
        out=self.l2(out)
        #out=self.relu(out)
        out=self.relu(out)

        out=self.l3(out)
        #out=nn.GroupNorm(1, 516)(out)  # put 516 channels in  1 groupand normalize it as one piece
        out=self.relu(out)
        #out=self.relu(out)
        out=self.l4(out)
        #out=self.l5(out)
        return out

model=LSTM(20,50).to(device)
summary(model, (1,2048, 20))  # sample, timeseries, len of one timeseries

# input n,20,2048



class LSTM(nn.Module):
     # only lstm network
    def __init__(self, input_size, hidden_size, num_layers=1):
        super(LSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size
      
        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size,num_layers=num_layers, batch_first=True).to(device)
        
        self.conv1=nn.Conv1d(20,40,3)
        self.conv2=nn.Conv1d(40,60,3)
        self.conv3=nn.Conv1d(2048,256,3)
        self.conv4=nn.Conv1d(1024,256,3)

        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(2048).to(device)
        
        self.grpNorm=nn.GroupNorm(1, 256*10)
        
        self.l1=nn.Linear(256*10, 4000)
        self.l2=nn.Linear(4000,2000)
        self.l3=nn.Linear(2000,1)
        self.l4=nn.Linear(1000,500)
        self.l5=nn.Linear(500,10)
        self.l6=nn.Linear(10,1)
  
       
    def forward(self, x):
        h_0 = Variable(torch.zeros(1, x.shape[0],self.hidden_size)).to(device) #x.shape[0],
        c_0 = Variable(torch.zeros(1,x.shape[0], self.hidden_size)).to(device) #x.shape[0],
        a,(h,c)= self.lstm1(x, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        out=self.batchNorm(a)
        out=self.conv1(out)
        out=torch.nn.MaxPool1d(2)(out)
        #out=torch.nn.AvgPool1d(2)(out)
        
        out=self.conv2(out)
        out=self.relu(out)
        out=torch.nn.MaxPool1d(2)(out)

        out=self.conv3(out)
        out=torch.nn.MaxPool1d(2)(out)
        
        out=nn.Flatten()(out)
        out=self.grpNorm(out)  ##out=nn.GroupNorm(1, 516)(out)  # put 516 channels in  1 groupand normalize it as one piece
        
        out=self.l1(out)
        out=self.l2(out)
        out=self.relu(out)
        out=self.l3(out)
        #out=self.l4(out)
        #out=self.relu(out)
        #out=self.l5(out)
        #out=self.l6(out)
        return out

model=LSTM(2048,10).to(device)
summary(model, (1,20, 2048))  # sample, timeseries, len of one timeseries

# model input n,2048,20




class LSTM(nn.Module):
     # only lstm network
    def __init__(self, input_size, hidden_size, num_layers=1):
        super(LSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size
      
        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size,num_layers=num_layers, batch_first=True).to(device)
        
        self.conv1=nn.Conv1d(2048,2048,3)
        self.conv2=nn.Conv1d(2048,2048,3)
        self.conv3=nn.Conv1d(2048,256,3)
        self.conv4=nn.Conv1d(1024,256,3)

        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(2048).to(device)
        
        self.grpNorm=nn.GroupNorm(1, 256*10)
        
        self.l1=nn.Linear(256*10, 4000)
        self.l2=nn.Linear(4000,2000)
        self.l3=nn.Linear(2000,1)
        self.l4=nn.Linear(1000,500)
        self.l5=nn.Linear(500,10)
        self.l6=nn.Linear(10,1)
  
       
    def forward(self, x):
        h_0 = Variable(torch.zeros(1, x.shape[0],self.hidden_size)).to(device) #x.shape[0],
        c_0 = Variable(torch.zeros(1,x.shape[0], self.hidden_size)).to(device) #x.shape[0],
        a,(h,c)= self.lstm1(x, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        out=self.batchNorm(a)
        out=self.conv1(out)
        out=torch.nn.MaxPool1d(2)(out)
        #out=torch.nn.AvgPool1d(2)(out)
        
        out=self.conv2(out)
        out=self.relu(out)
        out=torch.nn.MaxPool1d(2)(out)

        out=self.conv3(out)
        out=torch.nn.MaxPool1d(2)(out)
        
        out=nn.Flatten()(out)
        out=self.grpNorm(out)  ##out=nn.GroupNorm(1, 516)(out)  # put 516 channels in  1 groupand normalize it as one piece
        
        out=self.l1(out)
        out=self.l2(out)
        out=self.relu(out)
        out=self.l3(out)
        #out=self.l4(out)
        #out=self.relu(out)
        #out=self.l5(out)
        #out=self.l6(out)
        return out

model=LSTM(20,100).to(device)
summary(model, (1,2048, 20))  # sample, timeseries, len of one timeseries

# history model 2

class LSTM(nn.Module):
     # only lstm network
    def __init__(self, input_size, hidden_size, num_layers=1):
        super(LSTM, self).__init__()
        
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size
        
        
        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size,num_layers=num_layers, batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=50, hidden_size=100,num_layers=num_layers, batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=100, hidden_size=20,num_layers=num_layers, batch_first=True).to(device)
        #self.lstm4 = nn.LSTM(input_size=20, hidden_size=2,num_layers=num_layers, batch_first=True)
        #self.lstm5 = nn.LSTM(input_size=100, hidden_size=50,num_layers=num_layers, batch_first=True)
        #self.lstm6 = nn.LSTM(input_size=100, hidden_size=20,num_layers=num_layers, batch_first=True)
        #self.lstm7 = nn.LSTM(input_size=25, hidden_size=7,num_layers=num_layers, batch_first=True)
        

        # for conv2d
        """
        out=self.conv1(torch.unsqueeze(out,1))
        self.conv1=nn.Conv2d(1,1,3)
        """


        self.conv1=nn.Conv1d(2048,4096,3)
        self.conv2=nn.Conv1d(4096,2048,3)
        self.conv3=nn.Conv1d(2048,1024,3)
        self.conv4=nn.Conv1d(1024,256,3)
        #self.conv5=nn.Conv1d(256,128,5)
        #self.conv6=nn.Conv1d(128,64,5)

        self.GrpNorm=nn.GroupNorm(1, 516)  # put 516 channels in  1 groupand normalize it as one piece
        self.channelDropout=nn.Dropout2d(p=0.2) # input:n,channels-1028 timesteps, sequence_length-20    , it will mute zero to some channels 20%  from 1028.

        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(2048).to(device)
        self.batchNorm=torch.nn.BatchNorm1d(2048).to(device)
        
        self.l1=nn.Linear(256*12, 1500)
        self.l2=nn.Linear(1500,800)
        self.l3=nn.Linear(800,100)

        self.l4=nn.Linear(100,1)
        self.l5=nn.Linear(20,1)
       
    def forward(self, x):
        h_0 = Variable(torch.zeros(1, x.shape[0],self.hidden_size)).to(device) #x.shape[0],
        c_0 = Variable(torch.zeros(1,x.shape[0], self.hidden_size)).to(device) #x.shape[0],
        #xx=torch.nn.Dropout(0.2)(x).to(device)
        a,(h,c)= self.lstm1(x, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        out=self.batchNorm(a)

        h_0 = Variable(torch.zeros(1, x.shape[0],100)).to(device) #x.shape[0],
        c_0 = Variable(torch.zeros(1,x.shape[0], 100)).to(device) #x.shape[0],
        #aa=self.batchNorm(a).to(device)
        #aa1=torch.nn.Dropout(0.2)(aa).to(device)
        a1,(h1,c1)= self.lstm2(out, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        out=self.batchNorm(a1)

        h_0 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        c_0 = Variable(torch.zeros(1,x.shape[0], 20)).to(device) #x.shape[0],
        #aaa=self.relu(a1)
        a5,(h1,c1)= self.lstm3(self.relu(out), (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50

        # h_0 = Variable(torch.zeros(1, x.shape[0],2)).to(device) #x.shape[0],
        # c_0 = Variable(torch.zeros(1,x.shape[0], 2)).to(device) #x.shape[0],
        # #aaa=self.relu(a1)
        # a5,(h1,c1)= self.lstm4(a5, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50
        out=self.batchNorm(a5)
        print(a5.shape,h1.shape,c1.shape,"forard a,b,c shape")
        #self.conv=nn.Conv1d(a.shape[1],a.shape[1]/2,2)
        out=self.conv1(out)
        out=self.conv2(out)
        
        out=self.conv3(out)
        out=self.conv4(out)
        #out=self.conv5(out)
        #out=self.conv6(out)
        #s=sel.flat()(out)
        #out=nn.Conv1d(a.shape[1],a.shape[1]/2,2)(a)
        #out=torch.nn.BatchNorm1d(2048)(a5)

        
        out=nn.Flatten()(out)
        #out=self.dropout(out)
        #print(out.shape)
        out=self.l1(out)
        #out=self.relu(out)
        out=self.l2(out)
        #out=self.relu(out)
        out=self.l3(out)
        #out=nn.GroupNorm(1, 516)(out)  # put 516 channels in  1 groupand normalize it as one piece
        
        #out=self.relu(out)
        out=self.l4(out)
        #out=self.l5(out)
        return out

model=LSTM(20,50).to(device)
summary(model, (1,2048, 20))  # sample, timeseries, len of one timeseries

# input n,20,2048
## channel wise lstm


class LSTM(nn.Module):
     # only lstm network
    def __init__(self, input_size, hidden_size, num_layers=1):
        super(LSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size
      
        self.lstm1 = nn.LSTM(input_size=1, hidden_size=hidden_size,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=2, hidden_size=5,num_layers=num_layers,batch_first=True).to(device)
        
        self.conv1=nn.Conv1d(20,40,3)
        self.conv2=nn.Conv1d(40,60,3)
        self.conv3=nn.Conv1d(2048,256,3)
        self.conv4=nn.Conv1d(1024,256,3)

        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(2048).to(device)
        
        self.grpNorm=nn.GroupNorm(1, 256*10)
        
        self.l1=nn.Linear(10,1)
        self.l2=nn.Linear(4000,2000)
        self.l3=nn.Linear(2000,1)
        self.l4=nn.Linear(1000,500)
        self.l5=nn.Linear(500,10)
        self.l6=nn.Linear(10,1)
  
       
    def forward(self, x):
        print(x.shape,"x input shape")
        
        h_0 = Variable(torch.zeros( 1, x.shape[0],self.hidden_size)).to(device) #x.shape[0],
        c_0 = Variable(torch.zeros(1, x.shape[0], self.hidden_size)).to(device) #x.shape[0],
        a0,(h0,c)= self.lstm1(torch.unsqueeze(x[:,0,:],-1), (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50
        

        h_0 = Variable(torch.zeros(1, x.shape[0],self.hidden_size)).to(device) #x.shape[0],
        c_0 = Variable(torch.zeros(1,x.shape[0], self.hidden_size)).to(device) #x.shape[0],
        a1,(h1,c)= self.lstm1(torch.unsqueeze(x[:,1,:],-1), (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50
        main=torch.squeeze(torch.stack((h0,h1),1),2)
        
        

        if x.shape[0]>1:
          
          h_0 = Variable(torch.zeros( main.shape[0],5)).to(device) #x.shape[0],
          c_0 = Variable(torch.zeros(main.shape[0], 5)).to(device) #x.shape[0],
          print("if",h_0.shape,"h0 shape",main.shape,"main shape")
          
          a1,(h1,c)= self.lstm3(main, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50
        else:
          h_0 = Variable(torch.zeros(1, main.shape[0],5)).to(device) #x.shape[0],
          c_0 = Variable(torch.zeros(1,main.shape[0], 5)).to(device) #x.shape[0],
          a1,(h1,c)= self.lstm3(main, (h_0, c_0))  # a shape : 32,2048,50 while h abd c: 332,2048,50
        print(a1.shape,c.shape,h1.shape,"a,c,b")

        #out=self.batchNorm(a)
        #out=self.conv1(a)
        #out=torch.nn.MaxPool1d(2)(out)
        #out=torch.nn.AvgPool1d(2)(out)
        
        #out=self.conv2(out)
        #out=self.relu(out)
        #out=torch.nn.MaxPool1d(2)(out)

        # out=self.conv3(out)
        # out=torch.nn.MaxPool1d(2)(out)
        
        out=nn.Flatten()(a1)
        #out=self.grpNorm(out)  ##out=nn.GroupNorm(1, 516)(out)  # put 516 channels in  1 groupand normalize it as one piece
        
        out=self.l1(out)
        ##out=self.l2(out)
        #out=self.relu(out)
        #out=self.l3(out)
        #out=self.l4(out)
        #out=self.relu(out)
        #out=self.l5(out)
        #out=self.l6(out)
        return out

model=LSTM(2048,2).to(device)
summary(model, (1,20, 2048))  # sample, timeseries, len of one timeseries

"""#   data testing; Dataloader  Function"""

import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
import mne
# Ignore warnings
import warnings
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import random
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset

X_path =  '/home/adarsh/projects/rpp-doesburg/vpa20/Shared/EEG_CNN/EDF_128Hz_16sec'
Y_path =  '/home/adarsh/projects/rpp-doesburg/vpa20/age_ScanID.csv'



#NPY_CustomEEGData : use it  when you load data as 
class NPY_CustomEEGData:
    def __init__(self,x,y):
      """
      inputs:
      x: npy araay of all edf outputs
      y: age of all data in  npy array in x 
      """
      self.x = torch.from_numpy(np.asarray(x ,dtype=np.float32))#.double()
      self.y = torch.from_numpy(np.asarray(y,dtype=np.float32))#.double()
        
    def __len__(self):
      return len(self.x)

    def __getitem__(self, idx):
      """
      output: X and Y
      """
      return self.x[idx],self.y[idx]
    
    
# when you pass path of edf files and y :
class PATH_CustomEEGData:
  
    def __init__(self,edf_file_directory_path,main_age_metadata):
      """
      inputs:
      edf_files_directory_path: directory of all filtered edf  files 
      main_age_metadata_: main metadata containg id and corresponding age
      """
      self.edf_file_directory = edf_file_directory_path
      self.main_age_metadata = main_age_metadata
      pass
        
    def __len__(self):
      n=os.listdir(self.edf_file_directory)
      return len(n)

    def __getitem__(self, idx):
      """
      output: X and Y
      """
      file_i = os.listdir(self.edf_file_directory)[idx]
      x = mne.io.read_raw_edf(os.path.join(self.edf_file_directory,file_i), verbose=False)
      #print(x.info)
      freq = int(dict(x.info)['sfreq'])

      xdata = np.array(x.get_data()[:,:-freq]).astype(np.float32)
        
      id = x.filenames[0][-40:-4]
      id1 = pd.DataFrame([id],columns=["ScanID"])
      age1 = pd.merge(id1,pd.read_csv(self.main_age_metadata),how="inner").iloc[:,4].tolist()[0]
      age= np.array(age1).astype(np.float32)
      return xdata,age

dataset=PATH_CustomEEGData(X_path, Y_path)
xx1=dataset.__getitem__(10)





dataset=PATH_CustomEEGData(X_path, Y_path)   # contian x nd y data
dataset.__getitem__(10)

batch_size = 32
dataset=PATH_CustomEEGData(X_path, Y_path)   # contian x nd y data

validation_split = .2
shuffle_dataset = True
dataset_size = len(dataset)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))
np.random.shuffle(indices)

train_indices, val_indices = indices[split:], indices[:split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)
overall=torch.utils.data.DataLoader(dataset, batch_size=batch_size)
train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, 
                                           sampler=train_sampler)
validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,
                                                sampler=valid_sampler)


class ConvNet(nn.Module):
  
    def __init__(self):
        super(ConvNet, self).__init__()

        self.layer1 = nn.Conv1d(20,5,1,1)   #17*347 
        self.maxpool1= nn.MaxPool1d(4)    
        self.layer2 = nn.Conv1d(1,1,2)     #16*346
        self.maxpool2= nn.MaxPool1d(2)
        self.relu=  nn.ReLU()
        self.flatten=nn.Flatten()
        self.fc1 = nn.Linear(1280, 100)  # 5*5 from image dimension
        self.fc2 = nn.Linear(215, 10)
        self.fc3 = nn.Linear(100, 1)

    def forward(self, x):
        #print(x.shape,"I miss u swati ")
        out=self.layer1(x)
        print(out.shape)
        out=self.maxpool1(out)
        print(out.shape)
        #out=self.layer2(out)
        out=self.maxpool2(out)
        print(out.shape)
        #print(out.shape,"----------------")
        #print(out.shape,"flatten check ")
        out=self.flatten(out)
        print(out.shape)
        #print(out.shape,"flatten check  after flatten")
        out=self.relu(out)
        print(out.shape)
        #out=self.fc1(out)
        out=self.fc1(out)
        print(out.shape)
        out=self.fc3(out)
        print(out.shape)
        return out
model = ConvNet()
import torchsummary
from torchsummary import summary
summary(model, (20, 2048),32)

"""## Data Prep """

from google.colab import drive
drive.mount("/content/gdrive/")

import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
!pip install mne
import mne
# Ignore warnings
import warnings

!pip install torchinfo
import  torchinfo
from torchinfo import summary
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import torchsummary
from torch.autograd import Variable
import random
import tensorflow as tf
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
from keras.metrics import mean_absolute_error  as mae
from sklearn.metrics import explained_variance_score
print("stage: libraries importing done")


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

X_path =  '/content/gdrive/MyDrive/data/xdata_20_2048.npy'
Y_path =  '/content/gdrive/MyDrive/data/ydata_20_2048.npy'
xdata=np.load(X_path,allow_pickle=True)
ydata=np.load(Y_path,allow_pickle=True)


#NPY_CustomEEGData : use it  when you load data as 
class NPY_CustomEEGData:
    def __init__(self,x,y,device=device):
      """
      inputs:
      x: npy araay of all edf outputs
      y: age of all data in  npy array in x 
      """
      self.x = torch.from_numpy(np.asarray(x ,dtype=np.float32))#.double()
      self.y = torch.from_numpy(np.asarray(y,dtype=np.float32))#.double()
      self.device=device
        
    def __len__(self):
      return len(self.x)

    def __getitem__(self, idx):
      """
      output: X and Y

      """

      xdata=np.transpose(self.x[idx],(1,0))
      #xdata=self.x[idx]
      return xdata.to(self.device),self.y[idx].to(self.device)
dataset=NPY_CustomEEGData(np.stack(xdata,0), ydata)
xx1=dataset.__getitem__(10)


validation_split = .2
shuffle_dataset = True
dataset_size = len(dataset)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))
np.random.shuffle(indices)

train_indices, val_indices,test_indices = indices[split:], indices[:int(split/2)],indices[int(split/2):split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)
test_sampler= SubsetRandomSampler(test_indices)
batch_size=32
train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,sampler=valid_sampler)
test_loader= torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)
del xdata
del ydata
del indices
del dataset_size
del split

for x,y in train_loader:
  print(x.shape)
  break

"""## Model Preparation"""

# model input n,2048,20
#lstm model channel wise




class LSTM(nn.Module):
     # only lstm network
    def __init__(self, input_size, hidden_size, num_layers=1):
        super(LSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        
        self.conv1=nn.Conv1d(1,1,1)
        self.conv2=nn.Conv1d(1,1,1)
        self.conv3=nn.Conv1d(2,2,1)
        self.conv4=nn.Conv1d(500,500,3)
        self.conv1=nn.Conv1d(1,1,1)

        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(2048).to(device)
        
        #self.grpNorm=nn.GroupNorm(1, 2*20)
        
        self.l1=nn.Linear(4*20, 1)
        self.l2=nn.Linear(400,1)
        self.l3=nn.Linear(2000,1)
        self.l4=nn.Linear(1000,500)
        self.l5=nn.Linear(500,10)
        self.l6=nn.Linear(10,1)
  
       
    def forward(self, x):
        xl1=x[:,0:500,:]
        #print(xl1.shape,"xl1 shape")

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(xl1,(h1,c1))
        #print(a1.shape,"a1 shape")
        
        xl2=x[:,500:1000,:]
        #print(xl2.shape,"xl2 shape")
        h2 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c2 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],-
        a2,(h2,c2)=self.lstm2(xl2,(h2,c2))
        #print(h2.shape,"h2 shape")

        xl3=x[:,1000:1500,:]
        #print(xl2.shape,"xl2 shape")
        h3 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c3 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],-
        a3,(h3,c3)=self.lstm3(xl3,(h3,c3))

        xl4=x[:,1500:2000,:]
        #print(xl2.shape,"xl2 shape")
        h4 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c4 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],-
        a4,(h4,c4)=self.lstm4(xl4,(h4,c4))

        new=torch.stack((h1,h2,h3,h4),2)
        #print(new.shape,"new shape")


        # xx1=torch.unsqueeze(x[:,1,:],1)
        # xx2=torch.unsqueeze(x[:,2,:],1)
        


        #out=self.batchNorm(a)
        # out=self.conv1(xx1)
        # #out=torch.nn.MaxPool1d(2)(out)
        # #out=torch.nn.AvgPool1d(2)(out)

        # out1=self.conv2(xx2)
       
        # #print(new.shape)
        # out=self.conv3(new)
        # out=self.relu(out)
        # #out=torch.nn.MaxPool1d(2)(out) 

        # #out=self.conv3(out)
        # #out=torch.nn.MaxPool1d(2)(out)
        
        out=nn.Flatten(start_dim=2)(new)
        #print(out.shape,"flatten")
        #out=self.grpNorm(out)  ##out=nn.GroupNorm(1, 516)(out)  # put 516 channels in  1 groupand normalize it as one piece
        
        out=self.l1(out)

        #out=self.l2(out)
        #out=self.relu(out)
        #out=self.l3(out)
        #out=self.l4(out)
        #out=self.relu(out)
        #out=self.l5(out)
        #out=self.l6(out)
        return out

model=LSTM(20,20).to(device)  # len of sample at 1 timeseries
summary(model, (1,2048, 20))  # sample, timeseries, len of one timeseries

# model input n,2048,20
#cnn-1d model channel wise
class LSTM(nn.Module):
     # only lstm network
    def __init__(self, input_size, hidden_size, num_layers=1):
        super(LSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.conv1=nn.Conv1d(1,1,1)
        self.conv2=nn.Conv1d(1,1,1)
        self.conv3=nn.Conv1d(2,2,1)
        self.conv4=nn.Conv1d(500,500,3)
        self.conv1=nn.Conv1d(1,1,1)

        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(2048).to(device)
        
        #self.grpNorm=nn.GroupNorm(1, 2*20)
        
        self.l1=nn.Linear(2*2048, 1)
        self.l2=nn.Linear(400,1)
        self.l3=nn.Linear(2000,1)
        self.l4=nn.Linear(1000,500)
        self.l5=nn.Linear(500,10)
        self.l6=nn.Linear(10,1)
  
       
    def forward(self, x):
        
        xx1=torch.unsqueeze(x[:,1,:],1)
        xx2=torch.unsqueeze(x[:,2,:],1)
        


        #out=self.batchNorm(a)
        out1=self.conv1(xx1)
        # #out=torch.nn.MaxPool1d(2)(out)
        # #out=torch.nn.AvgPool1d(2)(out)

        out2=self.conv2(xx2)
        new = torch.cat((out1,out2),1)
        #print(new.shape)
        # #print(new.shape)
        # out=self.conv3(new)
        # out=self.relu(out)
        # #out=torch.nn.MaxPool1d(2)(out) 

        # #out=self.conv3(out)
        # #out=torch.nn.MaxPool1d(2)(out)
        
        out=nn.Flatten()(new)
        #print(out.shape,"flatten")
        #out=self.grpNorm(out)  ##out=nn.GroupNorm(1, 516)(out)  # put 516 channels in  1 groupand normalize it as one piece
        
        out=self.l1(out)

        #out=self.l2(out)
        #out=self.relu(out)
        #out=self.l3(out)
        #out=self.l4(out)
        #out=self.relu(out)
        #out=self.l5(out)
        #out=self.l6(out)
        return out

model=LSTM(2048,2048).to(device)  # len of sample at 1 timeseries
summary(model, (1,20, 2048))  # sample, timeseries, len of one timeseries

# model input n,2048,20
#cnn-2D model channel wise




class LSTM(nn.Module):
     # only lstm network
    def __init__(self, input_size, hidden_size, num_layers=1):
        super(LSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        
        self.conv1=nn.Conv2d(1,1,1,1)
        self.conv2=nn.Conv2d(1,1,1,1)
        self.conv3=nn.Conv2d(1,1,1,1)
        self.conv4=nn.Conv2d(1,1,1,1)
        

        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(2048).to(device)
        
        #self.grpNorm=nn.GroupNorm(1, 2*20)
        
        self.l1=nn.Linear(4*500*20, 1)
        self.l2=nn.Linear(400,1)
        self.l3=nn.Linear(2000,1)
        self.l4=nn.Linear(1000,500)
        self.l5=nn.Linear(500,10)
        self.l6=nn.Linear(10,1)
  
       
    def forward(self, x):
        print(x.shape)
        xl1=torch.unsqueeze(x[:,0:500,:],1)
        print(xl1.shape,"xl1")
        o1=self.conv1(xl1)
        print(o1.shape,"o1 shape")
        xl2=torch.unsqueeze(x[:,500:1000,:],1)
        print(xl2.shape,"xl2")
        o2=self.conv2(xl2)
        print(xl2.shape,o2.shape,"o2 shape")
        xl3=torch.unsqueeze(x[:,1000:1500,:],1)
        o3=self.conv3(xl3)
        
        xl4=torch.unsqueeze(x[:,1500:2000,:],1)
        o4=self.conv4(xl4)
        
        new=torch.cat((o1,o2,o3,o4),1)
        print(new.shape,"new shape")


        # xx1=torch.unsqueeze(x[:,1,:],1)
        # xx2=torch.unsqueeze(x[:,2,:],1)
        


        #out=self.batchNorm(a)
        # out=self.conv1(xx1)
        # #out=torch.nn.MaxPool1d(2)(out)
        # #out=torch.nn.AvgPool1d(2)(out)

        # out1=self.conv2(xx2)
       
        # #print(new.shape)
        # out=self.conv3(new)
        # out=self.relu(out)
        # #out=torch.nn.MaxPool1d(2)(out) 

        # #out=self.conv3(out)
        # #out=torch.nn.MaxPool1d(2)(out)
        
        out=nn.Flatten(start_dim=1)(new)
        print(out.shape,"flatten")
        #out=self.grpNorm(out)  ##out=nn.GroupNorm(1, 516)(out)  # put 516 channels in  1 groupand normalize it as one piece
        
        out=self.l1(out)

        #out=self.l2(out)
        #out=self.relu(out)
        #out=self.l3(out)
        #out=self.l4(out)
        #out=self.relu(out)
        #out=self.l5(out)
        #out=self.l6(out)
        return out

model=LSTM(20,20).to(device)  # len of sample at 1 timeseries
summary(model, (1,2048, 20))  # sample, timeseries, len of one timeseries

"""## Model Testing and Prediction"""

print("stage: Pytorch model formation done! ")

loss_fn=torch.nn.L1Loss()
#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.manual_seed(41)
criterion = nn.L1Loss()

#dataset = ConcatDataset([train_dataloader, test_dataloader])
num_epochs = 10
batch_size = 32
k = 10
# splits = KFold(n_splits = k, shuffle = True, random_state = 41)
# foldperf={}
def show_metrics(y_test, y_predict):
    from sklearn.metrics import mean_absolute_error as mae
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()
    pass


def train_epoch(model,start_index,end_index,device,dataloader,loss_fn,optimizer,epochs=1):
    train_loss=[]
    val0dation_loss=[]
    model.train().to(device)
    for epoch in range(epochs):
        counter_loss=0
        for i,(eeg_scan, age) in enumerate(dataloader,0): # start from zero
            model.train().to(device)
            eeg_scan, age0 = eeg_scan.to(device), age.to(device)
            # make 0 gradient initially for optimization
            optimizer.zero_grad()  # gradient : slope
            #print(eeg_scan.shape)
            age=(age0-torch.mean(age0))/(torch.var(age0)).to(device)
            age=age0
            output = model(eeg_scan[:,start_index:end_index,:])
            loss = loss_fn(output,age)
            loss.backward()  #  calculating gradients ,update weights to input 
            optimizer.step()  # move in direction of gradient to get minima
            counter_loss+=loss.item()
            if i%10==0:
                val_loss=valid_epoch(start_index,end_index)
                print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/10,"Validation loss after 20*32 batch is:  ",val_loss)
                train_loss.append(counter_loss/10)
                val0dation_loss.append(val_loss)
                counter_loss=0
            #break
    print(loss.item())
    plt.plot(val0dation_loss)
    plt.plot(train_loss)
    plt.legend(["val-loss","train-loss"])
    pass

@torch.no_grad()
def valid_epoch(start_index,end_index):
    valid_loss= 0.0
    model.eval()
    count=0
    for i, (x,y) in enumerate(validation_loader):
        eeg_scan, age = x.to(device), y.to(device)
        output = model(eeg_scan[:,start_index:end_index,:])
        loss = loss_fn(output,age)
        valid_loss += loss.item()
        #break
        count=i+1
    return valid_loss/count


@torch.no_grad()
def predict( model,start_index,end_index):
    ypred=[]
    ytest=[]
    model.eval()  
    for x,y in test_loader:
        yhat = model(x[:,start_index:end_index,:] )
        # retrieve numpy array
        yhat = yhat.detach().cpu().numpy()
     #   check=[i[0] for i in yhat[0]]
        ypred.extend(yhat)
        #print(ypred.shape)
        ytest.extend(y.cpu().numpy())
        #break
   
    # out=[]
    # for i in ypred[0]:
    #     out.append(i.tolist()[0])
    # show_metrics(np.array(ytest),np.array(out))
    return ytest,ypred
    #pass

train_epoch(model,0, 2048,device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer)

train_epoch(model,0, 2048,device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer)
ytest,ypred=predict(model,0,2048)
out=[]
# for i in ypred:
#   for j in i:
#     #print(j[0])
#     out.append(j[0])
for i in ypred:
  #print(i)
  out.append(i[0])
  #break
show_metrics(np.array(ytest), np.array(out))

plt.plot(out,".")
plt.plot(ytest,".")
plt.legend(["pred","real"])

new=[]
for i in range(len(out)):
  new.append(ytest[i]-out[i])#np.mean(ytest))
np.var(new)

import numpy as np
np.random.rand(2,5)

"""# Synthetic testing

## 1- libraries prep
"""

from google.colab import drive
drive.mount("/content/gdrive/")

import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
!pip install mne
import mne
# Ignore warnings
import warnings

!pip install torchinfo
import  torchinfo
from torchinfo import summary
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import torchsummary
from torch.autograd import Variable
import random
import tensorflow as tf
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
from keras.metrics import mean_absolute_error  as mae
from sklearn.metrics import explained_variance_score
print("stage: libraries importing done")


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

X_path =  '/content/gdrive/MyDrive/mitacs SFU 2022/data/syntetic/Synthetic_Data_noise_added.npy'
Y_path =  '/content/gdrive/MyDrive/mitacs SFU 2022/data/syntetic/Synthetic_Age.npy'
xdata=np.load(X_path,allow_pickle=True)
ydata=np.load(Y_path,allow_pickle=True)


#NPY_CustomEEGData : use it  when you load data as 
class NPY_CustomEEGData:
    def __init__(self,x,y,device=device):
      """
      inputs:
      x: npy araay of all edf outputs
      y: age of all data in  npy array in x 
      """
      self.x = torch.from_numpy(np.asarray(x ,dtype=np.float32))#.double()
      self.y = torch.from_numpy(np.asarray(y,dtype=np.float32))#.double()
      self.device=device
        
    def __len__(self):
      return len(self.x)

    def __getitem__(self, idx):
      """
      output: X and Y

      """

      #xdata=np.transpose(self.x[idx],(1,0))
      xdata=self.x[idx]
      return xdata.to(self.device),self.y[idx].to(self.device)
dataset=NPY_CustomEEGData(np.stack(xdata,0), ydata)
xx1=dataset.__getitem__(10)


validation_split = .2
shuffle_dataset = True
dataset_size = len(dataset)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))
np.random.shuffle(indices)

train_indices, val_indices,test_indices = indices[split:], indices[:int(split/2)],indices[int(split/2):split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)
test_sampler= SubsetRandomSampler(test_indices)
batch_size=32
train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,sampler=valid_sampler)
test_loader= torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)
del xdata
del ydata
del indices
del dataset_size
del split

for x,y in train_loader:
  print(x.shape)
  break
import torch
torch.cuda.empty_cache()
import gc
gc.collect()

"""## model prep

### model0:  channel wise
"""

# model input n,2048,20
#lstm model channel wise




class LSTM(nn.Module):
     # only lstm network
    def __init__(self, input_size, hidden_size, num_layers=1):
        super(LSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        
        self.conv1=nn.Conv1d(1,1,1)
        self.conv2=nn.Conv1d(1,1,1)
        self.conv3=nn.Conv1d(2,2,1)
        self.conv4=nn.Conv1d(500,500,3)
        self.conv1=nn.Conv1d(1,1,1)

        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(2048).to(device)
        
        #self.grpNorm=nn.GroupNorm(1, 2*20)
        
        self.l1=nn.Linear(4*20, 1)
        self.l2=nn.Linear(400,1)
        self.l3=nn.Linear(2000,1)
        self.l4=nn.Linear(1000,500)
        self.l5=nn.Linear(500,10)
        self.l6=nn.Linear(10,1)
  
       
    def forward(self, x):
        xl1=x[:,0:500,:]
        #print(xl1.shape,"xl1 shape")

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(xl1,(h1,c1))
        #print(a1.shape,"a1 shape")
        
        xl2=x[:,500:1000,:]
        #print(xl2.shape,"xl2 shape")
        h2 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c2 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],-
        a2,(h2,c2)=self.lstm2(xl2,(h2,c2))
        #print(h2.shape,"h2 shape")

        xl3=x[:,1000:1500,:]
        #print(xl2.shape,"xl2 shape")
        h3 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c3 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],-
        a3,(h3,c3)=self.lstm3(xl3,(h3,c3))

        xl4=x[:,1500:2000,:]
        #print(xl2.shape,"xl2 shape")
        h4 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c4 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],-
        a4,(h4,c4)=self.lstm4(xl4,(h4,c4))

        new=torch.stack((h1,h2,h3,h4),2)
        #print(new.shape,"new shape")


        # xx1=torch.unsqueeze(x[:,1,:],1)
        # xx2=torch.unsqueeze(x[:,2,:],1)
        


        #out=self.batchNorm(a)
        # out=self.conv1(xx1)
        # #out=torch.nn.MaxPool1d(2)(out)
        # #out=torch.nn.AvgPool1d(2)(out)

        # out1=self.conv2(xx2)
       
        # #print(new.shape)
        # out=self.conv3(new)
        # out=self.relu(out)
        # #out=torch.nn.MaxPool1d(2)(out) 

        # #out=self.conv3(out)
        # #out=torch.nn.MaxPool1d(2)(out)
        
        out=nn.Flatten(start_dim=2)(new)
        #print(out.shape,"flatten")
        #out=self.grpNorm(out)  ##out=nn.GroupNorm(1, 516)(out)  # put 516 channels in  1 groupand normalize it as one piece
        
        out=self.l1(out)

        #out=self.l2(out)
        #out=self.relu(out)
        #out=self.l3(out)
        #out=self.l4(out)
        #out=self.relu(out)
        #out=self.l5(out)
        #out=self.l6(out)
        return out

model0=LSTM(20,20).to(device)  # len of sample at 1 timeseries
summary(model0, (1,2048, 20))  # sample, timeseries, len of one timeseries

"""### model999: lstm one lstm for 1 channel:"""

# model input n,2048,20  (batch,timeseries,sequence length) where sequence length is lenth of input at any time series point


class model999(nn.Module):
     # only lstm network
    def __init__(self, input_size, hidden_size, num_layers=1):
        super(model999, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size
        self.h_out=2
        self.lstm1 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm5 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm6 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm7 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm8 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm9 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm10 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)

        self.conv1=nn.Conv1d(4096,4096,3)
        self.conv2=nn.Conv1d(4096,2048,3)
        self.conv3=nn.Conv1d(2,2,1)
        self.conv4=nn.Conv1d(500,500,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(2048).to(device)
        
        self.l1=nn.Linear(4096*self.h_out*10, 10)
        self.l2=nn.Linear(500,200)
        self.l3=nn.Linear(200,100)
        self.l4=nn.Linear(10,1)
       
    def forward(self, x):
        
        hidden=self.h_out
        h1 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a11,(h11,c11)=self.lstm1(torch.unsqueeze(x[:,1,:], -1) ,(h1,c1))
        

        h2 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c2 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a22,(h22,c22)=self.lstm2(torch.unsqueeze(x[:,2,:], -1) ,(h2,c2))

        h3 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c3 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a33,(h33,c33)=self.lstm3(torch.unsqueeze(x[:,3,:], -1) ,(h3,c3))

        h4 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c4 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a44,(h44,c44)=self.lstm4(torch.unsqueeze(x[:,4,:], -1) ,(h4,c4))
      
        h5 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c5 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a55,(h55,c55)=self.lstm5(torch.unsqueeze(x[:,5,:], -1) ,(h5,c5))

        h6 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c6 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a66,(h66,c66)=self.lstm6(torch.unsqueeze(x[:,6,:], -1) ,(h6,c6))

        h7 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c7 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a77,(h77,c77)=self.lstm7(torch.unsqueeze(x[:,7,:], -1) ,(h7,c7))

        h8 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c8 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a88,(h88,c88)=self.lstm8(torch.unsqueeze(x[:,8,:], -1) ,(h8,c8))

        h9 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c9 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a99,(h99,c99)=self.lstm9(torch.unsqueeze(x[:,9,:], -1) ,(h9,c9))
        
        h10 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c10 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a_1010,(h_1010,c_1010)=self.lstm10(torch.unsqueeze(x[:,10,:], -1) ,(h10,c10))
       

        final=torch.stack((a11,a22,a33,a44,a55,a66,a77,a88,a99,a_1010),2)
        #print(final.shape,"rerev")
        
        
        # #print(a4.shape,"a4")
        # out=self.conv1(a4)
        # out=self.conv2(out)

        out = out=nn.Flatten(start_dim=1)(final)
        #print(out.shape,"uihlufvjxxg")
        out=self.l1(out)
        #out=self.l2(out)
        #out=self.l3(out)
        out=self.l4(out)
        out=torch.squeeze(out,1)
        return out

model999=model999(4096,2).to(device)  # len of sample at 1 timeseries
import torch
torch.cuda.empty_cache()
import gc
gc.collect()
summary(model999, (1,20,4096))  # sample, timeseries, len of one timeseries

"""### model 1: simple lstm 4 lstm layers, end of 5 sec output."""

# model input n,2048,20  (batch,timeseries,sequence length) where sequence length is lenth of input at any time series point


class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=50,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=10,num_layers=num_layers,batch_first=True).to(device)


        self.conv1=nn.Conv1d(4096,4096,3)
        self.conv2=nn.Conv1d(4096,2048,3)
        self.conv3=nn.Conv1d(2,2,1)
        self.conv4=nn.Conv1d(500,500,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(2048).to(device)
        
        self.l1=nn.Linear(6*2048, 2000)
        self.l2=nn.Linear(2000,2000)
        self.l3=nn.Linear(2000,100)
        self.l4=nn.Linear(100,1)
       
    def forward(self, x):

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        h2 = Variable(torch.zeros( 1, x.shape[0],50)).to(device) #x.shape[0],
        c2 = Variable(torch.zeros(1, x.shape[0],50)).to(device) #x.shape[0],-
        a2,(h2,c2)=self.lstm2(a1,(h2,c2))
        
        h3 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c3 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],-
        a3,(h3,c3)=self.lstm3(a2,(h3,c3))

        h4 = Variable(torch.zeros( 1, x.shape[0],10)).to(device) #x.shape[0],
        c4 = Variable(torch.zeros(1, x.shape[0],10)).to(device) #x.shape[0],-
        a4,(h4,c4)=self.lstm4(a3,(h4,c4))
        
        #print(a4.shape,"a4")
        out=self.conv1(a4)
        out=self.conv2(out)

        out = self.flat(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        return out

model1=simpleLSTM(20,20).to(device)  # len of sample at 1 timeseries
summary(model1, (1,4096, 20))  # sample, timeseries, len of one timeseries

"""### model 2: simple lstm with 1 lstm layer only 10 units output"""

# model input n,2048,20  (batch,timeseries,sequence length) where sequence length is lenth of input at any time series point


class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=50,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=10,num_layers=num_layers,batch_first=True).to(device)


        self.conv1=nn.Conv1d(4096,4096,3)
        self.conv2=nn.Conv1d(4096,2048,3)
        self.conv3=nn.Conv1d(2048,1024,3)
        self.conv4=nn.Conv1d(500,500,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(2048).to(device)
        
        self.l1=nn.Linear(4*1024, 2000)
        self.l2=nn.Linear(2000,2000)
        self.l3=nn.Linear(2000,100)
        self.l4=nn.Linear(100,1)
       
    def forward(self, x):

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        h2 = Variable(torch.zeros( 1, x.shape[0],50)).to(device) #x.shape[0],
        c2 = Variable(torch.zeros(1, x.shape[0],50)).to(device) #x.shape[0],-
        a2,(h2,c2)=self.lstm2(a1,(h2,c2))
        
        h3 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c3 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],-
        a3,(h3,c3)=self.lstm3(a2,(h3,c3))

        h4 = Variable(torch.zeros( 1, x.shape[0],10)).to(device) #x.shape[0],
        c4 = Variable(torch.zeros(1, x.shape[0],10)).to(device) #x.shape[0],-
        a4,(h4,c4)=self.lstm4(a3,(h4,c4))
        
        #print(a4.shape,"a4")
        out=self.conv1(a4)
        out=self.conv2(out)
        out=self.conv3(out)

        out = self.flat(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        return out

model2=simpleLSTM(20,20).to(device)  # len of sample at 1 timeseries
summary(model2, (1,4096, 20))  # sample, timeseries, len of one timeseries

"""### model 3: 1 lstm with 2 units out"""

# model input n,2048,20  (batch,timeseries,sequence length) where sequence length is lenth of input at any time series point


class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=5,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=50,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        n=4000
        self.conv1=nn.Conv1d(n,2100,3)
        self.conv2=nn.Conv1d(2100,1400,3)
        self.conv3=nn.Conv1d(1400,700,3)
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(2048).to(device)
        
        self.l1=nn.Linear(5*4000, 2000)
        self.l2=nn.Linear(2000,2000)
        self.l3=nn.Linear(2000,500)
        self.l4=nn.Linear(500,10)
        self.l5=nn.Linear(10,1)
       
    def forward(self, x):
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:4000,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],5)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],5)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        # h2 = Variable(torch.zeros( 1, x.shape[0],50)).to(device) #x.shape[0],
        # c2 = Variable(torch.zeros(1, x.shape[0],50)).to(device) #x.shape[0],-
        # a2,(h2,c2)=self.lstm2(a1,(h2,c2))
        
        # h3 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        # c3 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],-
        # a3,(h3,c3)=self.lstm3(a2,(h3,c3))

        # h4 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        # c4 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],-
        # a4,(h4,c4)=self.lstm4(a3,(h4,c4))
        
        #print(a4.shape,"a4")
        # out=self.conv1(a1)
        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)

        out = self.flat(a1)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        return out

model3=simpleLSTM(20,5).to(device)  # len of sample at 1 timeseries
summary(model3, (1,4096, 20))  # sample, timeseries, len of one timeseries

"""## 2- Pytorch model  testing formulation"""

print("stage: Pytorch model formation done! ")

loss_fn=torch.nn.L1Loss()
#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.manual_seed(41)
criterion = nn.L1Loss()

#dataset = ConcatDataset([train_dataloader, test_dataloader])
num_epochs = 10
batch_size = 32
k = 10
# splits = KFold(n_splits = k, shuffle = True, random_state = 41)
# foldperf={}
def show_metrics(y_test, y_predict):
    from sklearn.metrics import mean_absolute_error as mae
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()
    pass


def train_epoch(model,start_index,end_index,device,dataloader,loss_fn,optimizer,epochs=1,patience_iterations=4,previous_loss_index=0,index_range=3):
    """
    model: rnn model u r going to use 
    start index: index to get piece of data/slicing start
    end index: slicinf=g index end
    device : device for pu
    dataloader: training dataloader
    lossfn: loss fn
    epochs: no of epochs for each sample
    patience_iterations: wait till no of iterations when train loss is more than val loss; part for rarly stopping, after patience iterations exceed training will stop
    previous_loss_index: index where previois  val loss is less than training loss
    index_range: range  of index where once tarining greater is less than val loss and again tarining loss is less than validation loss and now currently now training loss is greater than validation loss
    """
    train_loss=[]
    val0dation_loss=[]
    model.train().to(device)
    counter_loss=0
    count=0 ## no of times val loss is more than training loss
    previous_loss_index=previous_loss_index
    patience_iterations=patience_iterations
    index_range=index_range # difference between previous index came when val loss is than trainingb loss

    sample_dcount=0
    mylist=[]
    for epoch in range(epochs):
        
        for i,(eeg_scan, age) in enumerate(dataloader,0): 
            
            sample_dcount+=1
            #model.train().to(device)
            eeg_scan, age0 = eeg_scan.to(device), age.to(device)
            # make 0 gradient initially for optimization
            optimizer.zero_grad()  # gradient : slope
            age=(age0-torch.mean(age0))/(torch.var(age0)).to(device)
            age=age0
            output = model(eeg_scan[:,start_index:end_index,:])
            loss = loss_fn(output,age)
            loss.backward()  #  calculating gradients ,update weights to input 
            optimizer.step()  # move in direction of gradient to get minima

            counter_loss+=loss.item()  
            val_loss=valid_epoch(model,start_index,end_index)
            copy=counter_loss
            n=20
            
            if i%n==0 and i!=0:
                print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1),"Validation loss after 20*32 batch is:  ",val_loss)
                train_loss.append(counter_loss/(n+1))
                val0dation_loss.append(val_loss)
                x=counter_loss
                counter_loss=0
                sample_dcount=0

        
        if  val_loss>counter_loss/(sample_dcount):
          if len(mylist)>patience_iterations:
            break
          else:
            if len(mylist)>0:
              if (epoch-mylist[-1])>index_range:
                mylist=[epoch]
              else:
                mylist.append(epoch)
            else:
              mylist=[epoch]

    plt.plot(val0dation_loss)
    plt.plot(train_loss)
    plt.legend(["val-loss","train-loss"])
    pass

@torch.no_grad()
def valid_epoch(model,start_index,end_index):
    valid_loss= 0.0
    #model.eval()
    count=0
    for i, (x,y) in enumerate(validation_loader):
        eeg_scan, age = x.to(device), y.to(device)
        output = model(eeg_scan[:,start_index:end_index,:])
        loss = loss_fn(output,age)
        valid_loss += loss.item()
        #break
        count=i+1
    return valid_loss/count


@torch.no_grad()
def predict( model,start_index=0,end_index=-1):
    ypred=[]
    ytest=[]
    model.eval()  
    for x,y in test_loader:
        yhat = model(x[:,start_index:end_index,:] )
        # retrieve numpy array
        yhat = yhat.detach().cpu().numpy()
        #check=[i[0] for i in yhat[0]]
        ypred.extend(yhat)
        #print(ypred.shape)
        ytest.extend(y.cpu().numpy())
        break
   
    # out=[]
    # for i in ypred[0]:
    #     out.append(i.tolist()[0])
    # show_metrics(np.array(ytest),np.array(out))
    return ytest,ypred
    #pass

"""## training"""

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0001)
train_epoch(model,0, 4096,device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=20)

"""## prediction"""

ytest,ypred=predict(model)
out=[]
# for i in ypred:
#   for j in i:
#     #print(j[0])
#     out.append(j[0])
for i in ypred:
  #print(i)
  out.append(i[0])
  #break
show_metrics(np.array(ytest), np.array(out))

plt.plot(out,".")
plt.plot(ytest,".")
plt.legend(["pred","real"])

"""## model 999 testing and prediction"""

model=model999
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0001)
train_epoch(model,0, 20,device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50)

ytest,ypred=predict(model)
out=[]
# for i in ypred:
#   for j in i:
#     #print(j[0])
#     out.append(j[0])
for i in ypred:
  #print(i)
  out.append(i)
#   #break
show_metrics(np.array(ytest), np.array(out))

"""## model 3 again"""

# testinggg @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=2,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(128,2))
        self.conv2=nn.Conv2d(1,1,(128,2))
        self.conv3=nn.Conv2d(1,1,(256,1))
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(18*3746, 4000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        b=self.batchNorm(a1)
        #print(b.shape)
        c=torch.unsqueeze(b,1)
        #print(c.shape)
        out=self.conv1(c)
        out=self.conv2(out)
        #out=self.conv3(out)

        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4000,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4000, 20))  # sample, timeseries, len of one timeseries
print(s)

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=3,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

# model input n,2048,20  (batch,timeseries,sequence length) where sequence length is lenth of input at any time series point


class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=5,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=50,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv1d(4000,2100,3)
        self.conv2=nn.Conv1d(2100,1400,3)
        self.conv3=nn.Conv1d(1400,700,3)
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(2048).to(device)
        
        self.l1=nn.Linear(5*self.n, 2000)
        self.l2=nn.Linear(2000,2000)
        self.l3=nn.Linear(2000,500)
        self.l4=nn.Linear(500,10)
        self.l5=nn.Linear(10,1)
       
    def forward(self, x):
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],5)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],5)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        # h2 = Variable(torch.zeros( 1, x.shape[0],50)).to(device) #x.shape[0],
        # c2 = Variable(torch.zeros(1, x.shape[0],50)).to(device) #x.shape[0],-
        # a2,(h2,c2)=self.lstm2(a1,(h2,c2))
        
        # out=self.conv1(a1)
        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)
        out = self.flat(a1)
        #print(out.shape)
        out=self.l1(out)
        #out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=1200,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
summary(model3, (1,4000, 20))  # sample, timeseries, len of one timeseries

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=10,previous_loss_index=0,index_range=2)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

# model input n,2048,20  (batch,timeseries,sequence length) where sequence length is lenth of input at any time series point


class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=5,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=50,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv1d(4000,2100,3)
        self.conv2=nn.Conv1d(2100,1400,3)
        self.conv3=nn.Conv1d(1400,700,3)
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l1=nn.Linear(5*self.n, 2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],5)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],5)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        b=self.batchNorm(a1)
        # h2 = Variable(torch.zeros( 1, x.shape[0],50)).to(device) #x.shape[0],
        # c2 = Variable(torch.zeros(1, x.shape[0],50)).to(device) #x.shape[0],-
        # a2,(h2,c2)=self.lstm2(a1,(h2,c2))
        
        # out=self.conv1(a1)
        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)
        out = self.flat(b)
        #print(out.shape)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4000,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
summary(model3, (1,4000, 20))  # sample, timeseries, len of one timeseries

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=10,previous_loss_index=0,index_range=2)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

#state of art model

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=5,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=50,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(128,1))
        self.conv2=nn.Conv1d(2100,1400,3)
        self.conv3=nn.Conv1d(1400,700,3)
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(5*3873, 4000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],5)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],5)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        b=self.batchNorm(a1)
        #print(b.shape)
        c=torch.unsqueeze(b,1)
        #print(c.shape)
        d=self.conv1(c)
        
        # out=self.conv1(a1)
        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(d)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4000,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4000, 20))  # sample, timeseries, len of one timeseries
print(s)

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=5,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

# testinggg @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=5,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=50,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(128,1))
        self.conv2=nn.Conv1d(2100,1400,3)
        self.conv3=nn.Conv1d(1400,700,3)
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(5*3873, 4000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],5)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],5)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        b=self.batchNorm(a1)
        #print(b.shape)
        c=torch.unsqueeze(b,1)
        #print(c.shape)
        d=self.conv1(c)
        
        # out=self.conv1(a1)
        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(d)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4000,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4000, 20))  # sample, timeseries, len of one timeseries
print(s)

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=10,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

# testinggg @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=5,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=50,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(128,1))
        self.conv2=nn.Conv2d(1,1,(128,1))
        self.conv3=nn.Conv1d(1400,700,3)
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(5*3746, 4000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],5)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],5)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        b=self.batchNorm(a1)
        #print(b.shape)
        c=torch.unsqueeze(b,1)
        #print(c.shape)
        out=self.conv1(c)
        out=self.conv2(out)

        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4000,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4000, 20))  # sample, timeseries, len of one timeseries
print(s)

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=10,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

# testinggg @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=2,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(128,2))
        self.conv2=nn.Conv2d(1,1,(128,2))
        self.conv3=nn.Conv2d(1,1,(256,1))
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(18*3746, 4000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        b=self.batchNorm(a1)
        #print(b.shape)
        c=torch.unsqueeze(b,1)
        #print(c.shape)
        out=self.conv1(c)
        out=self.conv2(out)
        #out=self.conv3(out)

        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4000,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4000, 20))  # sample, timeseries, len of one timeseries
print(s)

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=5,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

# testinggg @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=2,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(128,2))
        self.conv2=nn.Conv2d(1,1,(128,2))
        self.conv3=nn.Conv2d(1,1,(256,1))
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(18*3746, 4000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        b=self.batchNorm(a1)
        #print(b.shape)
        c=torch.unsqueeze(b,1)
        #print(c.shape)
        out=self.conv1(c)
        out=self.conv2(out)
        #out=self.conv3(out)

        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4000,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4000, 20))  # sample, timeseries, len of one timeseries
print(s)

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=5,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

"""# Future Model (try when get free time)"""

# model input n,2048,20  (batch,timeseries,sequence length) where sequence length is lenth of input at any time series point


class model999(nn.Module):
     # only lstm network
    def __init__(self, input_size, hidden_size, num_layers=1):
        super(model999, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size
        self.h_out=256

        self.lstm0 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm1 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm5 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm6 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm7 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm8 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm9 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm10 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm11 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm12 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm13 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm14 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm15 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm16 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm17 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm18 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)
        self.lstm19 = nn.LSTM(input_size=1, hidden_size=self.h_out,num_layers=num_layers,batch_first=True).to(device)

        self.conv1=nn.Conv2d(15,10,(128,32))
        self.conv2=nn.Conv2d(10,7,(256,32))
        self.conv3=nn.Conv2d(7,5,(512,32))
        self.conv4=nn.Conv2d(5,1,(1024,32))
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(2048).to(device)
        
        self.l1=nn.Linear(287760, 2000)
        self.l2=nn.Linear(2000,500)
        self.l3=nn.Linear(500,100)
        self.l4=nn.Linear(100,1)
       
    def forward(self, x):
        

        hidden=self.h_out

        h0 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c0 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a00,(h00,c00)=self.lstm0(torch.unsqueeze(x[:,0,:], -1) ,(h0,c0))

      
        h1 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a11,(h11,c11)=self.lstm1(torch.unsqueeze(x[:,1,:], -1) ,(h1,c1))
        

        h2 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c2 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a22,(h22,c22)=self.lstm2(torch.unsqueeze(x[:,2,:], -1) ,(h2,c2))

        h3 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c3 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a33,(h33,c33)=self.lstm3(torch.unsqueeze(x[:,3,:], -1) ,(h3,c3))

        h4 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c4 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a44,(h44,c44)=self.lstm4(torch.unsqueeze(x[:,4,:], -1) ,(h4,c4))
      
        h5 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c5 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a55,(h55,c55)=self.lstm5(torch.unsqueeze(x[:,5,:], -1) ,(h5,c5))

        h6 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c6 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a66,(h66,c66)=self.lstm6(torch.unsqueeze(x[:,6,:], -1) ,(h6,c6))

        h7 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c7 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a77,(h77,c77)=self.lstm7(torch.unsqueeze(x[:,7,:], -1) ,(h7,c7))

        h8 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c8 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a88,(h88,c88)=self.lstm8(torch.unsqueeze(x[:,8,:], -1) ,(h8,c8))

        h9 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c9 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a99,(h99,c99)=self.lstm9(torch.unsqueeze(x[:,9,:], -1) ,(h9,c9))
        
        h10 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c10 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a_1010,(h_1010,c_1010)=self.lstm10(torch.unsqueeze(x[:,10,:], -1) ,(h10,c10))

        h11 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c11 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a_1111,(h_1111,c_1111)=self.lstm11(torch.unsqueeze(x[:,11,:], -1) ,(h11,c11))

        h12 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c12 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a_1212,(h_1212,c_1212)=self.lstm12(torch.unsqueeze(x[:,12,:], -1) ,(h12,c12))

        h13 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c13 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a_1313,(h_1313,c_1313)=self.lstm13(torch.unsqueeze(x[:,13,:], -1) ,(h13,c13))

        h14 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c14 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a_1414,(h_1414,c_1414)=self.lstm14(torch.unsqueeze(x[:,14,:], -1) ,(h14,c14))

        h15 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c15 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a_1515,(h_1515,c_1515)=self.lstm15(torch.unsqueeze(x[:,15,:], -1) ,(h15,c15))

        h16 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c16 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a_1616,(h_1616,c_1616)=self.lstm16(torch.unsqueeze(x[:,16,:], -1) ,(h16,c16))

        h17 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c17 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a_1717,(h_1717,c_1717)=self.lstm17(torch.unsqueeze(x[:,17,:], -1) ,(h17,c17))

        h18 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c18 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a_1818,(h_1818,c_1818)=self.lstm18(torch.unsqueeze(x[:,18,:], -1) ,(h18,c18))

        h19 = Variable(torch.zeros( 1, x.shape[0],hidden)).to(device) #x.shape[0],
        c19 = Variable(torch.zeros(1, x.shape[0],hidden)).to(device) #x.shape[0],
        a_1919,(h_1919,c_1919)=self.lstm19(torch.unsqueeze(x[:,19,:], -1) ,(h19,c19))
       

        final=torch.stack((a00,a11,a22,a33,a44,a55,a66,a77,a88,a99,a_1010,a_1111,a_1212,a_1313,a_1414,a_1515,a_1616,a_1717,a_1818,a_1919),1)
        print(final.shape,"final shape")
        out=self.conv1(final)
        out=self.conv2(out)
        out=self.conv3(out)
        out=self.conv4(out)
        
        # #print(a4.shape,"a4")
        # out=self.conv1(a4)
        # out=self.conv2(out)

        out = out=nn.Flatten(start_dim=1)(out)
        print(out.shape,"flatten")
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=torch.squeeze(out,1)
        return out

model999=model999(4096,2).to(device)  # len of sample at 1 timeseries
import torch
torch.cuda.empty_cache()
import gc
gc.collect()
summary(model999, (1,20,4096))  # sample, timeseries, len of one timeseries

torch.save(model.state_dict(), "/content/gdrive/MyDrive/mitacs SFU 2022/data/syntetic/model_synthetic_corr_0.96_var_0.91")

"""# Testing July 28"""

from google.colab import drive
drive.mount("/content/gdrive/")

import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
!pip install mne
import mne
# Ignore warnings
import warnings

!pip install torchinfo
import  torchinfo
from torchinfo import summary
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import torchsummary
from torch.autograd import Variable
import random
import tensorflow as tf
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
from keras.metrics import mean_absolute_error  as mae
from sklearn.metrics import explained_variance_score
print("stage: libraries importing done")


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
X_pure="/content/gdrive/MyDrive/data/synthetic/2000_without_alpha.npy"
Y_pure="/content/gdrive/MyDrive/data/synthetic/Synthetic_2000_Age.npy"

X_0_9_path =  '/content/gdrive/MyDrive/data/synthetic/Synthetic_Data_noise_added_alpha_0.9_2000samples.npy'
XX_0_99_path=  "/content/gdrive/MyDrive/data/synthetic/Synthetic_Data_noise_added_alpha_0.99_2000samples.npy"
Y_path =  '/content/gdrive/MyDrive/data/synthetic/Synthetic_Age.npy'
xdata=np.load(X_pure,allow_pickle=True)
ydata=np.load(Y_pure,allow_pickle=True)


#NPY_CustomEEGData : use it  when you load data as 
class NPY_CustomEEGData:
    def __init__(self,x,y,device=device):
      """
      inputs:
      x: npy araay of all edf outputs
      y: age of all data in  npy array in x 
      """
      self.x = torch.from_numpy(np.asarray(x ,dtype=np.float32))#.double()
      self.y = torch.from_numpy(np.asarray(y,dtype=np.float32))#.double()
      self.device=device
        
    def __len__(self):
      return len(self.x)

    def __getitem__(self, idx):
      """
      output: X and Y

      """

      xdata=np.transpose(self.x[idx],(1,0))
      #xdata=self.x[idx]
      return xdata.to(self.device),self.y[idx].to(self.device)
dataset=NPY_CustomEEGData(np.stack(xdata,0), ydata)
xx1=dataset.__getitem__(10)


validation_split = .5
shuffle_dataset = True
dataset_size = len(dataset)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))
np.random.shuffle(indices)

train_indices, val_indices,test_indices = indices[split:], indices[:int(split/5)],indices[int(split/5):split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)
test_sampler= SubsetRandomSampler(test_indices)

batch_size=8

train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,sampler=valid_sampler)
test_loader= torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)
del xdata
del ydata
del indices
del dataset_size
del split

for x,y in train_loader:
  print(x.shape)
  break
import torch
torch.cuda.empty_cache()
import gc
gc.collect()

print("stage: Pytorch model formation done! ")

loss_fn=torch.nn.L1Loss()
#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.manual_seed(41)
criterion = nn.L1Loss()

#dataset = ConcatDataset([train_dataloader, test_dataloader])
num_epochs = 10
batch_size = 32
k = 10
# splits = KFold(n_splits = k, shuffle = True, random_state = 41)
# foldperf={}
def show_metrics(y_test, y_predict):
    from sklearn.metrics import mean_absolute_error as mae
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()
    pass


def train_epoch(model,start_index,end_index,device,dataloader,loss_fn,optimizer,epochs=1,patience_iterations=4,previous_loss_index=0,index_range=3):
    """
    model: rnn model u r going to use 
    start index: index to get piece of data/slicing start
    end index: slicinf=g index end
    device : device for pu
    dataloader: training dataloader
    lossfn: loss fn
    epochs: no of epochs for each sample
    patience_iterations: wait till no of iterations when train loss is more than val loss; part for rarly stopping, after patience iterations exceed training will stop
    previous_loss_index: index where previois  val loss is less than training loss
    index_range: range  of index where once tarining greater is less than val loss and again tarining loss is less than validation loss and now currently now training loss is greater than validation loss
    """
    train_loss=[]
    val0dation_loss=[]
    model.train().to(device)
    counter_loss=0
    count=0 ## no of times val loss is more than training loss
    previous_loss_index=previous_loss_index
    patience_iterations=patience_iterations
    index_range=index_range # difference between previous index came when val loss is than trainingb loss

    sample_dcount=0
    mylist=[]
    should_break=0
    for epoch in range(epochs):
        
        for i,(eeg_scan, age) in enumerate(dataloader,0): 
            
            sample_dcount+=1
            #model.train().to(device)
            eeg_scan, age0 = eeg_scan.to(device), age.to(device)
            # make 0 gradient initially for optimization
            optimizer.zero_grad()  # gradient : slope
            age=(age0-torch.mean(age0))/(torch.var(age0)).to(device)
            age=age0
            output = model(eeg_scan[:,start_index:end_index,:])
            loss = loss_fn(output,age)
            loss.backward()  #  calculating gradients ,update weights to input 
            optimizer.step()  # move in direction of gradient to get minima

            counter_loss+=loss.item()  
            val_loss=valid_epoch(model,start_index,end_index)
          
            copy=counter_loss
            n=20
            
            if i%n==0 and i!=0:
                print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1),"Validation loss after 20*32 batch is:  ",val_loss)
                #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
                train_loss.append(counter_loss/(n+1))
                val0dation_loss.append(val_loss)
                x=counter_loss
                counter_loss=0
                sample_dcount=0
          # try:
          #   np.mean(val0dation_loss[index_range:])>np.mean(train_loss[index_range:])
          #   should_break+=1
          # except:
          #   pass
        # if should_break>=patience_iterations:
        #   break

        # #continue
        if  val_loss>counter_loss/(sample_dcount):
          if len(mylist)>patience_iterations:
            break
          else:
            if len(mylist)>0:
              if (epoch-mylist[-1])>index_range:
                mylist=[epoch]
              else:
                mylist.append(epoch)
            else:
              mylist=[epoch]

    plt.plot(val0dation_loss)
    plt.plot(train_loss)
    plt.legend(["val_loss","train-loss"])
    pass

@torch.no_grad()
def valid_epoch(model,start_index,end_index):
    valid_loss= 0.0
    #model.eval()
    count=0
    for i, (x,y) in enumerate(validation_loader):
        eeg_scan, age = x.to(device), y.to(device)
        output = model(eeg_scan[:,start_index:end_index,:])
        loss = loss_fn(output,age)
        valid_loss += loss.item()
        count=i+1
    return valid_loss/count


@torch.no_grad()
def predict( model,start_index=0,end_index=-1):
    ypred=[]
    ytest=[]
    model.eval()  
    for x,y in test_loader:
        yhat = model(x[:,start_index:end_index,:] )
        # retrieve numpy array
        yhat = yhat.detach().cpu().numpy()
        #check=[i[0] for i in yhat[0]]
        ypred.extend(yhat)
        #print(ypred.shape)
        ytest.extend(y.cpu().numpy())
   
    # out=[]
    # for i in ypred[0]:
    #     out.append(i.tolist()[0])
    # show_metrics(np.array(ytest),np.array(out))
    return ytest,ypred

import torch, gc
gc.collect()
torch.cuda.empty_cache()

"""## testing"""

# best accuracy for 2000 sample

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=2,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(128,2))
        self.conv2=nn.Conv2d(1,1,(128,2))
        self.conv3=nn.Conv2d(1,1,(256,1))
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(18*3746, 4000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        b=self.batchNorm(a1)
        #print(b.shape)
        c=torch.unsqueeze(b,1)
        #print(c.shape)
        out=self.conv1(c)
        out=self.conv2(out)
        #out=self.conv3(out)

        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4000,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4000, 20))  # sample, timeseries, len of one timeseries
print(s)

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=3,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=2,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

# best accuracy for 2000 sample

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=2,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(128,2))
        self.conv2=nn.Conv2d(1,1,(128,2))
        self.conv3=nn.Conv2d(1,1,(256,1))
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(18*3746, 4000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        b=self.batchNorm(a1)
        #print(b.shape)
        c=torch.unsqueeze(b,1)
        #print(c.shape)
        out=self.conv1(c)
        out=self.conv2(out)
        #out=self.conv3(out)

        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4000,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4000, 20))  # sample, timeseries, len of one timeseries
print(s)

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=2,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

"""# Excellence --trial 3"""

from google.colab import drive
drive.mount("/content/gdrive/")

import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
!pip install mne
import mne
# Ignore warnings
import warnings

!pip install torchinfo
import  torchinfo
from torchinfo import summary
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import torchsummary
from torch.autograd import Variable
import random
import tensorflow as tf
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
from keras.metrics import mean_absolute_error  as mae
from sklearn.metrics import explained_variance_score
print("stage: libraries importing done")


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
X_pure="/content/gdrive/MyDrive/data/synthetic/2000_without_alpha.npy"
Y_pure="/content/gdrive/MyDrive/data/synthetic/Synthetic_2000_Age.npy"

X_0_9_path =  '/content/gdrive/MyDrive/data/synthetic/Synthetic_Data_noise_added_alpha_0.9_2000samples.npy'
XX_0_99_path=  "/content/gdrive/MyDrive/data/synthetic/Synthetic_Data_noise_added_alpha_0.99_2000samples.npy"
Y_path =  '/content/gdrive/MyDrive/data/synthetic/Synthetic_Age.npy'
xdata=np.load(X_pure,allow_pickle=True)
ydata=np.load(Y_pure,allow_pickle=True)


#NPY_CustomEEGData : use it  when you load data as 
class NPY_CustomEEGData:
    def __init__(self,x,y,device=device):
      """
      inputs:
      x: npy araay of all edf outputs
      y: age of all data in  npy array in x 
      """
      self.x = torch.from_numpy(np.asarray(x ,dtype=np.float32))#.double()
      self.y = torch.from_numpy(np.asarray(y,dtype=np.float32))#.double()
      self.device=device
        
    def __len__(self):
      return len(self.x)

    def __getitem__(self, idx):
      """
      output: X and Y

      """

      xdata=np.transpose(self.x[idx],(1,0))
      #xdata=self.x[idx]
      return xdata.to(self.device),self.y[idx].to(self.device)
dataset=NPY_CustomEEGData(np.stack(xdata,0), ydata)
xx1=dataset.__getitem__(10)


validation_split = .5
shuffle_dataset = True
dataset_size = len(dataset)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))
np.random.shuffle(indices)

train_indices, val_indices,test_indices = indices[split:], indices[:int(split/5)],indices[int(split/5):split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)
test_sampler= SubsetRandomSampler(test_indices)

batch_size=32

train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,sampler=valid_sampler)
test_loader= torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)
del xdata
del ydata
del indices
del dataset_size
del split

for x,y in train_loader:
  print(x.shape)
  break
import torch
torch.cuda.empty_cache()
import gc
gc.collect()

print("stage: Pytorch model formation done! ")

loss_fn=torch.nn.L1Loss()
#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.manual_seed(41)
criterion = nn.L1Loss()

#dataset = ConcatDataset([train_dataloader, test_dataloader])
num_epochs = 10
batch_size = 32
k = 10
# splits = KFold(n_splits = k, shuffle = True, random_state = 41)
# foldperf={}
def show_metrics(y_test, y_predict):
    from sklearn.metrics import mean_absolute_error as mae
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()
    pass


def train_epoch(model,start_index,end_index,device,dataloader,loss_fn,optimizer,epochs=1,patience_iterations=4,previous_loss_index=0,index_range=3):
    """
    model: rnn model u r going to use 
    start index: index to get piece of data/slicing start
    end index: slicinf=g index end
    device : device for pu
    dataloader: training dataloader
    lossfn: loss fn
    epochs: no of epochs for each sample
    patience_iterations: wait till no of iterations when train loss is more than val loss; part for rarly stopping, after patience iterations exceed training will stop
    previous_loss_index: index where previois  val loss is less than training loss
    index_range: range  of index where once tarining greater is less than val loss and again tarining loss is less than validation loss and now currently now training loss is greater than validation loss
    """
    train_loss=[]
    val0dation_loss=[]
    model.train().to(device)
    counter_loss=0
    count=0 ## no of times val loss is more than training loss
    previous_loss_index=previous_loss_index
    patience_iterations=patience_iterations
    index_range=index_range # difference between previous index came when val loss is than trainingb loss

    sample_dcount=0
    mylist=[]
    should_break=0
    for epoch in range(epochs):
        
        for i,(eeg_scan, age) in enumerate(dataloader,0): 
            
            sample_dcount+=1
            #model.train().to(device)
            eeg_scan, age0 = eeg_scan.to(device), age.to(device)
            # make 0 gradient initially for optimization
            optimizer.zero_grad()  # gradient : slope
            age=(age0-torch.mean(age0))/(torch.var(age0)).to(device)
            age=age0
            output = model(eeg_scan[:,start_index:end_index,:])
            loss = loss_fn(output,age)
            loss.backward()  #  calculating gradients ,update weights to input 
            optimizer.step()  # move in direction of gradient to get minima

            counter_loss+=loss.item()  
            #val_loss=valid_epoch(model,start_index,end_index)
          
            copy=counter_loss
            n=20
            ii=i
        
        val_loss=valid_epoch(model,start_index,end_index)
        print("epoch: ",epoch," traing loss after 1 batch is:  ",counter_loss/(ii+1),"Validation loss after 20*32 batch is:  ",val_loss)
                #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
        train_loss.append(counter_loss/(ii+1))
        val0dation_loss.append(val_loss)
        x=counter_loss
        counter_loss=0
                #sample_dcount=0
            
          #   if i%n==0 and i!=0:
          #       print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1),"Validation loss after 20*32 batch is:  ",val_loss)
          #       #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
          #       train_loss.append(counter_loss/(n+1))
          #       val0dation_loss.append(val_loss)
          #       x=counter_loss
          #       counter_loss=0
          #       sample_dcount=0
          # # try:
          #   np.mean(val0dation_loss[index_range:])>np.mean(train_loss[index_range:])
          #   should_break+=1
          # except:
          #   pass
        # if should_break>=patience_iterations:
        #   break

        # #continue
        if  val_loss>counter_loss/(ii+1):
          if len(mylist)>patience_iterations:
            break
          else:
            if len(mylist)>0:
              if (epoch-mylist[-1])>index_range:
                mylist=[epoch]
              else:
                mylist.append(epoch)
            else:
              mylist=[epoch]

    plt.plot(val0dation_loss)
    plt.plot(train_loss)
    plt.legend(["val_loss","train-loss"])
    pass

@torch.no_grad()
def valid_epoch(model,start_index,end_index):
    valid_loss= 0.0
    #model.eval()
    count=0
    for i, (x,y) in enumerate(validation_loader):
        eeg_scan, age = x.to(device), y.to(device)
        output = model(eeg_scan[:,start_index:end_index,:])
        loss = loss_fn(output,age)
        valid_loss += loss.item()
        count=i+1
    return valid_loss/count


@torch.no_grad()
def predict( model,start_index=0,end_index=-1):
    ypred=[]
    ytest=[]
    model.eval()  
    for x,y in test_loader:
        yhat = model(x)
        # retrieve numpy array
        yhat = yhat.detach().cpu().numpy()
        #check=[i[0] for i in yhat[0]]
        ypred.extend(yhat)
        #print(ypred.shape)
        ytest.extend(y.cpu().numpy())
   
    # out=[]
    # for i in ypred[0]:
    #     out.append(i.tolist()[0])
    # show_metrics(np.array(ytest),np.array(out))
    return ytest,ypred

import torch, gc
gc.collect()
torch.cuda.empty_cache()

"""## testing"""

# best accuracy for 2000 sample

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=2,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(16,2))
        self.conv2=nn.Conv2d(1,1,(32,2))
        self.conv3=nn.Conv2d(1,1,(256,1))
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(506*18, 4000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        b=self.batchNorm(a1)
        #print(b.shape)
        c=torch.unsqueeze(b,1)
        #print(c.shape)
        out=self.conv1(c)
        out=self.conv2(out)
        out=torch.nn.MaxPool2d((8,1))(out)
        #print(out.shape)
        #out=self.conv3(out)

        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4096,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4096, 20))  # sample, timeseries, len of one timeseries
print(s)

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=50,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

# best accuracy for 2000 sample

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=2,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(16,2))
        self.conv2=nn.Conv2d(1,1,(32,2))
        self.conv3=nn.Conv2d(1,1,(256,1))
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(506*18, 4000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        b=self.batchNorm(a1)
        #print(b.shape)
        c=torch.unsqueeze(b,1)
        #print(c.shape)
        out=self.conv1(c)
        out=self.conv2(out)
        out=torch.nn.MaxPool2d((8,1))(out)
        #print(out.shape)
        #out=self.conv3(out)

        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4096,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4096, 20))  # sample, timeseries, len of one timeseries
print(s)

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=45,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

# for 90% noise data

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=40,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=2,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv0=nn.Conv2d(1,1,(8,2))
        self.conv1=nn.Conv2d(1,1,(16,2))
        self.conv2=nn.Conv2d(1,1,(32,2))
        self.conv3=nn.Conv2d(1,1,(64,2))
        self.conv4=nn.Conv2d(1,1,(128,2))
        self.conv5=nn.Conv2d(1,1,(256,2))
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(141*34, 4000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,50)
        self.l7=nn.Linear(50,10)
        self.l8=nn.Linear(10,1)
       
    def forward(self, x):
        x=x[:,0:self.n,:]
        
        h1 = Variable(torch.zeros( 1, x.shape[0],40)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],40)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        b=self.batchNorm(a1)
    
        c=torch.unsqueeze(b,1)
        
        out=self.conv0(c)
        out=self.conv1(out)
        out=self.conv2(out)
        out=torch.nn.MaxPool2d((4,1))(out)
        out=self.conv3(out)
       
        out=self.conv4(out)
        out=self.conv5(out)
        out=torch.nn.MaxPool2d((4,1))(out)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)
        out=self.l8(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4096,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4096, 20))  # sample, timeseries, len of one timeseries
print(s)

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=30,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

"""# Excellence trial 4 with noise data"""

from google.colab import drive
drive.mount("/content/gdrive/")

import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
!pip install mne
import mne
# Ignore warnings
import warnings

!pip install torchinfo
import  torchinfo
from torchinfo import summary
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import torchsummary
from torch.autograd import Variable
import random
import tensorflow as tf
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
from keras.metrics import mean_absolute_error  as mae
from sklearn.metrics import explained_variance_score
print("stage: libraries importing done")


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
X_pure="/content/gdrive/MyDrive/mitacs 2023/2000_without_alpha.npy"
Y_pure="/content/gdrive/MyDrive/mitacs 2023/Synthetic_2000_Age.npy"

X_0_9_path =  '/content/gdrive/MyDrive/mitacs 2023/Synthetic_Data_noise_added_alpha_0.9_2000samples.npy'
XX_0_99_path=  "/content/gdrive/MyDrive/mitacs 2023/Synthetic_Data_noise_added_alpha_0.99_2000samples.npy"

Y_path =  '/content/gdrive/MyDrive/mitacs 2023/Synthetic_Age.npy'
xdata=np.load(X_0_9_path,allow_pickle=True)
ydata=np.load(Y_path,allow_pickle=True)


#NPY_CustomEEGData : use it  when you load data as 
class NPY_CustomEEGData:
    def __init__(self,x,y,device=device):
      """
      inputs:
      x: npy araay of all edf outputs
      y: age of all data in  npy array in x 
      """
      self.x = torch.from_numpy(np.asarray(x ,dtype=np.float32))#.double()
      self.y = torch.from_numpy(np.asarray(y,dtype=np.float32))#.double()
      self.device=device
        
    def __len__(self):
      return len(self.x)

    def __getitem__(self, idx):
      """
      output: X and Y

      """

      xdata=np.transpose(self.x[idx],(1,0))
      #xdata=self.x[idx]
      return xdata.to(self.device),self.y[idx].to(self.device)
dataset=NPY_CustomEEGData(np.stack(xdata,0), ydata)
xx1=dataset.__getitem__(10)


validation_split = .5
shuffle_dataset = True
dataset_size = len(dataset)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))
np.random.shuffle(indices)

train_indices, val_indices,test_indices = indices[split:], indices[:int(split/5)],indices[int(split/5):split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)
test_sampler= SubsetRandomSampler(test_indices)

batch_size=32

train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,sampler=valid_sampler)
test_loader= torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)
del xdata
del ydata
del indices
del dataset_size
del split

for x,y in train_loader:
  print(x.shape)
  break
import torch
torch.cuda.empty_cache()
import gc
gc.collect()

print("stage: Pytorch model formation done! ")

loss_fn=torch.nn.L1Loss()
#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.manual_seed(41)
criterion = nn.L1Loss()

#dataset = ConcatDataset([train_dataloader, test_dataloader])
num_epochs = 10
batch_size = 32
k = 10
# splits = KFold(n_splits = k, shuffle = True, random_state = 41)
# foldperf={}
def show_metrics(y_test, y_predict):
    from sklearn.metrics import mean_absolute_error as mae
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()
    pass


def train_epoch(model,start_index,end_index,device,dataloader,loss_fn,optimizer,epochs=1,patience_iterations=4,previous_loss_index=0,index_range=3):
    """
    model: rnn model u r going to use 
    start index: index to get piece of data/slicing start
    end index: slicinf=g index end
    device : device for pu
    dataloader: training dataloader
    lossfn: loss fn
    epochs: no of epochs for each sample
    patience_iterations: wait till no of iterations when train loss is more than val loss; part for rarly stopping, after patience iterations exceed training will stop
    previous_loss_index: index where previois  val loss is less than training loss
    index_range: range  of index where once tarining greater is less than val loss and again tarining loss is less than validation loss and now currently now training loss is greater than validation loss
    """
    train_loss=[]
    val0dation_loss=[]
    model.train().to(device)
    counter_loss=0
    count=0 ## no of times val loss is more than training loss
    previous_loss_index=previous_loss_index
    patience_iterations=patience_iterations
    index_range=index_range # difference between previous index came when val loss is than trainingb loss

    sample_dcount=0
    mylist=[]
    should_break=0
    for epoch in range(epochs):
        
        for i,(eeg_scan, age) in enumerate(dataloader,0): 
            
            sample_dcount+=1
            #model.train().to(device)
            eeg_scan, age0 = eeg_scan.to(device), age.to(device)
            # make 0 gradient initially for optimization
            optimizer.zero_grad()  # gradient : slope
            age=(age0-torch.mean(age0))/(torch.var(age0)).to(device)
            age=age0
            output = model(eeg_scan[:,start_index:end_index,:])
            loss = loss_fn(output,age)
            loss.backward()  #  calculating gradients ,update weights to input 
            optimizer.step()  # move in direction of gradient to get minima

            counter_loss+=loss.item()  
            #val_loss=valid_epoch(model,start_index,end_index)
          
            copy=counter_loss
            n=20
            ii=i
        
        val_loss=valid_epoch(model,start_index,end_index)
        print("epoch: ",epoch," traing loss after 1 batch is:  ",counter_loss/(ii+1),"Validation loss after 20*32 batch is:  ",val_loss)
                #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
        train_loss.append(counter_loss/(ii+1))
        val0dation_loss.append(val_loss)
        x=counter_loss
        counter_loss=0
                #sample_dcount=0
            
          #   if i%n==0 and i!=0:
          #       print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1),"Validation loss after 20*32 batch is:  ",val_loss)
          #       #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
          #       train_loss.append(counter_loss/(n+1))
          #       val0dation_loss.append(val_loss)
          #       x=counter_loss
          #       counter_loss=0
          #       sample_dcount=0
          # # try:
          #   np.mean(val0dation_loss[index_range:])>np.mean(train_loss[index_range:])
          #   should_break+=1
          # except:
          #   pass
        # if should_break>=patience_iterations:
        #   break

        # #continue
        if  val_loss>counter_loss/(ii+1):
          if len(mylist)>patience_iterations:
            break
          else:
            if len(mylist)>0:
              if (epoch-mylist[-1])>index_range:
                mylist=[epoch]
              else:
                mylist.append(epoch)
            else:
              mylist=[epoch]

    plt.plot(val0dation_loss)
    plt.plot(train_loss)
    plt.legend(["val_loss","train-loss"])
    pass

@torch.no_grad()
def valid_epoch(model,start_index,end_index):
    valid_loss= 0.0
    #model.eval()
    count=0
    for i, (x,y) in enumerate(validation_loader):
        eeg_scan, age = x.to(device), y.to(device)
        output = model(eeg_scan[:,start_index:end_index,:])
        loss = loss_fn(output,age)
        valid_loss += loss.item()
        count=i+1
    return valid_loss/count


@torch.no_grad()
def predict( model,start_index=0,end_index=-1):
    ypred=[]
    ytest=[]
    model.eval()  
    for x,y in test_loader:
        yhat = model(x)
        # retrieve numpy array
        yhat = yhat.detach().cpu().numpy()
        #check=[i[0] for i in yhat[0]]
        ypred.extend(yhat)
        #print(ypred.shape)
        ytest.extend(y.cpu().numpy())
   
    # out=[]
    # for i in ypred[0]:
    #     out.append(i.tolist()[0])
    # show_metrics(np.array(ytest),np.array(out))
    return ytest,ypred

import torch, gc
gc.collect()
torch.cuda.empty_cache()

"""## testing"""

# for 90% noise data

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=100,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=2,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv0=nn.Conv2d(1,1,(4,1))
        self.conv1=nn.Conv2d(1,1,(8,3))
        self.conv2=nn.Conv2d(1,1,(16,5))
        self.conv3=nn.Conv2d(1,1,(32,7))
        self.conv4=nn.Conv2d(1,1,(64,9))
        self.conv5=nn.Conv2d(1,1,(128,11))
        self.conv6=nn.Conv2d(1,1,(256,13))
        self.conv7=nn.Conv2d(1,1,(512,15))
        self.conv8=nn.Conv2d(1,1,(1024,17))
        self.conv9=nn.Conv2d(1,1,(2048,19))
        self.conv10=nn.Conv2d(1,1,(2048,21))
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(2061*28, 3000)
        self.l1=nn.Linear(3000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,50)
        self.l7=nn.Linear(50,10)
        self.l8=nn.Linear(10,1)
       
    def forward(self, x):
        x=x[:,0:self.n,:]
        
        h1 = Variable(torch.zeros( 1, x.shape[0],100)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],100)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        b=self.batchNorm(a1)
    
        c=torch.unsqueeze(b,1)
        
        out=self.conv0(c)
        out=self.conv1(out)
        out=self.conv2(out)
        #out=torch.nn.MaxPool2d((4,1))(out)
        out=self.conv3(out)
        out=self.conv4(out)
        out=self.conv5(out)
        out=self.conv6(out)
        out=self.conv7(out) #2061,10
        out=self.conv8(out)
        
        # out=torch.nn.MaxPool2d((4,1))(out)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        
        out=torch.nn.Dropout(0.2)(out)
        
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)
        out=self.l8(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4096,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4096, 20))  # sample, timeseries, len of one timeseries
print(s)

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=30,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

"""# practice

"""

from google.colab import drive
drive.mount("/content/gdrive/")

import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
!pip install mne
import mne
# Ignore warnings
import warnings

!pip install torchinfo
import  torchinfo
from torchinfo import summary
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import torchsummary
from torch.autograd import Variable
import random
import tensorflow as tf
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
from keras.metrics import mean_absolute_error  as mae
from sklearn.metrics import explained_variance_score
print("stage: libraries importing done")


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
X_pure="/content/gdrive/MyDrive/mitacs 2023/2000_without_alpha.npy"
Y_pure="/content/gdrive/MyDrive/mitacs 2023/Synthetic_2000_Age.npy"

X_0_9_path =  '/content/gdrive/MyDrive/mitacs 2023/Synthetic_Data_noise_added_alpha_0.9_2000samples.npy'
XX_0_99_path=  "/content/gdrive/MyDrive/mitacs 2023/Synthetic_Data_noise_added_alpha_0.99_2000samples.npy"

Y_path =  '/content/gdrive/MyDrive/mitacs 2023/Synthetic_Age.npy'
xdata=np.load(X_pure,allow_pickle=True)
ydata=np.load(Y_pure,allow_pickle=True)



#NPY_CustomEEGData : use it  when you load data as 
class NPY_CustomEEGData:
    def __init__(self,x,y,device=device):
      """
      inputs:
      x: npy araay of all edf outputs
      y: age of all data in  npy array in x 
      """
      self.x = torch.from_numpy(np.asarray(x ,dtype=np.float32))#.double()
      self.y = torch.from_numpy(np.asarray(y,dtype=np.float32))#.double()
      self.device=device
        
    def __len__(self):
      return len(self.x)

    def __getitem__(self, idx):
      """
      output: X and Y

      """

      xdata=np.transpose(self.x[idx],(1,0))
      #xdata=self.x[idx]
      return xdata.to(self.device),self.y[idx].to(self.device)
dataset=NPY_CustomEEGData(np.stack(xdata,0), ydata)
xx1=dataset.__getitem__(10)


validation_split = .5
shuffle_dataset = True
dataset_size = len(dataset)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))
np.random.shuffle(indices)

train_indices, val_indices,test_indices = indices[split:], indices[:int(split/5)],indices[int(split/5):split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)
test_sampler= SubsetRandomSampler(test_indices)

batch_size=32

train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,sampler=valid_sampler)
test_loader= torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)
del xdata
del ydata
del indices
del dataset_size
del split

for x,y in train_loader:
  print(x.shape)
  break
import torch
torch.cuda.empty_cache()
import gc
gc.collect()

print("stage: Pytorch model formation done! ")

loss_fn=torch.nn.L1Loss()
#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.manual_seed(41)
criterion = nn.L1Loss()

#dataset = ConcatDataset([train_dataloader, test_dataloader])
num_epochs = 10
batch_size = 32
k = 10
# splits = KFold(n_splits = k, shuffle = True, random_state = 41)
# foldperf={}
def show_metrics(y_test, y_predict):
    from sklearn.metrics import mean_absolute_error as mae
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()
    pass


def train_epoch(model,start_index,end_index,device,dataloader,loss_fn,optimizer,epochs=1,patience_iterations=4,previous_loss_index=0,index_range=3):
    """
    model: rnn model u r going to use 
    start index: index to get piece of data/slicing start
    end index: slicinf=g index end
    device : device for pu
    dataloader: training dataloader
    lossfn: loss fn
    epochs: no of epochs for each sample
    patience_iterations: wait till no of iterations when train loss is more than val loss; part for rarly stopping, after patience iterations exceed training will stop
    previous_loss_index: index where previois  val loss is less than training loss
    index_range: range  of index where once tarining greater is less than val loss and again tarining loss is less than validation loss and now currently now training loss is greater than validation loss
    """
    train_loss=[]
    val0dation_loss=[]
    model.train().to(device)
    counter_loss=0
    count=0 ## no of times val loss is more than training loss
    previous_loss_index=previous_loss_index
    patience_iterations=patience_iterations
    index_range=index_range # difference between previous index came when val loss is than trainingb loss

    sample_dcount=0
    mylist=[]
    should_break=0
    for epoch in range(epochs):
        
        for i,(eeg_scan, age) in enumerate(dataloader,0): 
            
            sample_dcount+=1
            #model.train().to(device)
            eeg_scan, age0 = eeg_scan.to(device), age.to(device)
            # make 0 gradient initially for optimization
            optimizer.zero_grad()  # gradient : slope
            age=(age0-torch.mean(age0))/(torch.var(age0)).to(device)
            age=age0
            output = model(eeg_scan[:,start_index:end_index,:])
            loss = loss_fn(output,age)
            loss.backward()  #  calculating gradients ,update weights to input 
            optimizer.step()  # move in direction of gradient to get minima

            counter_loss+=loss.item()  
            #val_loss=valid_epoch(model,start_index,end_index)
          
            copy=counter_loss
            n=20
            ii=i
        
        val_loss=valid_epoch(model,start_index,end_index)
        print("epoch: ",epoch," traing loss after 1 batch is:  ",counter_loss/(ii+1),"Validation loss after 20*32 batch is:  ",val_loss)
                #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
        train_loss.append(counter_loss/(ii+1))
        val0dation_loss.append(val_loss)
        x=counter_loss
        counter_loss=0
                #sample_dcount=0
            
          #   if i%n==0 and i!=0:
          #       print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1),"Validation loss after 20*32 batch is:  ",val_loss)
          #       #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
          #       train_loss.append(counter_loss/(n+1))
          #       val0dation_loss.append(val_loss)
          #       x=counter_loss
          #       counter_loss=0
          #       sample_dcount=0
          # # try:
          #   np.mean(val0dation_loss[index_range:])>np.mean(train_loss[index_range:])
          #   should_break+=1
          # except:
          #   pass
        # if should_break>=patience_iterations:
        #   break

        # #continue
        if  val_loss>counter_loss/(ii+1):
          if len(mylist)>patience_iterations:
            break
          else:
            if len(mylist)>0:
              if (epoch-mylist[-1])>index_range:
                mylist=[epoch]
              else:
                mylist.append(epoch)
            else:
              mylist=[epoch]

    plt.plot(val0dation_loss)
    plt.plot(train_loss)
    plt.legend(["val_loss","train-loss"])
    pass

@torch.no_grad()
def valid_epoch(model,start_index,end_index):
    valid_loss= 0.0
    #model.eval()
    count=0
    for i, (x,y) in enumerate(validation_loader):
        eeg_scan, age = x.to(device), y.to(device)
        output = model(eeg_scan[:,start_index:end_index,:])
        loss = loss_fn(output,age)
        valid_loss += loss.item()
        count=i+1
    return valid_loss/count


@torch.no_grad()
def predict( model,start_index=0,end_index=-1):
    ypred=[]
    ytest=[]
    model.eval()  
    for x,y in test_loader:
        yhat = model(x)
        # retrieve numpy array
        yhat = yhat.detach().cpu().numpy()
        #check=[i[0] for i in yhat[0]]
        ypred.extend(yhat)
        #print(ypred.shape)
        ytest.extend(y.cpu().numpy())
   
    # out=[]
    # for i in ypred[0]:
    #     out.append(i.tolist()[0])
    # show_metrics(np.array(ytest),np.array(out))
    return ytest,ypred

import torch, gc
gc.collect()
torch.cuda.empty_cache()

# best accuracy for 2000 sample

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=2,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(16,2))
        self.conv2=nn.Conv2d(1,1,(32,2))
        self.conv3=nn.Conv2d(1,1,(256,1))
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(506*18, 4000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        b=self.batchNorm(a1)
        #print(b.shape)
        c=torch.unsqueeze(b,1)
        #print(c.shape)
        out=self.conv1(c)
        out=self.conv2(out)
        out=torch.nn.MaxPool2d((8,1))(out)
        #print(out.shape)
        #out=self.conv3(out)

        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4096,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4096, 20))  # sample, timeseries, len of one timeseries
print(s)

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=50,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

"""# **Task wavelet and 0.6-0.8**

## 0.5 noise +data
"""

from google.colab import drive
drive.mount("/content/gdrive/")

import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
!pip install mne
import mne
# Ignore warnings
import warnings

!pip install torchinfo
import  torchinfo
from torchinfo import summary
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import torchsummary
from torch.autograd import Variable
import random
import tensorflow as tf
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
from keras.metrics import mean_absolute_error  as mae
from sklearn.metrics import explained_variance_score

from scipy import stats

print("stage: libraries importing done")


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
X_pure="/content/gdrive/MyDrive/mitacs 2023/2000_without_alpha.npy"
Y_pure="/content/gdrive/MyDrive/mitacs 2023/Synthetic_2000_Age.npy"

X_0_9_path =  '/content/gdrive/MyDrive/data/synthetic/Synthetic_Data_noise_added_alpha_0.9_2000samples.npy'
XX_0_99_path=  "/content/gdrive/MyDrive/data/synthetic/Synthetic_Data_noise_added_alpha_0.99_2000samples.npy"
Y_path =  '/content/gdrive/MyDrive/data/synthetic/Synthetic_Age.npy'

x1=np.load(X_pure,allow_pickle=True)
xdata=stats.zscore(x1,axis=2)

ydata=np.load(Y_pure,allow_pickle=True)


#NPY_CustomEEGData : use it  when you load data as 
class NPY_CustomEEGData:
    def __init__(self,x,y,device=device):
      """
      inputs:
      x: npy araay of all edf outputs
      y: age of all data in  npy array in x 
      """
      self.x = torch.from_numpy(np.asarray(x ,dtype=np.float32))#.double()
      self.y = torch.from_numpy(np.asarray(y,dtype=np.float32))#.double()
      self.device=device
        
    def __len__(self):
      return len(self.x)

    def __getitem__(self, idx):
      """
      output: X and Y

      """

      xdata=np.transpose(self.x[idx],(1,0))
      #xdata=self.x[idx]
      return xdata.to(self.device),self.y[idx].to(self.device)
dataset=NPY_CustomEEGData(np.stack(xdata,0), ydata)
xx1=dataset.__getitem__(10)


validation_split = .5
shuffle_dataset = True
dataset_size = len(dataset)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))
np.random.shuffle(indices)

train_indices, val_indices,test_indices = indices[split:], indices[:int(split/5)],indices[int(split/5):split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)
test_sampler= SubsetRandomSampler(test_indices)

batch_size=32

train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,sampler=valid_sampler)
test_loader= torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)
del xdata
del ydata
del indices
del dataset_size
del split

for x,y in train_loader:
  print(x.shape)
  break
import torch
torch.cuda.empty_cache()
import gc
gc.collect()


print("stage: Pytorch model formation done! ")

loss_fn=torch.nn.L1Loss()
#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.manual_seed(41)
criterion = nn.L1Loss()

#dataset = ConcatDataset([train_dataloader, test_dataloader])
num_epochs = 10
batch_size = 32
k = 10
# splits = KFold(n_splits = k, shuffle = True, random_state = 41)
# foldperf={}
def show_metrics(y_test, y_predict):
    from sklearn.metrics import mean_absolute_error as mae
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()
    pass


def train_epoch(model,start_index,end_index,device,dataloader,loss_fn,optimizer,epochs=1,patience_iterations=4,previous_loss_index=0,index_range=3):
    """
    model: rnn model u r going to use 
    start index: index to get piece of data/slicing start
    end index: slicinf=g index end
    device : device for pu
    dataloader: training dataloader
    lossfn: loss fn
    epochs: no of epochs for each sample
    patience_iterations: wait till no of iterations when train loss is more than val loss; part for rarly stopping, after patience iterations exceed training will stop
    previous_loss_index: index where previois  val loss is less than training loss
    index_range: range  of index where once tarining greater is less than val loss and again tarining loss is less than validation loss and now currently now training loss is greater than validation loss
    """
    train_loss=[]
    val0dation_loss=[]
    model.train().to(device)
    counter_loss=0
    count=0 ## no of times val loss is more than training loss
    previous_loss_index=previous_loss_index
    patience_iterations=patience_iterations
    index_range=index_range # difference between previous index came when val loss is than trainingb loss

    sample_dcount=0
    mylist=[]
    should_break=0
    for epoch in range(epochs):
        
        for i,(eeg_scan, age) in enumerate(dataloader,0): 
            
            sample_dcount+=1
            #model.train().to(device)
            eeg_scan, age0 = eeg_scan.to(device), age.to(device)
            # make 0 gradient initially for optimization
            optimizer.zero_grad()  # gradient : slope
            age=(age0-torch.mean(age0))/(torch.var(age0)).to(device)
            age=age0
            output = model(eeg_scan[:,start_index:end_index,:])
            loss = loss_fn(output,age)
            loss.backward()  #  calculating gradients ,update weights to input 
            optimizer.step()  # move in direction of gradient to get minima

            counter_loss+=loss.item()  
            #val_loss=valid_epoch(model,start_index,end_index)
          
            copy=counter_loss
            n=20
            ii=i
        
        val_loss=valid_epoch(model,start_index,end_index)
        print("epoch: ",epoch," traing loss after 1 batch is:  ",counter_loss/(ii+1),"Validation loss after 20*32 batch is:  ",val_loss)
                #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
        train_loss.append(counter_loss/(ii+1))
        val0dation_loss.append(val_loss)
        x=counter_loss
        counter_loss=0
                #sample_dcount=0
            
          #   if i%n==0 and i!=0:
          #       print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1),"Validation loss after 20*32 batch is:  ",val_loss)
          #       #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
          #       train_loss.append(counter_loss/(n+1))
          #       val0dation_loss.append(val_loss)
          #       x=counter_loss
          #       counter_loss=0
          #       sample_dcount=0
          # # try:
          #   np.mean(val0dation_loss[index_range:])>np.mean(train_loss[index_range:])
          #   should_break+=1
          # except:
          #   pass
        # if should_break>=patience_iterations:
        #   break

        # #continue
        if  val_loss>counter_loss/(ii+1):
          if len(mylist)>patience_iterations:
            break
          else:
            if len(mylist)>0:
              if (epoch-mylist[-1])>index_range:
                mylist=[epoch]
              else:
                mylist.append(epoch)
            else:
              mylist=[epoch]

    plt.plot(val0dation_loss)
    plt.plot(train_loss)
    plt.legend(["val_loss","train-loss"])
    pass

@torch.no_grad()
def valid_epoch(model,start_index,end_index):
    valid_loss= 0.0
    #model.eval()
    count=0
    for i, (x,y) in enumerate(validation_loader):
        eeg_scan, age = x.to(device), y.to(device)
        output = model(eeg_scan[:,start_index:end_index,:])
        loss = loss_fn(output,age)
        valid_loss += loss.item()
        count=i+1
    return valid_loss/count


@torch.no_grad()
def predict( model,start_index=0,end_index=-1):
    ypred=[]
    ytest=[]
    model.eval()  
    for x,y in test_loader:
        yhat = model(x)
        # retrieve numpy array
        yhat = yhat.detach().cpu().numpy()
        #check=[i[0] for i in yhat[0]]
        ypred.extend(yhat)
        #print(ypred.shape)
        ytest.extend(y.cpu().numpy())
   
    # out=[]
    # for i in ypred[0]:
    #     out.append(i.tolist()[0])
    # show_metrics(np.array(ytest),np.array(out))
    return ytest,ypred

import torch, gc
gc.collect()
torch.cuda.empty_cache()

# best accuracy for 2000 sample

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=2,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(16,2))
        self.conv2=nn.Conv2d(1,1,(32,2))
        self.conv3=nn.Conv2d(1,1,(256,1))
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(506*18, 4000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        b=self.batchNorm(a1)
        #print(b.shape)
        c=torch.unsqueeze(b,1)
        #print(c.shape)
        out=self.conv1(c)
        out=self.conv2(out)
        out=torch.nn.MaxPool2d((8,1))(out)
        #print(out.shape)
        #out=self.conv3(out)

        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4096,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4096, 20))  # sample, timeseries, len of one timeseries
print(s)

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=50,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

"""## 0.6 noise +data"""

from google.colab import drive
drive.mount("/content/gdrive/")

import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
!pip install mne
import mne
# Ignore warnings
import warnings

!pip install torchinfo
import  torchinfo
from torchinfo import summary
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import torchsummary
from torch.autograd import Variable
import random
import tensorflow as tf
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
from keras.metrics import mean_absolute_error  as mae
from sklearn.metrics import explained_variance_score
print("stage: libraries importing done")


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
X_pure="/content/gdrive/MyDrive/data/synthetic/2000_without_alpha.npy"

Y_pure="/content/gdrive/MyDrive/data/synthetic/Synthetic_2000_Age.npy"

X_0_9_path =  '/content/gdrive/MyDrive/data/synthetic/Synthetic_Data_noise_added_alpha_0.9_2000samples.npy'
XX_0_99_path=  "/content/gdrive/MyDrive/data/synthetic/Synthetic_Data_noise_added_alpha_0.99_2000samples.npy"

x_0_6="/content/gdrive/MyDrive/mitacs 2023/Synthetic_Data_noise_added_alpha_0.6_2000samples.npy"
Y_path =  '/content/gdrive/MyDrive/mitacs 2023/Synthetic_Age.npy'
xdata=np.load(x_0_6,allow_pickle=True)
ydata=np.load(Y_path,allow_pickle=True)


#NPY_CustomEEGData : use it  when you load data as 
class NPY_CustomEEGData:
    def __init__(self,x,y,device=device):
      """
      inputs:
      x: npy araay of all edf outputs
      y: age of all data in  npy array in x 
      """
      self.x = torch.from_numpy(np.asarray(x ,dtype=np.float32))#.double()
      self.y = torch.from_numpy(np.asarray(y,dtype=np.float32))#.double()
      self.device=device
        
    def __len__(self):
      return len(self.x)

    def __getitem__(self, idx):
      """
      output: X and Y

      """

      xdata=np.transpose(self.x[idx],(1,0))
      #xdata=self.x[idx]
      return xdata.to(self.device),self.y[idx].to(self.device)
dataset=NPY_CustomEEGData(np.stack(xdata,0), ydata)
xx1=dataset.__getitem__(10)


validation_split = .5
shuffle_dataset = True
dataset_size = len(dataset)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))
np.random.shuffle(indices)

train_indices, val_indices,test_indices = indices[split:], indices[:int(split/5)],indices[int(split/5):split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)
test_sampler= SubsetRandomSampler(test_indices)

batch_size=32

train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,sampler=valid_sampler)
test_loader= torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)
del xdata
del ydata
del indices
del dataset_size
del split

for x,y in train_loader:
  print(x.shape)
  break
import torch
torch.cuda.empty_cache()
import gc
gc.collect()


print("stage: Pytorch model formation done! ")

loss_fn=torch.nn.L1Loss()
#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.manual_seed(41)
criterion = nn.L1Loss()

#dataset = ConcatDataset([train_dataloader, test_dataloader])
num_epochs = 10
batch_size = 32
k = 10
# splits = KFold(n_splits = k, shuffle = True, random_state = 41)
# foldperf={}
def show_metrics(y_test, y_predict):
    from sklearn.metrics import mean_absolute_error as mae
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()
    pass


def train_epoch(model,start_index,end_index,device,dataloader,loss_fn,optimizer,epochs=1,patience_iterations=4,previous_loss_index=0,index_range=3):
    """
    model: rnn model u r going to use 
    start index: index to get piece of data/slicing start
    end index: slicinf=g index end
    device : device for pu
    dataloader: training dataloader
    lossfn: loss fn
    epochs: no of epochs for each sample
    patience_iterations: wait till no of iterations when train loss is more than val loss; part for rarly stopping, after patience iterations exceed training will stop
    previous_loss_index: index where previois  val loss is less than training loss
    index_range: range  of index where once tarining greater is less than val loss and again tarining loss is less than validation loss and now currently now training loss is greater than validation loss
    """
    train_loss=[]
    val0dation_loss=[]
    model.train().to(device)
    counter_loss=0
    count=0 ## no of times val loss is more than training loss
    previous_loss_index=previous_loss_index
    patience_iterations=patience_iterations
    index_range=index_range # difference between previous index came when val loss is than trainingb loss

    sample_dcount=0
    mylist=[]
    should_break=0
    for epoch in range(epochs):
        
        for i,(eeg_scan, age) in enumerate(dataloader,0): 
            
            sample_dcount+=1
            #model.train().to(device)
            eeg_scan, age0 = eeg_scan.to(device), age.to(device)
            # make 0 gradient initially for optimization
            optimizer.zero_grad()  # gradient : slope
            age=(age0-torch.mean(age0))/(torch.var(age0)).to(device)
            age=age0
            output = model(eeg_scan[:,start_index:end_index,:])
            loss = loss_fn(output,age)
            loss.backward()  #  calculating gradients ,update weights to input 
            optimizer.step()  # move in direction of gradient to get minima

            counter_loss+=loss.item()  
            #val_loss=valid_epoch(model,start_index,end_index)
          
            copy=counter_loss
            n=20
            ii=i
        
        val_loss=valid_epoch(model,start_index,end_index)
        print("epoch: ",epoch," traing loss after 1 batch is:  ",counter_loss/(ii+1),"Validation loss after 20*32 batch is:  ",val_loss)
                #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
        train_loss.append(counter_loss/(ii+1))
        val0dation_loss.append(val_loss)
        x=counter_loss
        counter_loss=0
                #sample_dcount=0
            
          #   if i%n==0 and i!=0:
          #       print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1),"Validation loss after 20*32 batch is:  ",val_loss)
          #       #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
          #       train_loss.append(counter_loss/(n+1))
          #       val0dation_loss.append(val_loss)
          #       x=counter_loss
          #       counter_loss=0
          #       sample_dcount=0
          # # try:
          #   np.mean(val0dation_loss[index_range:])>np.mean(train_loss[index_range:])
          #   should_break+=1
          # except:
          #   pass
        # if should_break>=patience_iterations:
        #   break

        # #continue
        if  val_loss>counter_loss/(ii+1):
          if len(mylist)>patience_iterations:
            break
          else:
            if len(mylist)>0:
              if (epoch-mylist[-1])>index_range:
                mylist=[epoch]
              else:
                mylist.append(epoch)
            else:
              mylist=[epoch]

    plt.plot(val0dation_loss)
    plt.plot(train_loss)
    plt.legend(["val_loss","train-loss"])
    pass

@torch.no_grad()
def valid_epoch(model,start_index,end_index):
    valid_loss= 0.0
    #model.eval()
    count=0
    for i, (x,y) in enumerate(validation_loader):
        eeg_scan, age = x.to(device), y.to(device)
        output = model(eeg_scan[:,start_index:end_index,:])
        loss = loss_fn(output,age)
        valid_loss += loss.item()
        count=i+1
    return valid_loss/count


@torch.no_grad()
def predict( model,start_index=0,end_index=-1):
    ypred=[]
    ytest=[]
    model.eval()  
    for x,y in test_loader:
        yhat = model(x)
        # retrieve numpy array
        yhat = yhat.detach().cpu().numpy()
        #check=[i[0] for i in yhat[0]]
        ypred.extend(yhat)
        #print(ypred.shape)
        ytest.extend(y.cpu().numpy())
   
    # out=[]
    # for i in ypred[0]:
    #     out.append(i.tolist()[0])
    # show_metrics(np.array(ytest),np.array(out))
    return ytest,ypred

import torch, gc
gc.collect()
torch.cuda.empty_cache()

# best accuracy for 2000 sample

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=2,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(16,2))
        self.conv2=nn.Conv2d(1,1,(32,2))
        self.conv3=nn.Conv2d(1,1,(256,1))
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(506*18, 4000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        b=self.batchNorm(a1)
        #print(b.shape)
        c=torch.unsqueeze(b,1)
        #print(c.shape)
        out=self.conv1(c)
        out=self.conv2(out)
        out=torch.nn.MaxPool2d((8,1))(out)
        #print(out.shape)
        #out=self.conv3(out)

        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4096,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4096, 20))  # sample, timeseries, len of one timeseries
print(s)

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=50,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

"""## 0.7 noise +data"""

from google.colab import drive
drive.mount("/content/gdrive/")

import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
!pip install mne
import mne
# Ignore warnings
import warnings

!pip install torchinfo
import  torchinfo
from torchinfo import summary
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import torchsummary
from torch.autograd import Variable
import random
import tensorflow as tf
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
from keras.metrics import mean_absolute_error  as mae
from sklearn.metrics import explained_variance_score
print("stage: libraries importing done")


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
X_pure="/content/gdrive/MyDrive/data/synthetic/2000_without_alpha.npy"
Y_pure="/content/gdrive/MyDrive/data/synthetic/Synthetic_2000_Age.npy"

X_0_9_path =  '/content/gdrive/MyDrive/data/synthetic/Synthetic_Data_noise_added_alpha_0.9_2000samples.npy'
XX_0_99_path=  "/content/gdrive/MyDrive/data/synthetic/Synthetic_Data_noise_added_alpha_0.99_2000samples.npy"
x_0_7="/content/gdrive/MyDrive/mitacs 2023/Synthetic_Data_noise_added_alpha_0.7_2000samples.npy"
Y_path =  '/content/gdrive/MyDrive/mitacs 2023/Synthetic_Age.npy'
xdata=np.load(x_0_7,allow_pickle=True)
ydata=np.load(Y_path,allow_pickle=True)

#NPY_CustomEEGData : use it  when you load data as 
class NPY_CustomEEGData:
    def __init__(self,x,y,device=device):
      """
      inputs:
      x: npy araay of all edf outputs
      y: age of all data in  npy array in x 
      """
      self.x = torch.from_numpy(np.asarray(x ,dtype=np.float32))#.double()
      self.y = torch.from_numpy(np.asarray(y,dtype=np.float32))#.double()
      self.device=device
        
    def __len__(self):
      return len(self.x)

    def __getitem__(self, idx):
      """
      output: X and Y

      """

      xdata=np.transpose(self.x[idx],(1,0))
      #xdata=self.x[idx]
      return xdata.to(self.device),self.y[idx].to(self.device)
dataset=NPY_CustomEEGData(np.stack(xdata,0), ydata)
xx1=dataset.__getitem__(10)


validation_split = .5
shuffle_dataset = True
dataset_size = len(dataset)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))
np.random.shuffle(indices)

train_indices, val_indices,test_indices = indices[split:], indices[:int(split/5)],indices[int(split/5):split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)
test_sampler= SubsetRandomSampler(test_indices)

batch_size=32

train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,sampler=valid_sampler)
test_loader= torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)
del xdata
del ydata
del indices
del dataset_size
del split

for x,y in train_loader:
  print(x.shape)
  break
import torch
torch.cuda.empty_cache()
import gc
gc.collect()


print("stage: Pytorch model formation done! ")

loss_fn=torch.nn.L1Loss()
#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.manual_seed(41)
criterion = nn.L1Loss()

#dataset = ConcatDataset([train_dataloader, test_dataloader])
num_epochs = 10
batch_size = 32
k = 10
# splits = KFold(n_splits = k, shuffle = True, random_state = 41)
# foldperf={}
def show_metrics(y_test, y_predict):
    from sklearn.metrics import mean_absolute_error as mae
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()
    pass


def train_epoch(model,start_index,end_index,device,dataloader,loss_fn,optimizer,epochs=1,patience_iterations=4,previous_loss_index=0,index_range=3):
    """
    model: rnn model u r going to use 
    start index: index to get piece of data/slicing start
    end index: slicinf=g index end
    device : device for pu
    dataloader: training dataloader
    lossfn: loss fn
    epochs: no of epochs for each sample
    patience_iterations: wait till no of iterations when train loss is more than val loss; part for rarly stopping, after patience iterations exceed training will stop
    previous_loss_index: index where previois  val loss is less than training loss
    index_range: range  of index where once tarining greater is less than val loss and again tarining loss is less than validation loss and now currently now training loss is greater than validation loss
    """
    train_loss=[]
    val0dation_loss=[]
    model.train().to(device)
    counter_loss=0
    count=0 ## no of times val loss is more than training loss
    previous_loss_index=previous_loss_index
    patience_iterations=patience_iterations
    index_range=index_range # difference between previous index came when val loss is than trainingb loss

    sample_dcount=0
    mylist=[]
    should_break=0
    for epoch in range(epochs):
        
        for i,(eeg_scan, age) in enumerate(dataloader,0): 
            
            sample_dcount+=1
            #model.train().to(device)
            eeg_scan, age0 = eeg_scan.to(device), age.to(device)
            # make 0 gradient initially for optimization
            optimizer.zero_grad()  # gradient : slope
            age=(age0-torch.mean(age0))/(torch.var(age0)).to(device)
            age=age0
            output = model(eeg_scan[:,start_index:end_index,:])
            loss = loss_fn(output,age)
            loss.backward()  #  calculating gradients ,update weights to input 
            optimizer.step()  # move in direction of gradient to get minima

            counter_loss+=loss.item()  
            #val_loss=valid_epoch(model,start_index,end_index)
          
            copy=counter_loss
            n=20
            ii=i
        
        val_loss=valid_epoch(model,start_index,end_index)
        print("epoch: ",epoch," traing loss after 1 batch is:  ",counter_loss/(ii+1),"Validation loss after 20*32 batch is:  ",val_loss)
                #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
        train_loss.append(counter_loss/(ii+1))
        val0dation_loss.append(val_loss)
        x=counter_loss
        counter_loss=0
                #sample_dcount=0
            
          #   if i%n==0 and i!=0:
          #       print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1),"Validation loss after 20*32 batch is:  ",val_loss)
          #       #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
          #       train_loss.append(counter_loss/(n+1))
          #       val0dation_loss.append(val_loss)
          #       x=counter_loss
          #       counter_loss=0
          #       sample_dcount=0
          # # try:
          #   np.mean(val0dation_loss[index_range:])>np.mean(train_loss[index_range:])
          #   should_break+=1
          # except:
          #   pass
        # if should_break>=patience_iterations:
        #   break

        # #continue
        if  val_loss>counter_loss/(ii+1):
          if len(mylist)>patience_iterations:
            break
          else:
            if len(mylist)>0:
              if (epoch-mylist[-1])>index_range:
                mylist=[epoch]
              else:
                mylist.append(epoch)
            else:
              mylist=[epoch]

    plt.plot(val0dation_loss)
    plt.plot(train_loss)
    plt.legend(["val_loss","train-loss"])
    pass

@torch.no_grad()
def valid_epoch(model,start_index,end_index):
    valid_loss= 0.0
    #model.eval()
    count=0
    for i, (x,y) in enumerate(validation_loader):
        eeg_scan, age = x.to(device), y.to(device)
        output = model(eeg_scan[:,start_index:end_index,:])
        loss = loss_fn(output,age)
        valid_loss += loss.item()
        count=i+1
    return valid_loss/count


@torch.no_grad()
def predict( model,start_index=0,end_index=-1):
    ypred=[]
    ytest=[]
    model.eval()  
    for x,y in test_loader:
        yhat = model(x)
        # retrieve numpy array
        yhat = yhat.detach().cpu().numpy()
        #check=[i[0] for i in yhat[0]]
        ypred.extend(yhat)
        #print(ypred.shape)
        ytest.extend(y.cpu().numpy())
   
    # out=[]
    # for i in ypred[0]:
    #     out.append(i.tolist()[0])
    # show_metrics(np.array(ytest),np.array(out))
    return ytest,ypred

import torch, gc
gc.collect()
torch.cuda.empty_cache()

# best accuracy for 2000 sample

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=2,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(16,2))
        self.conv2=nn.Conv2d(1,1,(32,2))
        self.conv3=nn.Conv2d(1,1,(256,1))
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(506*18, 4000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        b=self.batchNorm(a1)
        #print(b.shape)
        c=torch.unsqueeze(b,1)
        #print(c.shape)
        out=self.conv1(c)
        out=self.conv2(out)
        out=torch.nn.MaxPool2d((8,1))(out)
        #print(out.shape)
        #out=self.conv3(out)

        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4096,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4096, 20))  # sample, timeseries, len of one timeseries
print(s)

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=50,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

"""## 0.8 noise +data"""

from google.colab import drive
drive.mount("/content/gdrive/")

import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
!pip install mne
import mne
# Ignore warnings
import warnings

!pip install torchinfo
import  torchinfo
from torchinfo import summary
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import torchsummary
from torch.autograd import Variable
import random
import tensorflow as tf
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
from keras.metrics import mean_absolute_error  as mae
from sklearn.metrics import explained_variance_score
print("stage: libraries importing done")


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
X_pure="/content/gdrive/MyDrive/data/synthetic/2000_without_alpha.npy"
Y_pure="/content/gdrive/MyDrive/data/synthetic/Synthetic_2000_Age.npy"

X_0_9_path =  '/content/gdrive/MyDrive/data/synthetic/Synthetic_Data_noise_added_alpha_0.9_2000samples.npy'
XX_0_99_path=  "/content/gdrive/MyDrive/data/synthetic/Synthetic_Data_noise_added_alpha_0.99_2000samples.npy"
x_0_8="/content/gdrive/MyDrive/mitacs 2023/Synthetic_Data_noise_added_alpha_0.8_2000samples.npy"
Y_path =  '/content/gdrive/MyDrive/mitacs 2023/Synthetic_Age.npy'
xdata=np.load(x_0_8,allow_pickle=True)
ydata=np.load(Y_path,allow_pickle=True)


#NPY_CustomEEGData : use it  when you load data as 
class NPY_CustomEEGData:
    def __init__(self,x,y,device=device):
      """
      inputs:
      x: npy araay of all edf outputs
      y: age of all data in  npy array in x 
      """
      self.x = torch.from_numpy(np.asarray(x ,dtype=np.float32))#.double()
      self.y = torch.from_numpy(np.asarray(y,dtype=np.float32))#.double()
      self.device=device
        
    def __len__(self):
      return len(self.x)

    def __getitem__(self, idx):
      """
      output: X and Y

      """

      xdata=np.transpose(self.x[idx],(1,0))
      #xdata=self.x[idx]
      return xdata.to(self.device),self.y[idx].to(self.device)
dataset=NPY_CustomEEGData(np.stack(xdata,0), ydata)
xx1=dataset.__getitem__(10)


validation_split = .5
shuffle_dataset = True
dataset_size = len(dataset)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))
np.random.shuffle(indices)

train_indices, val_indices,test_indices = indices[split:], indices[:int(split/5)],indices[int(split/5):split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)
test_sampler= SubsetRandomSampler(test_indices)

batch_size=32

train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,sampler=valid_sampler)
test_loader= torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)
del xdata
del ydata
del indices
del dataset_size
del split

for x,y in train_loader:
  print(x.shape)
  break
import torch
torch.cuda.empty_cache()
import gc
gc.collect()


print("stage: Pytorch model formation done! ")

loss_fn=torch.nn.L1Loss()
#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.manual_seed(41)
criterion = nn.L1Loss()

#dataset = ConcatDataset([train_dataloader, test_dataloader])
num_epochs = 10
batch_size = 32
k = 10
# splits = KFold(n_splits = k, shuffle = True, random_state = 41)
# foldperf={}
def show_metrics(y_test, y_predict):
    from sklearn.metrics import mean_absolute_error as mae
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()
    pass


def train_epoch(model,start_index,end_index,device,dataloader,loss_fn,optimizer,epochs=1,patience_iterations=4,previous_loss_index=0,index_range=3):
    """
    model: rnn model u r going to use 
    start index: index to get piece of data/slicing start
    end index: slicinf=g index end
    device : device for pu
    dataloader: training dataloader
    lossfn: loss fn
    epochs: no of epochs for each sample
    patience_iterations: wait till no of iterations when train loss is more than val loss; part for rarly stopping, after patience iterations exceed training will stop
    previous_loss_index: index where previois  val loss is less than training loss
    index_range: range  of index where once tarining greater is less than val loss and again tarining loss is less than validation loss and now currently now training loss is greater than validation loss
    """
    train_loss=[]
    val0dation_loss=[]
    model.train().to(device)
    counter_loss=0
    count=0 ## no of times val loss is more than training loss
    previous_loss_index=previous_loss_index
    patience_iterations=patience_iterations
    index_range=index_range # difference between previous index came when val loss is than trainingb loss

    sample_dcount=0
    mylist=[]
    should_break=0
    for epoch in range(epochs):
        
        for i,(eeg_scan, age) in enumerate(dataloader,0): 
            
            sample_dcount+=1
            #model.train().to(device)
            eeg_scan, age0 = eeg_scan.to(device), age.to(device)
            # make 0 gradient initially for optimization
            optimizer.zero_grad()  # gradient : slope
            age=(age0-torch.mean(age0))/(torch.var(age0)).to(device)
            age=age0
            output = model(eeg_scan[:,start_index:end_index,:])
            loss = loss_fn(output,age)
            loss.backward()  #  calculating gradients ,update weights to input 
            optimizer.step()  # move in direction of gradient to get minima

            counter_loss+=loss.item()  
            #val_loss=valid_epoch(model,start_index,end_index)
          
            copy=counter_loss
            n=20
            ii=i
        
        val_loss=valid_epoch(model,start_index,end_index)
        print("epoch: ",epoch," traing loss after 1 batch is:  ",counter_loss/(ii+1),"Validation loss after 20*32 batch is:  ",val_loss)
                #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
        train_loss.append(counter_loss/(ii+1))
        val0dation_loss.append(val_loss)
        x=counter_loss
        counter_loss=0
                #sample_dcount=0
            
          #   if i%n==0 and i!=0:
          #       print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1),"Validation loss after 20*32 batch is:  ",val_loss)
          #       #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
          #       train_loss.append(counter_loss/(n+1))
          #       val0dation_loss.append(val_loss)
          #       x=counter_loss
          #       counter_loss=0
          #       sample_dcount=0
          # # try:
          #   np.mean(val0dation_loss[index_range:])>np.mean(train_loss[index_range:])
          #   should_break+=1
          # except:
          #   pass
        # if should_break>=patience_iterations:
        #   break

        # #continue
        if  val_loss>counter_loss/(ii+1):
          if len(mylist)>patience_iterations:
            break
          else:
            if len(mylist)>0:
              if (epoch-mylist[-1])>index_range:
                mylist=[epoch]
              else:
                mylist.append(epoch)
            else:
              mylist=[epoch]

    plt.plot(val0dation_loss)
    plt.plot(train_loss)
    plt.legend(["val_loss","train-loss"])
    pass

@torch.no_grad()
def valid_epoch(model,start_index,end_index):
    valid_loss= 0.0
    #model.eval()
    count=0
    for i, (x,y) in enumerate(validation_loader):
        eeg_scan, age = x.to(device), y.to(device)
        output = model(eeg_scan[:,start_index:end_index,:])
        loss = loss_fn(output,age)
        valid_loss += loss.item()
        count=i+1
    return valid_loss/count


@torch.no_grad()
def predict( model,start_index=0,end_index=-1):
    ypred=[]
    ytest=[]
    model.eval()  
    for x,y in test_loader:
        yhat = model(x)
        # retrieve numpy array
        yhat = yhat.detach().cpu().numpy()
        #check=[i[0] for i in yhat[0]]
        ypred.extend(yhat)
        #print(ypred.shape)
        ytest.extend(y.cpu().numpy())
   
    # out=[]
    # for i in ypred[0]:
    #     out.append(i.tolist()[0])
    # show_metrics(np.array(ytest),np.array(out))
    return ytest,ypred

import torch, gc
gc.collect()
torch.cuda.empty_cache()

# best accuracy for 2000 sample

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=2,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(16,2))
        self.conv2=nn.Conv2d(1,1,(32,2))
        self.conv3=nn.Conv2d(1,1,(256,1))
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(506*18, 4000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        b=self.batchNorm(a1)
        #print(b.shape)
        c=torch.unsqueeze(b,1)
        #print(c.shape)
        out=self.conv1(c)
        out=self.conv2(out)
        out=torch.nn.MaxPool2d((8,1))(out)
        #print(out.shape)
        #out=self.conv3(out)

        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4096,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4096, 20))  # sample, timeseries, len of one timeseries
print(s)

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=50,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

"""## 0.9 noise +data"""

from google.colab import drive
drive.mount("/content/gdrive/")

import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
!pip install mne
import mne
# Ignore warnings
import warnings

!pip install torchinfo
import  torchinfo
from torchinfo import summary
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import torchsummary
from torch.autograd import Variable
import random
import tensorflow as tf
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
from keras.metrics import mean_absolute_error  as mae
from sklearn.metrics import explained_variance_score
print("stage: libraries importing done")


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
X_pure="/content/gdrive/MyDrive/data/synthetic/2000_without_alpha.npy"
Y_pure="/content/gdrive/MyDrive/data/synthetic/Synthetic_2000_Age.npy"

X_0_9_path =  '/content/gdrive/MyDrive/mitacs 2023/Synthetic_Data_noise_added_alpha_0.9_2000samples.npy'
XX_0_99_path=  "/content/gdrive/MyDrive/data/synthetic/Synthetic_Data_noise_added_alpha_0.99_2000samples.npy"
Y_path =  '/content/gdrive/MyDrive/mitacs 2023/Synthetic_Age.npy'
xdata=np.load(X_0_9_path,allow_pickle=True)
ydata=np.load(Y_path,allow_pickle=True)


#NPY_CustomEEGData : use it  when you load data as 
class NPY_CustomEEGData:
    def __init__(self,x,y,device=device):
      """
      inputs:
      x: npy araay of all edf outputs
      y: age of all data in  npy array in x 
      """
      self.x = torch.from_numpy(np.asarray(x ,dtype=np.float32))#.double()
      self.y = torch.from_numpy(np.asarray(y,dtype=np.float32))#.double()
      self.device=device
        
    def __len__(self):
      return len(self.x)

    def __getitem__(self, idx):
      """
      output: X and Y

      """

      xdata=np.transpose(self.x[idx],(1,0))
      #xdata=self.x[idx]
      return xdata.to(self.device),self.y[idx].to(self.device)
dataset=NPY_CustomEEGData(np.stack(xdata,0), ydata)
xx1=dataset.__getitem__(10)


validation_split = .5
shuffle_dataset = True
dataset_size = len(dataset)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))
np.random.shuffle(indices)

train_indices, val_indices,test_indices = indices[split:], indices[:int(split/5)],indices[int(split/5):split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)
test_sampler= SubsetRandomSampler(test_indices)

batch_size=32

train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,sampler=valid_sampler)
test_loader= torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)
del xdata
del ydata
del indices
del dataset_size
del split

for x,y in train_loader:
  print(x.shape)
  break
import torch
torch.cuda.empty_cache()
import gc
gc.collect()


print("stage: Pytorch model formation done! ")

loss_fn=torch.nn.L1Loss()
#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.manual_seed(41)
criterion = nn.L1Loss()

#dataset = ConcatDataset([train_dataloader, test_dataloader])
num_epochs = 10
batch_size = 32
k = 10
# splits = KFold(n_splits = k, shuffle = True, random_state = 41)
# foldperf={}
def show_metrics(y_test, y_predict):
    from sklearn.metrics import mean_absolute_error as mae
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()
    pass


def train_epoch(model,start_index,end_index,device,dataloader,loss_fn,optimizer,epochs=1,patience_iterations=4,previous_loss_index=0,index_range=3):
    """
    model: rnn model u r going to use 
    start index: index to get piece of data/slicing start
    end index: slicinf=g index end
    device : device for pu
    dataloader: training dataloader
    lossfn: loss fn
    epochs: no of epochs for each sample
    patience_iterations: wait till no of iterations when train loss is more than val loss; part for rarly stopping, after patience iterations exceed training will stop
    previous_loss_index: index where previois  val loss is less than training loss
    index_range: range  of index where once tarining greater is less than val loss and again tarining loss is less than validation loss and now currently now training loss is greater than validation loss
    """
    train_loss=[]
    val0dation_loss=[]
    model.train().to(device)
    counter_loss=0
    count=0 ## no of times val loss is more than training loss
    previous_loss_index=previous_loss_index
    patience_iterations=patience_iterations
    index_range=index_range # difference between previous index came when val loss is than trainingb loss

    sample_dcount=0
    mylist=[]
    should_break=0
    for epoch in range(epochs):
        
        for i,(eeg_scan, age) in enumerate(dataloader,0): 
            
            sample_dcount+=1
            #model.train().to(device)
            eeg_scan, age0 = eeg_scan.to(device), age.to(device)
            # make 0 gradient initially for optimization
            optimizer.zero_grad()  # gradient : slope
            age=(age0-torch.mean(age0))/(torch.var(age0)).to(device)
            age=age0
            output = model(eeg_scan[:,start_index:end_index,:])
            loss = loss_fn(output,age)
            loss.backward()  #  calculating gradients ,update weights to input 
            optimizer.step()  # move in direction of gradient to get minima

            counter_loss+=loss.item()  
            #val_loss=valid_epoch(model,start_index,end_index)
          
            copy=counter_loss
            n=20
            ii=i
        
        val_loss=valid_epoch(model,start_index,end_index)
        print("epoch: ",epoch," traing loss after 1 batch is:  ",counter_loss/(ii+1),"Validation loss after 20*32 batch is:  ",val_loss)
                #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
        train_loss.append(counter_loss/(ii+1))
        val0dation_loss.append(val_loss)
        x=counter_loss
        counter_loss=0
                #sample_dcount=0
            
          #   if i%n==0 and i!=0:
          #       print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1),"Validation loss after 20*32 batch is:  ",val_loss)
          #       #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
          #       train_loss.append(counter_loss/(n+1))
          #       val0dation_loss.append(val_loss)
          #       x=counter_loss
          #       counter_loss=0
          #       sample_dcount=0
          # # try:
          #   np.mean(val0dation_loss[index_range:])>np.mean(train_loss[index_range:])
          #   should_break+=1
          # except:
          #   pass
        # if should_break>=patience_iterations:
        #   break

        # #continue
        if  val_loss>counter_loss/(ii+1):
          if len(mylist)>patience_iterations:
            break
          else:
            if len(mylist)>0:
              if (epoch-mylist[-1])>index_range:
                mylist=[epoch]
              else:
                mylist.append(epoch)
            else:
              mylist=[epoch]

    plt.plot(val0dation_loss)
    plt.plot(train_loss)
    plt.legend(["val_loss","train-loss"])
    pass

@torch.no_grad()
def valid_epoch(model,start_index,end_index):
    valid_loss= 0.0
    #model.eval()
    count=0
    for i, (x,y) in enumerate(validation_loader):
        eeg_scan, age = x.to(device), y.to(device)
        output = model(eeg_scan[:,start_index:end_index,:])
        loss = loss_fn(output,age)
        valid_loss += loss.item()
        count=i+1
    return valid_loss/count


@torch.no_grad()
def predict( model,start_index=0,end_index=-1):
    ypred=[]
    ytest=[]
    model.eval()  
    for x,y in test_loader:
        yhat = model(x)
        # retrieve numpy array
        yhat = yhat.detach().cpu().numpy()
        #check=[i[0] for i in yhat[0]]
        ypred.extend(yhat)
        #print(ypred.shape)
        ytest.extend(y.cpu().numpy())
   
    # out=[]
    # for i in ypred[0]:
    #     out.append(i.tolist()[0])
    # show_metrics(np.array(ytest),np.array(out))
    return ytest,ypred

import torch, gc
gc.collect()
torch.cuda.empty_cache()

# best accuracy for 2000 sample

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=2,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(16,2))
        self.conv2=nn.Conv2d(1,1,(32,2))
        self.conv3=nn.Conv2d(1,1,(256,1))
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(506*18, 4000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        b=self.batchNorm(a1)
        #print(b.shape)
        c=torch.unsqueeze(b,1)
        #print(c.shape)
        out=self.conv1(c)
        out=self.conv2(out)
        out=torch.nn.MaxPool2d((8,1))(out)
        #print(out.shape)
        #out=self.conv3(out)

        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4096,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4096, 20))  # sample, timeseries, len of one timeseries
print(s)

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=50,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

"""## 0.99 noise+data"""

from google.colab import drive
drive.mount("/content/gdrive/")

import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
!pip install mne
import mne
# Ignore warnings
import warnings

!pip install torchinfo
import  torchinfo
from torchinfo import summary
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import torchsummary
from torch.autograd import Variable
import random
import tensorflow as tf
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
from keras.metrics import mean_absolute_error  as mae
from sklearn.metrics import explained_variance_score
print("stage: libraries importing done")


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
X_pure="/content/gdrive/MyDrive/data/synthetic/2000_without_alpha.npy"
Y_pure="/content/gdrive/MyDrive/data/synthetic/Synthetic_2000_Age.npy"

X_0_9_path =  '/content/gdrive/MyDrive/data/synthetic/Synthetic_Data_noise_added_alpha_0.9_2000samples.npy'
XX_0_99_path=  "/content/gdrive/MyDrive/mitacs 2023/Synthetic_Data_noise_added_alpha_0.99_2000samples.npy"
Y_path =  '/content/gdrive/MyDrive/mitacs 2023/Synthetic_Age.npy'
xdata=np.load(XX_0_99_path,allow_pickle=True)
ydata=np.load(Y_path,allow_pickle=True)


#NPY_CustomEEGData : use it  when you load data as 
class NPY_CustomEEGData:
    def __init__(self,x,y,device=device):
      """
      inputs:
      x: npy araay of all edf outputs
      y: age of all data in  npy array in x 
      """
      self.x = torch.from_numpy(np.asarray(x ,dtype=np.float32))#.double()
      self.y = torch.from_numpy(np.asarray(y,dtype=np.float32))#.double()
      self.device=device
        
    def __len__(self):
      return len(self.x)

    def __getitem__(self, idx):
      """
      output: X and Y

      """

      xdata=np.transpose(self.x[idx],(1,0))
      #xdata=self.x[idx]
      return xdata.to(self.device),self.y[idx].to(self.device)
dataset=NPY_CustomEEGData(np.stack(xdata,0), ydata)
xx1=dataset.__getitem__(10)


validation_split = .5
shuffle_dataset = True
dataset_size = len(dataset)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))
np.random.shuffle(indices)

train_indices, val_indices,test_indices = indices[split:], indices[:int(split/5)],indices[int(split/5):split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)
test_sampler= SubsetRandomSampler(test_indices)

batch_size=32

train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,sampler=valid_sampler)
test_loader= torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)
del xdata
del ydata
del indices
del dataset_size
del split

for x,y in train_loader:
  print(x.shape)
  break
import torch
torch.cuda.empty_cache()
import gc
gc.collect()


print("stage: Pytorch model formation done! ")

loss_fn=torch.nn.L1Loss()
#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.manual_seed(41)
criterion = nn.L1Loss()

#dataset = ConcatDataset([train_dataloader, test_dataloader])
num_epochs = 10
batch_size = 32
k = 10
# splits = KFold(n_splits = k, shuffle = True, random_state = 41)
# foldperf={}
def show_metrics(y_test, y_predict):
    from sklearn.metrics import mean_absolute_error as mae
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()
    pass


def train_epoch(model,start_index,end_index,device,dataloader,loss_fn,optimizer,epochs=1,patience_iterations=4,previous_loss_index=0,index_range=3):
    """
    model: rnn model u r going to use 
    start index: index to get piece of data/slicing start
    end index: slicinf=g index end
    device : device for pu
    dataloader: training dataloader
    lossfn: loss fn
    epochs: no of epochs for each sample
    patience_iterations: wait till no of iterations when train loss is more than val loss; part for rarly stopping, after patience iterations exceed training will stop
    previous_loss_index: index where previois  val loss is less than training loss
    index_range: range  of index where once tarining greater is less than val loss and again tarining loss is less than validation loss and now currently now training loss is greater than validation loss
    """
    train_loss=[]
    val0dation_loss=[]
    model.train().to(device)
    counter_loss=0
    count=0 ## no of times val loss is more than training loss
    previous_loss_index=previous_loss_index
    patience_iterations=patience_iterations
    index_range=index_range # difference between previous index came when val loss is than trainingb loss

    sample_dcount=0
    mylist=[]
    should_break=0
    for epoch in range(epochs):
        
        for i,(eeg_scan, age) in enumerate(dataloader,0): 
            
            sample_dcount+=1
            #model.train().to(device)
            eeg_scan, age0 = eeg_scan.to(device), age.to(device)
            # make 0 gradient initially for optimization
            optimizer.zero_grad()  # gradient : slope
            age=(age0-torch.mean(age0))/(torch.var(age0)).to(device)
            age=age0
            output = model(eeg_scan[:,start_index:end_index,:])
            loss = loss_fn(output,age)
            loss.backward()  #  calculating gradients ,update weights to input 
            optimizer.step()  # move in direction of gradient to get minima

            counter_loss+=loss.item()  
            #val_loss=valid_epoch(model,start_index,end_index)
          
            copy=counter_loss
            n=20
            ii=i
        
        val_loss=valid_epoch(model,start_index,end_index)
        print("epoch: ",epoch," traing loss after 1 batch is:  ",counter_loss/(ii+1),"Validation loss after 20*32 batch is:  ",val_loss)
                #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
        train_loss.append(counter_loss/(ii+1))
        val0dation_loss.append(val_loss)
        x=counter_loss
        counter_loss=0
                #sample_dcount=0
            
          #   if i%n==0 and i!=0:
          #       print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1),"Validation loss after 20*32 batch is:  ",val_loss)
          #       #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
          #       train_loss.append(counter_loss/(n+1))
          #       val0dation_loss.append(val_loss)
          #       x=counter_loss
          #       counter_loss=0
          #       sample_dcount=0
          # # try:
          #   np.mean(val0dation_loss[index_range:])>np.mean(train_loss[index_range:])
          #   should_break+=1
          # except:
          #   pass
        # if should_break>=patience_iterations:
        #   break

        # #continue
        if  val_loss>counter_loss/(ii+1):
          if len(mylist)>patience_iterations:
            break
          else:
            if len(mylist)>0:
              if (epoch-mylist[-1])>index_range:
                mylist=[epoch]
              else:
                mylist.append(epoch)
            else:
              mylist=[epoch]

    plt.plot(val0dation_loss)
    plt.plot(train_loss)
    plt.legend(["val_loss","train-loss"])
    pass

@torch.no_grad()
def valid_epoch(model,start_index,end_index):
    valid_loss= 0.0
    #model.eval()
    count=0
    for i, (x,y) in enumerate(validation_loader):
        eeg_scan, age = x.to(device), y.to(device)
        output = model(eeg_scan[:,start_index:end_index,:])
        loss = loss_fn(output,age)
        valid_loss += loss.item()
        count=i+1
    return valid_loss/count


@torch.no_grad()
def predict( model,start_index=0,end_index=-1):
    ypred=[]
    ytest=[]
    model.eval()  
    for x,y in test_loader:
        yhat = model(x)
        # retrieve numpy array
        yhat = yhat.detach().cpu().numpy()
        #check=[i[0] for i in yhat[0]]
        ypred.extend(yhat)
        #print(ypred.shape)
        ytest.extend(y.cpu().numpy())
   
    # out=[]
    # for i in ypred[0]:
    #     out.append(i.tolist()[0])
    # show_metrics(np.array(ytest),np.array(out))
    return ytest,ypred

import torch, gc
gc.collect()
torch.cuda.empty_cache()

# best accuracy for 2000 sample

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=2,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(16,2))
        self.conv2=nn.Conv2d(1,1,(32,2))
        self.conv3=nn.Conv2d(1,1,(256,1))
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(506*18, 4000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        b=self.batchNorm(a1)
        #print(b.shape)
        c=torch.unsqueeze(b,1)
        #print(c.shape)
        out=self.conv1(c)
        out=self.conv2(out)
        out=torch.nn.MaxPool2d((8,1))(out)
        #print(out.shape)
        #out=self.conv3(out)

        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4096,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4096, 20))  # sample, timeseries, len of one timeseries
print(s)

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=50,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

"""# wavelet prqctice

## fft frequency learning
"""

import numpy as np
import matplotlib.pyplot as plt
sr=1500
Range=15
x=np.linspace(0,Range,sr)
y=100*np.sin(2*np.pi*30*x) # frequency is 100
plt.plot(x,y)
plt.figure()
x=np.linspace(0,Range,sr)
y1=10*np.sin(2*np.pi*20*x) # frequency is 100
plt.figure()
plt.plot(x,y1)
y_fin=y+y1
plt.figure()
plt.plot(x,y_fin,"r")

y_fin_fft=np.fft.fft(y_fin)
y_fin_fft_abs=np.abs(y_fin_fft)
y_fin_fft_shift=np.fft.fftshift(y_fin_fft)
y_fin_fft_abs=np.abs(y_fin_fft_shift)
x_fft=np.arange(-len(y_fin_fft)/2,len(y_fin_fft)/2,1)/Range # Range -15 was our total x limit
plt.plot(x_fft,y_fin_fft_abs,"g")12
plt.xlabel("Frequency in hertz")
plt.ylabel("Frequency amplitude")

"""# Synthetic data generation"""

!pip install pyplnoise
!pip install numpy
!pip install pandas
!pip install matplotlib
!pip install scipy


import pyplnoise
import numpy as np
import matplotlib.pyplot as plt
import scipy
from scipy.stats import zscore
from scipy.signal import butter, lfilter


def butter_bandpass(lowcut, highcut, fs = 128.0, order=5):
    return butter(order, [lowcut, highcut], fs=fs, btype='band')

def butter_bandpass_filter(data, lowcut, highcut, fs = 128.0, order=5):
    b, a = butter_bandpass(lowcut, highcut, fs, order=order)
    y = lfilter(b, a, data)
    return y



def lower_frequency(age):
  fL = (age - 20)/6 + 5
  return fL

def upper_frequency(age):
  fH = (age - 20)/6 + 10
  return fH


def uniform_age(units):
  age = np.linspace(20, 80, units)
  return age

# datapoints = 10*128
# freq_l = lower_frequency(20)
# freq_h = upper_frequency(20)
# b, a = butter_bandpass(freq_l, freq_h)
# x = np.empty((20, datapoints), dtype = 'float32')

units= 5000 # no of samples
age=uniform_age(units)
np.save("/content/gdrive/MyDrive/mitacs/data/Synthetic_Age_32Sec_128hz_5000sample_10channel_06_noise", age)


def sample(age,sample_freq=128,timescale=32,sample=units,alpha=0.6,channel=10):
  datapoints=sample_freq*timescale

  final_sample=[]

  for i in range(sample): # for ith smaple
    freq_l = lower_frequency(age[i])
    freq_h = upper_frequency(age[i])

    
    x = np.random.randn(channel, datapoints)

    sample_i=[]
    for j in range(channel):
      
      a=pyplnoise.PinkNoise(sample_freq,freq_l,freq_h)
      noise=a.get_series(datapoints)
      noise_hat = zscore(noise, axis = 0)

      y = butter_bandpass_filter(x[j][:], freq_l, freq_h)
      y_hat = zscore(y, axis = 0)

      y_hat_new = alpha*noise_hat + (1-alpha)*y_hat
      data_final = zscore(y_hat_new, axis = 0)

      sample_i.append(data_final)
    final_sample.append(sample_i)
    
  return final_sample # shape will be (units,channekls,samplefreq*timescale)


current1=sample(age)

np.save("/content/gdrive/MyDrive/mitacs/data/Synthetic_Data_noise_added_alpha_0.6_32Sec_128hz_5000sample_10channel.npy",current1)

"""## wavelet formation code"""

from google.colab import drive
drive.mount("/content/gdrive/")

import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
!pip install mne
import mne
# Ignore warnings
import warnings
from scipy import stats
!pip install torchinfo
import  torchinfo
from torchinfo import summary
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import torchsummary
from torch.autograd import Variable
import random
import tensorflow as tf
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
from keras.metrics import mean_absolute_error  as mae
from sklearn.metrics import explained_variance_score
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


X_0_8_path="/content/gdrive/MyDrive/mitacs 2023/Synthetic_Data_noise_added_alpha_0.8_2000samples.npy"

index=np.load("/content/gdrive/MyDrive/mitacs 2023/indexes_400.npy",allow_pickle=True)

#data=np.load(X_0_8_path,allow_pickle=True)[index] # only few samples to avoid memory issue
import gc
gc.collect()

import pywt
# Input(2000,20,4096)  # (toal_eeg_recordings,Toatl_channels, Toatal_datapoints)
# output:(2000,10,20,4096) (Samples, Wavelets_pseudo_frequency,Toatal_channels,Toatal_datapoints)

N_Samples=2000 
filter_count=10  # no of pseudo-frequencies used to filter eeg data # how many filters u need between (0,max_freq_value)
N_datapoints= 4096 #Total no of datapoints in 1 channels
sample_rate=128  # sampling rate of eeg data
channels=20

max_freq_value=55 # max pseudo-frequenciy # maximum frequency we can use to filter eeg data
freq_=np.logspace(np.log10(1),np.log10(max_freq_value),filter_count)

# s1=np.logspace(1,55,20) # (base=10)**start,(base==10)**stop
# s2=np.linspace(1,55,20) # (base=1)**start,(base==10)**stop
# plt.plot(s1)
# plt.plot(s2)
print(freq_)
Wavelet_processed_data = np.ndarray(shape=(N_Samples, filter_count, channels, N_datapoints), dtype = 'float32')
pseudo_frequency = sample_rate*pywt.scale2frequency('cgau8',freq_, precision=8) #returns the pseudo-frequencies corresponding to the scales given by  wavelet "cgau8" specified by wname 

# for sample in range(int(data.shape[0]/2),int(data.shape[0])):
#   print(sample)
#   for channel in range(data.shape[1]):
#     present_data=data[sample,channel,:]

#     coef, freqs=pywt.cwt(present_data,pseudo_frequency,'cgau8',1/sample_rate)
#     Wavelet_processed_data[sample,:,channel,:] = coef  # filtered signal for respective frequency

#   gc.collect()

final=[] #np.ndarray(shape=(N_Samples, filter_count*channels, N_datapoints), dtype = 'float32')
for sample in range(0,int(data.shape[0])):
  current = np.ndarray(shape=( filter_count*channels, N_datapoints), dtype = 'float32')
  
  for channel in range(data.shape[1]):
    present_data=data[sample,channel,:]
    coef, freqs=pywt.cwt(present_data,pseudo_frequency,'cgau8',1/sample_rate)
    current=np.concatenate((current,np.real(coef)))
  new=current[filter_count*channels:,]
  final.append(new)

#print(np.array(final).shape)
# check=np.any(np.isnan(np.array(final)))
del data
d=np.array(final)

# due to memory issue i m notrying normalization as given below
#Wavelet_transformed_arrray=stats.zscore(d,axis=2)
#del d

X_0_8_path="/content/gdrive/MyDrive/mitacs 2023/abs_logspace_Wavelet_without_zscore_data_0_400_samples_0_8_.npy"
data=np.load(X_0_8_path,allow_pickle=True)
Wavelet_transformed_arrray=stats.zscore(data,axis=2)
del data

#plt.subplots(200,1,figsize=(25,2));
freq_=np.logspace(np.log10(1),np.log10(55),10)
import time
data=Wavelet_transformed_arrray[17,:,:]
print(data.shape)
count=1
channel=0
for i in range(data.shape[0]):
  if i%10==0:
    channel+=1
  plt.figure(figsize=(25,5));
  if i%10==0:
    count=1
    plt.title("Channel No: {j},wavelet Function scale value : {i}".format(j=channel,i=freq_[count-1]),fontsize=30);
  else:
    plt.title("Channel No: {j},wavelet Function scale value: {i}".format(j=channel,i=freq_[count-1]),fontsize=30);
  count+=1
  plt.plot(data[i,:]);
  plt.show();
  time.sleep(1)
  plt.close()
  plt.clf()

np.save("/content/gdrive/MyDrive/mitacs 2023/real_logspace_Wavelet_without_zscore_data_0_400_samples_0_8_.npy",d)

print(Wavelet_processed_data.shape)
plt.plot(Wavelet_processed_data[10,1,1,:])

"""# wavelet for 0.9 checking lstm

"""

from google.colab import drive
drive.mount("/content/gdrive/")

from google.colab import drive
drive.mount("/content/gdrive/")

import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
!pip install mne
import mne
# Ignore warnings
import warnings

!pip install torchinfo
import  torchinfo
from torchinfo import summary
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import torchsummary
from torch.autograd import Variable
import random
import tensorflow as tf
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
from keras.metrics import mean_absolute_error  as mae
from sklearn.metrics import explained_variance_score
print("stage: libraries importing done")


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
X_pure="/content/gdrive/MyDrive/mitacs 2023/2000_without_alpha.npy"
Y_pure="/content/gdrive/MyDrive/mitacs 2023/Synthetic_2000_Age.npy"

X_0_9_path =  '/content/gdrive/MyDrive/data/synthetic/Synthetic_Data_noise_added_alpha_0.9_2000samples.npy'
XX_0_99_path=  "/content/gdrive/MyDrive/data/synthetic/Synthetic_Data_noise_added_alpha_0.99_2000samples.npy"
Y_path =  '/content/gdrive/MyDrive/mitacs 2023/Synthetic_Age.npy'

print("data loading")
x_0_9_wavelet="/content/gdrive/MyDrive/mitacs 2023/Wavelet_processed_data_0_1000.npy"
xdata=np.load(x_0_9_wavelet,allow_pickle=True)[:10]
ydata=np.load(Y_path,allow_pickle=True)[:10]


#NPY_CustomEEGData : use it  when you load data as 
class NPY_CustomEEGData:
    def __init__(self,x,y,device=device):
      """
      inputs:
      x: npy araay of all edf outputs
      y: age of all data in  npy array in x 
      """
      self.x = torch.from_numpy(np.asarray(x ,dtype=np.float32))#.double()
      self.y = torch.from_numpy(np.asarray(y,dtype=np.float32))#.double()
      self.device=device
        
    def __len__(self):
      return len(self.x)

    def __getitem__(self, idx):
      """
      output: X and Y

      """

      xdata=np.transpose(self.x[idx],(1,0))
      #xdata=self.x[idx]
      return xdata.to(self.device),self.y[idx].to(self.device)
dataset=NPY_CustomEEGData(np.stack(xdata,0), ydata)
xx1=dataset.__getitem__(10)


validation_split = .5
shuffle_dataset = True
dataset_size = len(dataset)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))
np.random.shuffle(indices)

train_indices, val_indices,test_indices = indices[split:], indices[:int(split/5)],indices[int(split/5):split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)
test_sampler= SubsetRandomSampler(test_indices)

batch_size=32

train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,sampler=valid_sampler)
test_loader= torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)
del xdata
del ydata
del indices
del dataset_size
del split

for x,y in train_loader:
  print(x.shape)
  break
import torch
torch.cuda.empty_cache()
import gc
gc.collect()


print("stage: Pytorch model formation done! ")

loss_fn=torch.nn.L1Loss()
#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.manual_seed(41)
criterion = nn.L1Loss()

#dataset = ConcatDataset([train_dataloader, test_dataloader])
num_epochs = 10
batch_size = 32
k = 10
# splits = KFold(n_splits = k, shuffle = True, random_state = 41)
# foldperf={}
def show_metrics(y_test, y_predict):
    from sklearn.metrics import mean_absolute_error as mae
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()
    pass


def train_epoch(model,start_index,end_index,device,dataloader,loss_fn,optimizer,epochs=1,patience_iterations=4,previous_loss_index=0,index_range=3):
    """
    model: rnn model u r going to use 
    start index: index to get piece of data/slicing start
    end index: slicinf=g index end
    device : device for pu
    dataloader: training dataloader
    lossfn: loss fn
    epochs: no of epochs for each sample
    patience_iterations: wait till no of iterations when train loss is more than val loss; part for rarly stopping, after patience iterations exceed training will stop
    previous_loss_index: index where previois  val loss is less than training loss
    index_range: range  of index where once tarining greater is less than val loss and again tarining loss is less than validation loss and now currently now training loss is greater than validation loss
    """
    train_loss=[]
    val0dation_loss=[]
    model.train().to(device)
    counter_loss=0
    count=0 ## no of times val loss is more than training loss
    previous_loss_index=previous_loss_index
    patience_iterations=patience_iterations
    index_range=index_range # difference between previous index came when val loss is than trainingb loss

    sample_dcount=0
    mylist=[]
    should_break=0
    for epoch in range(epochs):
        
        for i,(eeg_scan, age) in enumerate(dataloader,0): 
            
            sample_dcount+=1
            #model.train().to(device)
            eeg_scan, age0 = eeg_scan.to(device), age.to(device)
            # make 0 gradient initially for optimization
            optimizer.zero_grad()  # gradient : slope
            age=(age0-torch.mean(age0))/(torch.var(age0)).to(device)
            age=age0
            output = model(eeg_scan[:,start_index:end_index,:])
            loss = loss_fn(output,age)
            loss.backward()  #  calculating gradients ,update weights to input 
            optimizer.step()  # move in direction of gradient to get minima

            counter_loss+=loss.item()  
            #val_loss=valid_epoch(model,start_index,end_index)
          
            copy=counter_loss
            n=20
            ii=i
        
        val_loss=valid_epoch(model,start_index,end_index)
        print("epoch: ",epoch," traing loss after 1 batch is:  ",counter_loss/(ii+1),"Validation loss after 20*32 batch is:  ",val_loss)
                #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
        train_loss.append(counter_loss/(ii+1))
        val0dation_loss.append(val_loss)
        x=counter_loss
        counter_loss=0
                #sample_dcount=0
            
          #   if i%n==0 and i!=0:
          #       print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1),"Validation loss after 20*32 batch is:  ",val_loss)
          #       #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
          #       train_loss.append(counter_loss/(n+1))
          #       val0dation_loss.append(val_loss)
          #       x=counter_loss
          #       counter_loss=0
          #       sample_dcount=0
          # # try:
          #   np.mean(val0dation_loss[index_range:])>np.mean(train_loss[index_range:])
          #   should_break+=1
          # except:
          #   pass
        # if should_break>=patience_iterations:
        #   break

        # #continue
        if  val_loss>counter_loss/(ii+1):
          if len(mylist)>patience_iterations:
            break
          else:
            if len(mylist)>0:
              if (epoch-mylist[-1])>index_range:
                mylist=[epoch]
              else:
                mylist.append(epoch)
            else:
              mylist=[epoch]

    plt.plot(val0dation_loss)
    plt.plot(train_loss)
    plt.legend(["val_loss","train-loss"])
    pass

@torch.no_grad()
def valid_epoch(model,start_index,end_index):
    valid_loss= 0.0
    #model.eval()
    count=0
    for i, (x,y) in enumerate(validation_loader):
        eeg_scan, age = x.to(device), y.to(device)
        output = model(eeg_scan[:,start_index:end_index,:])
        loss = loss_fn(output,age)
        valid_loss += loss.item()
        count=i+1
    return valid_loss/count


@torch.no_grad()
def predict( model,start_index=0,end_index=-1):
    ypred=[]
    ytest=[]
    model.eval()  
    for x,y in test_loader:
        yhat = model(x)
        # retrieve numpy array
        yhat = yhat.detach().cpu().numpy()
        #check=[i[0] for i in yhat[0]]
        ypred.extend(yhat)
        #print(ypred.shape)
        ytest.extend(y.cpu().numpy())
   
    # out=[]
    # for i in ypred[0]:
    #     out.append(i.tolist()[0])
    # show_metrics(np.array(ytest),np.array(out))
    return ytest,ypred

import torch, gc
gc.collect()
torch.cuda.empty_cache()

import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
!pip install mne
import mne
# Ignore warnings
import warnings

!pip install torchinfo
import  torchinfo
from torchinfo import summary
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import torchsummary
from torch.autograd import Variable
import random
import tensorflow as tf
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
from keras.metrics import mean_absolute_error  as mae
from sklearn.metrics import explained_variance_score
print("stage: libraries importing done")

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=2,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(16,2))
        self.conv2=nn.Conv2d(1,1,(32,2))
        self.conv3=nn.Conv2d(1,1,(256,1))
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(506*18, 4000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        print(x.shape,"input shape")
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,1,:,:]
        print(x.shape,"updated shape")
        

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        print(a1.shape,h1.shape)
        
        #b=self.batchNorm(a1)
        #print(b.shape)
        c=torch.unsqueeze(a1,1)
        #print(c.shape)
        out=self.conv1(c)
        out=self.conv2(out)
        out=torch.nn.MaxPool2d((8,1))(out)
        #print(out.shape)
        #out=self.conv3(out)

        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=20,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,20,4096,20))  # sample, timeseries, len of one timeseries
print(s)

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=50,previous_loss_index=0,index_range=1)

a=[   #samples
    [    # wavelet
        [22,33],[33,44]],
   
   [
       [44,55],[66,77]]
   ]
np.array(a).shape

import numpy as np
x = np.zeros((3, 4, 5))
np.moveaxis(x, 0, -1).shape

"""# data saving edf files"""

import os
import pandas as pd
import numpy as np


from edf_preprocessing import slice_edfs

import pandas as pd
import mne
import warnings
warnings.filterwarnings('ignore')
mne.set_log_level('warning')

source_folder = "/home/adarsh/projects/rpp-doesburg/databases/eeg_fha/release_001/edf/Burnaby"
target_folder = "/home/adarsh/projects/rpp-doesburg/vpa20/Shared/Synthetic/eeg_fragments_32sec_128HZ_new/"


labels_file = pd.read_csv(r"/home/adarsh/projects/rpp-doesburg/adarsh/data/edf_10sec/age_ScanID.csv")

scan_ids = labels_file[~(labels_file['AgeYears'].isnull())]['ScanID']
scan_ids = labels['ScanID']

# takes scan ids from the list, look for them in source folder, and apply the preprocessing to 100 files in total
# filter the data between 1 Hz and 55 Hz, resample to 200 Hz, extract 1 segment of 10 seconds from each EDF file
# saves new segments as EDF into target folder

print("preprocessing start..")

slice_edfs(source_scan_ids=scan_ids, source_folder=source_folder, target_folder=target_folder,
           target_frequency=128, lfreq=1, hfreq=55, target_length=32, target_segments=1, nfiles=None)

print("preprpocessing done")

import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
import mne
# Ignore warnings
import warnings
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import random
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
X_path =  '/home/adarsh/projects/rpp-doesburg/vpa20/Shared/EEG_CNN/EDF_128Hz_16sec'
Y_path =  '/home/adarsh/projects/rpp-doesburg/vpa20/age_ScanID.csv'


files =os.listdir(X_path)
len(files)
final_xdata=[]
final_ydata=[]
for i in files :
    x = mne.io.read_raw_edf(os.path.join(X_path,i), verbose=False)
    freq = int(dict(x.info)['sfreq'])
    xdata1 = x.get_data()[:,:-freq]
    final_xdata.append(xdata1)
    
    id = x.filenames[0][-40:-4]
    id1 = pd.DataFrame([id],columns=["ScanID"])
    
    age1 = pd.merge(id1,pd.read_csv(Y_path),how="inner").iloc[:,4].tolist()[0]
    final_ydata.append(age1)
    
    #break
print(np.array(final_xdata).shape,np.array(final_ydata).shape)\


#np.stack(np.array(xfinal),axis=0).shape
xdata_20_2048=np.array(final_xdata)
ydata_20_2048=np.array(final_ydata)
np.save("/home/adarsh/projects/rpp-doesburg/adarsh/data/edf_20_2048/xdata_20_2048",xdata_20_2048)
np.save("/home/adarsh/projects/rpp-doesburg/adarsh/data/edf_20_2048/ydata_20_2048",ydata_20_2048)

from edf_preprocessing import load_edf_data

folder = target_folder
label_file = labels_file

# X - np_array of shape (n_samples, 20, length is seconds * frequency), 
# labels - pd.DataFrame with scan_ids and age, same length as X
X, labels = load_edf_data(folder, label_file)

np.save("/home/adarsh/projects/rpp-doesburg/vpa20/Shared/Synthetic/eeg_fragments_32sec_128HZ_new_X",X)
np.save("/home/adarsh/projects/rpp-doesburg/vpa20/Shared/Synthetic/eeg_fragments_32sec_128HZ_new_Y",labels)

#!/bin/bash
#SBATCH --account=rpp-doesburg
#SBATCH --time=30:0:0
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=200G
#SBATCH --output=/home/adarsh/projects/rpp-doesburg/adarsh/pytorch/preprocess/main1.out
#SBATCH --error=/home/adarsh/projects/rpp-doesburg/adarsh/pytorch/preprocess/main1.err
pytorch/preprocess/untitled.py

echo "HII Adarsh, main_prep.sh going to run...."
python /home/adarsh/projects/rpp-doesburg/adarsh/pytorch/preprocess/main1.py

"""#CNN for wavelets

"""

import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
!pip install mne
import mne
# Ignore warnings
import warnings

!pip install torchinfo
import  torchinfo
from torchinfo import summary
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import torchsummary
from torch.autograd import Variable
import random
import tensorflow as tf
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
from keras.metrics import mean_absolute_error  as mae
from sklearn.metrics import explained_variance_score
print("stage: libraries importing done")

from google.colab import drive
drive.mount("/content/gdrive/")

import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
!pip install mne
import mne
# Ignore warnings
import warnings

!pip install torchinfo
import  torchinfo
from torchinfo import summary
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import torchsummary
from torch.autograd import Variable
import random
import tensorflow as tf
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
from keras.metrics import mean_absolute_error  as mae
from sklearn.metrics import explained_variance_score

from scipy import stats

print("stage: libraries importing done")


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
X_pure="/content/gdrive/MyDrive/mitacs 2023/2000_without_alpha.npy"
Y_pure="/content/gdrive/MyDrive/mitacs 2023/Synthetic_2000_Age.npy"

X_0_9_path =  '/content/gdrive/MyDrive/data/synthetic/Synthetic_Data_noise_added_alpha_0.9_2000samples.npy'
XX_0_99_path=  "/content/gdrive/MyDrive/data/synthetic/Synthetic_Data_noise_added_alpha_0.99_2000samples.npy"
Y_path =  '/content/gdrive/MyDrive/data/synthetic/Synthetic_Age.npy'

x1=np.load(X_pure,allow_pickle=True)
xdata=stats.zscore(x1,axis=2)

ydata=np.load(Y_pure,allow_pickle=True)


#NPY_CustomEEGData : use it  when you load data as 
class NPY_CustomEEGData:
    def __init__(self,x,y,device=device):
      """
      inputs:
      x: npy araay of all edf outputs
      y: age of all data in  npy array in x 
      """
      self.x = torch.from_numpy(np.asarray(x ,dtype=np.float32))#.double()
      self.y = torch.from_numpy(np.asarray(y,dtype=np.float32))#.double()
      self.device=device
        
    def __len__(self):
      return len(self.x)

    def __getitem__(self, idx):
      """
      output: X and Y

      """

      xdata=np.transpose(self.x[idx],(1,0))
      #xdata=self.x[idx]
      return xdata.to(self.device),self.y[idx].to(self.device)
dataset=NPY_CustomEEGData(np.stack(xdata,0), ydata)
xx1=dataset.__getitem__(10)


validation_split = .5
shuffle_dataset = True
dataset_size = len(dataset)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))
np.random.shuffle(indices)

train_indices, val_indices,test_indices = indices[split:], indices[:int(split/5)],indices[int(split/5):split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)
test_sampler= SubsetRandomSampler(test_indices)

batch_size=32

train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,sampler=valid_sampler)
test_loader= torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)
del xdata
del ydata
del indices
del dataset_size
del split

for x,y in train_loader:
  print(x.shape)
  break
import torch
torch.cuda.empty_cache()
import gc
gc.collect()


print("stage: Pytorch model formation done! ")

loss_fn=torch.nn.L1Loss()
#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.manual_seed(41)
criterion = nn.L1Loss()

#dataset = ConcatDataset([train_dataloader, test_dataloader])
num_epochs = 10
batch_size = 32
k = 10
# splits = KFold(n_splits = k, shuffle = True, random_state = 41)
# foldperf={}
def show_metrics(y_test, y_predict):
    from sklearn.metrics import mean_absolute_error as mae
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()
    pass


def train_epoch(model,start_index,end_index,device,dataloader,loss_fn,optimizer,epochs=1,patience_iterations=4,previous_loss_index=0,index_range=3):
    """
    model: rnn model u r going to use 
    start index: index to get piece of data/slicing start
    end index: slicinf=g index end
    device : device for pu
    dataloader: training dataloader
    lossfn: loss fn
    epochs: no of epochs for each sample
    patience_iterations: wait till no of iterations when train loss is more than val loss; part for rarly stopping, after patience iterations exceed training will stop
    previous_loss_index: index where previois  val loss is less than training loss
    index_range: range  of index where once tarining greater is less than val loss and again tarining loss is less than validation loss and now currently now training loss is greater than validation loss
    """
    train_loss=[]
    val0dation_loss=[]
    model.train().to(device)
    counter_loss=0
    count=0 ## no of times val loss is more than training loss
    previous_loss_index=previous_loss_index
    patience_iterations=patience_iterations
    index_range=index_range # difference between previous index came when val loss is than trainingb loss

    sample_dcount=0
    mylist=[]
    should_break=0
    for epoch in range(epochs):
        
        for i,(eeg_scan, age) in enumerate(dataloader,0): 
            
            sample_dcount+=1
            #model.train().to(device)
            eeg_scan, age0 = eeg_scan.to(device), age.to(device)
            # make 0 gradient initially for optimization
            optimizer.zero_grad()  # gradient : slope
            age=(age0-torch.mean(age0))/(torch.var(age0)).to(device)
            age=age0
            output = model(eeg_scan[:,start_index:end_index,:])
            loss = loss_fn(output,age)
            loss.backward()  #  calculating gradients ,update weights to input 
            optimizer.step()  # move in direction of gradient to get minima

            counter_loss+=loss.item()  
            #val_loss=valid_epoch(model,start_index,end_index)
          
            copy=counter_loss
            n=20
            ii=i
        
        val_loss=valid_epoch(model,start_index,end_index)
        print("epoch: ",epoch," traing loss after 1 batch is:  ",counter_loss/(ii+1),"Validation loss after 20*32 batch is:  ",val_loss)
                #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
        train_loss.append(counter_loss/(ii+1))
        val0dation_loss.append(val_loss)
        x=counter_loss
        counter_loss=0
                #sample_dcount=0
            
          #   if i%n==0 and i!=0:
          #       print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1),"Validation loss after 20*32 batch is:  ",val_loss)
          #       #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
          #       train_loss.append(counter_loss/(n+1))
          #       val0dation_loss.append(val_loss)
          #       x=counter_loss
          #       counter_loss=0
          #       sample_dcount=0
          # # try:
          #   np.mean(val0dation_loss[index_range:])>np.mean(train_loss[index_range:])
          #   should_break+=1
          # except:
          #   pass
        # if should_break>=patience_iterations:
        #   break

        # #continue
        if  val_loss>counter_loss/(ii+1):
          if len(mylist)>patience_iterations:
            break
          else:
            if len(mylist)>0:
              if (epoch-mylist[-1])>index_range:
                mylist=[epoch]
              else:
                mylist.append(epoch)
            else:
              mylist=[epoch]

    plt.plot(val0dation_loss)
    plt.plot(train_loss)
    plt.legend(["val_loss","train-loss"])
    pass

@torch.no_grad()
def valid_epoch(model,start_index,end_index):
    valid_loss= 0.0
    #model.eval()
    count=0
    for i, (x,y) in enumerate(validation_loader):
        eeg_scan, age = x.to(device), y.to(device)
        output = model(eeg_scan[:,start_index:end_index,:])
        loss = loss_fn(output,age)
        valid_loss += loss.item()
        count=i+1
    return valid_loss/count


@torch.no_grad()
def predict( model,start_index=0,end_index=-1):
    ypred=[]
    ytest=[]
    model.eval()  
    for x,y in test_loader:
        yhat = model(x)
        # retrieve numpy array
        yhat = yhat.detach().cpu().numpy()
        #check=[i[0] for i in yhat[0]]
        ypred.extend(yhat)
        #print(ypred.shape)
        ytest.extend(y.cpu().numpy())
   
    # out=[]
    # for i in ypred[0]:
    #     out.append(i.tolist()[0])
    # show_metrics(np.array(ytest),np.array(out))
    return ytest,ypred

import torch, gc
gc.collect()
torch.cuda.empty_cache()

# best accuracy for 2000 sample

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=2,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(16,2))
        self.conv2=nn.Conv2d(1,1,(32,2))
        self.conv3=nn.Conv2d(1,1,(256,1))
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(506*18, 4000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        b=self.batchNorm(a1)
        #print(b.shape)
        c=torch.unsqueeze(b,1)
        #print(c.shape)
        out=self.conv1(c)
        out=self.conv2(out)
        out=torch.nn.MaxPool2d((8,1))(out)
        #print(out.shape)
        #out=self.conv3(out)

        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4096,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4096, 20))  # sample, timeseries, len of one timeseries
print(s)

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=50,previous_loss_index=0,index_range=1)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=2,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(20,20,(16,2))
        self.conv2=nn.Conv2d(20,1,(32,2))
        self.conv3=nn.Conv2d(1,1,(256,1))
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(1012*9, 5000)
        self.l1=nn.Linear(5000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        #self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(250,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        print(x.shape,"input shape")
        #x=torch.unsqueeze(x[:,100:200,:],0)
        #x=x[:,:,:,:]
        
        

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        
        #a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        
        #b=self.batchNorm(a1)
        #print(b.shape)
        #c=torch.unsqueeze(a1,1)
        #print(c.shape)


        out=self.conv1(x)
        
        out=self.conv2(out)
        
        out=torch.nn.MaxPool2d((4,2))(out)
        print(out.shape)
        #out=self.conv3(out)

        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        #out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=20,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,20,4096,20))  # sample, timeseries, len of one timeseries
print(s)

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=40,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=2,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(16,2))
        self.conv2=nn.Conv2d(1,1,(32,2))
        self.conv3=nn.Conv2d(1,1,(64,3))
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(474*8, 4000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],40)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],40)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        b=self.batchNorm(a1)  #4096,40
        #print(b.shape)
        c=torch.unsqueeze(b,1)
        #print(c.shape)
        out=self.conv1(c)    #4081,39
        out=self.conv2(out)  #4050,38
        out=torch.nn.MaxPool2d((4,2))(out)   #1012,19
        out=self.conv3(out)                  #949, 17
        out=torch.nn.MaxPool2d((2,2))(out)   #474, 8
        #print(out.shape)
        #out=self.conv3(out)

        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4096,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4096, 20))  # sample, timeseries, len of one timeseries
print(s)

"""# fresh model 1

"""

from google.colab import drive
drive.mount("/content/gdrive/")

import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
!pip install mne
import mne
# Ignore warnings
import warnings

!pip install torchinfo
import  torchinfo
from torchinfo import summary
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import torchsummary
from torch.autograd import Variable
import random
import tensorflow as tf
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
from keras.metrics import mean_absolute_error  as mae
from sklearn.metrics import explained_variance_score

from scipy import stats

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

X_wavelet_0_99="/content/gdrive/MyDrive/mitacs 2023/eeg_fragments_32sec_128HZ_new_X.npy"
Y_wavelet_0_99="/content/gdrive/MyDrive/mitacs 2023/eeg_fragments_32sec_128HZ_new_Y.npy"

xdata=np.load(X_wavelet_0_99,allow_pickle=True)
# x2=stats.zscore(x1,axis=2)
# xdata=np.nan_to_num(x2)


ydata=np.load(Y_wavelet_0_99,allow_pickle=True)
#NPY_CustomEEGData : use it  when you load data as 
class NPY_CustomEEGData:
    def __init__(self,x,y,device=device):
      """
      inputs:
      x: npy araay of all edf outputs
      y: age of all data in  npy array in x 
      """
      self.x = torch.from_numpy(np.asarray(x ,dtype=np.float32))#.double()
      #self.y = torch.from_numpy(np.asarray(y,dtype=np.float32))#.double()
      self.y=y
      self.device=device
        
    def __len__(self):
      return len(self.x)

    def __getitem__(self, idx):
      """
      output: X and Y

      """
      # xdata=np.transpose(self.x[idx],(1,0))
      x1=np.transpose(self.x[idx],(1,0))
      #print(xdata1.shape)
      xdata1=stats.zscore(x1,axis=0)
      xdata=torch.nan_to_num(xdata1)
      #xdata=torch.moveaxis(self.x[idx],-1,-2)  # samples, wavelets,datapoints,chaneels
      yy=torch.from_numpy(np.asarray(self.y[idx][1],dtype=np.float32))#.double()
      #xdata=self.x[idx]
        
      #print(torch.any(torch.isnan(xdata)),"dataloader checking")
      return xdata.to(self.device),   yy.to(self.device) #self.y[idx].to(self.device)
dataset=NPY_CustomEEGData(np.stack(xdata,0), ydata)
xx1=dataset.__getitem__(10)

validation_split = .5
shuffle_dataset = True
dataset_size = len(dataset)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))
np.random.shuffle(indices)

train_indices, val_indices,test_indices = indices[split:], indices[:int(split/5)],indices[int(split/5):split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)
test_sampler= SubsetRandomSampler(test_indices)

batch_size=32

train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,sampler=valid_sampler)
test_loader= torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)
del xdata
del ydata
del indices
del dataset_size
del split

for x,y in train_loader:
  print(x.shape)

  break
import torch
torch.cuda.empty_cache()
import gc
gc.collect()

print("stage: Pytorch model formation done! ")

loss_fn=torch.nn.L1Loss()
#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.manual_seed(41)
criterion = nn.L1Loss()

#dataset = ConcatDataset([train_dataloader, test_dataloader])
num_epochs = 10
batch_size = 32
k = 10
# splits = KFold(n_splits = k, shuffle = True, random_state = 41)
# foldperf={}
def show_metrics(y_test, y_predict):
    from sklearn.metrics import mean_absolute_error as mae
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()
    pass


def train_epoch(model,start_index,end_index,device,dataloader,loss_fn,optimizer,epochs=1,patience_iterations=4,previous_loss_index=0,index_range=3):
    """
    model: rnn model u r going to use 
    start index: index to get piece of data/slicing start
    end index: slicinf=g index end
    device : device for pu
    dataloader: training dataloader
    lossfn: loss fn
    epochs: no of epochs for each sample
    patience_iterations: wait till no of iterations when train loss is more than val loss; part for rarly stopping, after patience iterations exceed training will stop
    previous_loss_index: index where previois  val loss is less than training loss
    index_range: range  of index where once tarining greater is less than val loss and again tarining loss is less than validation loss and now currently now training loss is greater than validation loss
    """
    train_loss=[]
    val0dation_loss=[]
    model.train().to(device)
    counter_loss=0
    count=0 ## no of times val loss is more than training loss
    previous_loss_index=previous_loss_index
    patience_iterations=patience_iterations
    index_range=index_range # difference between previous index came when val loss is than trainingb loss

    sample_dcount=0
    mylist=[]
    should_break=0
    for epoch in range(epochs):
        
        for i,(eeg_scan, age) in enumerate(dataloader,0): 
            
            sample_dcount+=1
            #model.train().to(device)
            eeg_scan, age0 = eeg_scan.to(device), age.to(device)
            # make 0 gradient initially for optimization
            optimizer.zero_grad()  # gradient : slope
            age=(age0-torch.mean(age0))/(torch.var(age0)).to(device)
            age=age0
            output = model(eeg_scan[:,start_index:end_index,:])
            loss = loss_fn(output,age)
            loss.backward()  #  calculating gradients ,update weights to input 
            optimizer.step()  # move in direction of gradient to get minima

            counter_loss+=loss.item()  
            #val_loss=valid_epoch(model,start_index,end_index)
          
            copy=counter_loss
            n=20
            ii=i
        
        val_loss=valid_epoch(model,start_index,end_index)
        print("epoch: ",epoch," traing loss after 1 batch is:  ",counter_loss/(ii+1),"Validation loss after 20*32 batch is:  ",val_loss)
                #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
        train_loss.append(counter_loss/(ii+1))
        val0dation_loss.append(val_loss)
        x=counter_loss
        counter_loss=0
                #sample_dcount=0
            
          #   if i%n==0 and i!=0:
          #       print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1),"Validation loss after 20*32 batch is:  ",val_loss)
          #       #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
          #       train_loss.append(counter_loss/(n+1))
          #       val0dation_loss.append(val_loss)
          #       x=counter_loss
          #       counter_loss=0
          #       sample_dcount=0
          # # try:
          #   np.mean(val0dation_loss[index_range:])>np.mean(train_loss[index_range:])
          #   should_break+=1
          # except:
          #   pass
        # if should_break>=patience_iterations:
        #   break

        # #continue
        if  val_loss>counter_loss/(ii+1):
          if len(mylist)>patience_iterations:
            break
          else:
            if len(mylist)>0:
              if (epoch-mylist[-1])>index_range:
                mylist=[epoch]
              else:
                mylist.append(epoch)
            else:
              mylist=[epoch]

    plt.plot(val0dation_loss)
    plt.plot(train_loss)
    plt.legend(["val_loss","train-loss"])
    pass

@torch.no_grad()
def valid_epoch(model,start_index,end_index):
    valid_loss= 0.0
    #model.eval()
    count=0
    for i, (x,y) in enumerate(validation_loader):
        eeg_scan, age = x.to(device), y.to(device)
        output = model(eeg_scan[:,start_index:end_index,:])
        loss = loss_fn(output,age)
        valid_loss += loss.item()
        count=i+1
    return valid_loss/count


@torch.no_grad()
def predict( model,start_index=0,end_index=-1):
    ypred=[]
    ytest=[]
    model.eval()  
    for x,y in test_loader:
        yhat = model(x)
        # retrieve numpy array
        yhat = yhat.detach().cpu().numpy()
        #check=[i[0] for i in yhat[0]]
        ypred.extend(yhat)
        #print(ypred.shape)
        ytest.extend(y.cpu().numpy())
   
    # out=[]
    # for i in ypred[0]:
    #     out.append(i.tolist()[0])
    # show_metrics(np.array(ytest),np.array(out))
    return ytest,ypred

import torch, gc
gc.collect()
torch.cuda.empty_cache()


# best accuracy for 2000 sample

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=2,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(16,2))
        self.conv2=nn.Conv2d(1,1,(32,2))
        self.conv3=nn.Conv2d(1,1,(256,1))
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(506*18, 4000)
        self.l1=nn.Linear(4000,100)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,1)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        b=self.batchNorm(a1)
        #print(b.shape)
        c=torch.unsqueeze(b,1)
        #print(c.shape)
        out=self.conv1(c)
        out=self.conv2(out)
        out=torch.nn.MaxPool2d((8,1))(out)
        #print(out.shape)
        #out=self.conv3(out)

        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        #out=self.l2(out)
        #out=self.l3(out)
        #out=self.l4(out)
        #out=self.l5(out)
        out=self.l6(out)
        #out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4096,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4096, 20))  # sample, timeseries, len of one timeseries
print(s)

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=25,patience_iterations=50,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])


#path="/home/adarsh/projects/rpp-doesburg/adarsh/pytorch/preprocess/model_history2"
#torch.save(model.state_dict(), path)



plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

"""# task new

"""

import numpy as np
s1=np.logspace(1,55,20) # (base=10)**start,(base==10)**stop
s2=np.linspace(1,55,20) # (base=1)**start,(base==10)**stop
plt.plot(s1)
plt.plot(s2)

"""# comparison between 0.9 % synthetic a nd wavelets """

from google.colab import drive
drive.mount("/content/gdrive/")

import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
!pip install mne
import mne
# Ignore warnings
import warnings

!pip install torchinfo
import  torchinfo
from torchinfo import summary
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import torchsummary
from torch.autograd import Variable
import random
import tensorflow as tf
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
from keras.metrics import mean_absolute_error  as mae
from sklearn.metrics import explained_variance_score
print("stage: libraries importing done")


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
X_pure="/content/gdrive/MyDrive/data/synthetic/2000_without_alpha.npy"
Y_pure="/content/gdrive/MyDrive/data/synthetic/Synthetic_2000_Age.npy"

X_0_9_path =  '/content/gdrive/MyDrive/mitacs 2023/Synthetic_Data_noise_added_alpha_0.9_2000samples.npy'
XX_0_99_path=  "/content/gdrive/MyDrive/data/synthetic/Synthetic_Data_noise_added_alpha_0.99_2000samples.npy"
Y_path =  '/content/gdrive/MyDrive/mitacs 2023/Synthetic_Age.npy'

index=np.load("/content/gdrive/MyDrive/mitacs 2023/indexes_400.npy",allow_pickle=True)
xdata=np.load(X_0_9_path,allow_pickle=True)[index]
ydata=np.load(Y_path,allow_pickle=True)[index]


#NPY_CustomEEGData : use it  when you load data as 
class NPY_CustomEEGData:
    def __init__(self,x,y,device=device):
      """
      inputs:
      x: npy araay of all edf outputs
      y: age of all data in  npy array in x 
      """
      self.x = torch.from_numpy(np.asarray(x ,dtype=np.float32))#.double()
      self.y = torch.from_numpy(np.asarray(y,dtype=np.float32))#.double()
      self.device=device
        
    def __len__(self):
      return len(self.x)

    def __getitem__(self, idx):
      """
      output: X and Y

      """

      xdata=np.transpose(self.x[idx],(1,0))
      #xdata=self.x[idx]
      return xdata.to(self.device),self.y[idx].to(self.device)
dataset=NPY_CustomEEGData(np.stack(xdata,0), ydata)
xx1=dataset.__getitem__(10)


validation_split = .5
shuffle_dataset = True
dataset_size = len(dataset)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))
np.random.shuffle(indices)

train_indices, val_indices,test_indices = indices[split:], indices[:int(split/5)],indices[int(split/5):split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)
test_sampler= SubsetRandomSampler(test_indices)

batch_size=32

train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,sampler=valid_sampler)
test_loader= torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)
del xdata
del ydata
del indices
del dataset_size
del split

for x,y in train_loader:
  print(x.shape)
  break
import torch
torch.cuda.empty_cache()
import gc
gc.collect()


print("stage: Pytorch model formation done! ")

loss_fn=torch.nn.L1Loss()
#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.manual_seed(41)
criterion = nn.L1Loss()

#dataset = ConcatDataset([train_dataloader, test_dataloader])
num_epochs = 10
batch_size = 32
k = 10
# splits = KFold(n_splits = k, shuffle = True, random_state = 41)
# foldperf={}
def show_metrics(y_test, y_predict):
    from sklearn.metrics import mean_absolute_error as mae
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()
    pass


def train_epoch(model,start_index,end_index,device,dataloader,loss_fn,optimizer,epochs=1,patience_iterations=4,previous_loss_index=0,index_range=3):
    """
    model: rnn model u r going to use 
    start index: index to get piece of data/slicing start
    end index: slicinf=g index end
    device : device for pu
    dataloader: training dataloader
    lossfn: loss fn
    epochs: no of epochs for each sample
    patience_iterations: wait till no of iterations when train loss is more than val loss; part for rarly stopping, after patience iterations exceed training will stop
    previous_loss_index: index where previois  val loss is less than training loss
    index_range: range  of index where once tarining greater is less than val loss and again tarining loss is less than validation loss and now currently now training loss is greater than validation loss
    """
    train_loss=[]
    val0dation_loss=[]
    model.train().to(device)
    counter_loss=0
    count=0 ## no of times val loss is more than training loss
    previous_loss_index=previous_loss_index
    patience_iterations=patience_iterations
    index_range=index_range # difference between previous index came when val loss is than trainingb loss

    sample_dcount=0
    mylist=[]
    should_break=0
    for epoch in range(epochs):
        
        for i,(eeg_scan, age) in enumerate(dataloader,0): 
            
            sample_dcount+=1
            #model.train().to(device)
            eeg_scan, age0 = eeg_scan.to(device), age.to(device)
            # make 0 gradient initially for optimization
            optimizer.zero_grad()  # gradient : slope
            age=(age0-torch.mean(age0))/(torch.var(age0)).to(device)
            age=age0
            output = model(eeg_scan[:,start_index:end_index,:])
            loss = loss_fn(output,age)
            loss.backward()  #  calculating gradients ,update weights to input 
            optimizer.step()  # move in direction of gradient to get minima

            counter_loss+=loss.item()  
            #val_loss=valid_epoch(model,start_index,end_index)
          
            copy=counter_loss
            n=20
            ii=i
        
        val_loss=valid_epoch(model,start_index,end_index)
        print("epoch: ",epoch," traing loss after 1 batch is:  ",counter_loss/(ii+1),"Validation loss after 20*32 batch is:  ",val_loss)
                #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
        train_loss.append(counter_loss/(ii+1))
        val0dation_loss.append(val_loss)
        x=counter_loss
        counter_loss=0
                #sample_dcount=0
            
          #   if i%n==0 and i!=0:
          #       print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1),"Validation loss after 20*32 batch is:  ",val_loss)
          #       #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
          #       train_loss.append(counter_loss/(n+1))
          #       val0dation_loss.append(val_loss)
          #       x=counter_loss
          #       counter_loss=0
          #       sample_dcount=0
          # # try:
          #   np.mean(val0dation_loss[index_range:])>np.mean(train_loss[index_range:])
          #   should_break+=1
          # except:
          #   pass
        # if should_break>=patience_iterations:
        #   break

        # #continue
        if  val_loss>counter_loss/(ii+1):
          if len(mylist)>patience_iterations:
            break
          else:
            if len(mylist)>0:
              if (epoch-mylist[-1])>index_range:
                mylist=[epoch]
              else:
                mylist.append(epoch)
            else:
              mylist=[epoch]

    plt.plot(val0dation_loss)
    plt.plot(train_loss)
    plt.legend(["val_loss","train-loss"])
    pass

@torch.no_grad()
def valid_epoch(model,start_index,end_index):
    valid_loss= 0.0
    #model.eval()
    count=0
    for i, (x,y) in enumerate(validation_loader):
        eeg_scan, age = x.to(device), y.to(device)
        output = model(eeg_scan[:,start_index:end_index,:])
        loss = loss_fn(output,age)
        valid_loss += loss.item()
        count=i+1
    return valid_loss/count


@torch.no_grad()
def predict( model,start_index=0,end_index=-1):
    ypred=[]
    ytest=[]
    model.eval()  
    for x,y in test_loader:
        yhat = model(x)
        # retrieve numpy array
        yhat = yhat.detach().cpu().numpy()
        #check=[i[0] for i in yhat[0]]
        ypred.extend(yhat)
        #print(ypred.shape)
        ytest.extend(y.cpu().numpy())
   
    # out=[]
    # for i in ypred[0]:
    #     out.append(i.tolist()[0])
    # show_metrics(np.array(ytest),np.array(out))
    return ytest,ypred

import torch, gc
gc.collect()
torch.cuda.empty_cache()

# best accuracy for 2000 sample

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=2,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(16,2))
        self.conv2=nn.Conv2d(1,1,(32,2))
        self.conv3=nn.Conv2d(1,1,(256,1))
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(506*18, 400)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(400,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        b=self.batchNorm(a1)
        #print(b.shape)
        c=torch.unsqueeze(b,1)
        #print(c.shape)
        out=self.conv1(c)
        out=self.conv2(out)
        out=torch.nn.MaxPool2d((8,1))(out)
        #print(out.shape)
        #out=self.conv3(out)

        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        #out=self.l1(out)
        #out=self.l2(out)
        #out=self.l3(out)
        #out=self.l4(out)
        #out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4096,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4096, 20))  # sample, timeseries, len of one timeseries
print(s)

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=50,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

"""## waveletion section for same data"""

from google.colab import drive
drive.mount("/content/gdrive/")

print("stage1................. 10:51am start")
import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
!pip install mne
import mne
# Ignore warnings
import warnings

!pip install torchinfo
from scipy import stats
import  torchinfo
from torchinfo import summary
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import torchsummary
from torch.autograd import Variable
import random
import tensorflow as tf
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
from keras.metrics import mean_absolute_error  as mae
from sklearn.metrics import explained_variance_score
print("stage: libraries importing done")


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
index=np.load("/content/gdrive/MyDrive/mitacs 2023/indexes_400.npy",allow_pickle=True)
Y_path =  '/content/gdrive/MyDrive/mitacs 2023/Synthetic_Age.npy'

wavelet_un_normalized_data="/content/gdrive/MyDrive/mitacs 2023/Wavelet_without_zscore_data_0_400_samples_0_9_.npy"
d=np.load(wavelet_un_normalized_data,allow_pickle=True)
xdata=stats.zscore(d,axis=2)
del d
ydata=np.load(Y_path,allow_pickle=True)[index]


print("stage1.................")
#NPY_CustomEEGData : use it  when you load data as 
class NPY_CustomEEGData:
    def __init__(self,x,y,device=device):
      """
      inputs:
      x: npy araay of all edf outputs
      y: age of all data in  npy array in x 
      """
      self.x = torch.from_numpy(np.asarray(x ,dtype=np.float32))#.double()
      #self.y = torch.from_numpy(np.asarray(y,dtype=np.float32))#.double()
      self.y=y
      self.device=device
        
    def __len__(self):
      return len(self.x)

    def __getitem__(self, idx):
      """
      output: X and Y

      """
      # xdata=np.transpose(self.x[idx],(1,0))
      xdata=np.transpose(self.x[idx],(1,0))
      #print(xdata1.shape)
      #xdata=stats.zscore(xdata1,axis=0)
      #xdata=torch.moveaxis(self.x[idx],-1,-2)  # samples, wavelets,datapoints,chaneels
      yy=torch.from_numpy(np.asarray(self.y[idx],dtype=np.float32))#.double()
      #xdata=self.x[idx]
        
      #print(torch.any(torch.isnan(xdata)),"dataloader checking")
      return xdata.to(self.device),   yy.to(self.device) #self.y[idx].to(self.device)
dataset=NPY_CustomEEGData(np.stack(xdata,0), ydata)
xx1=dataset.__getitem__(10)



validation_split = .5
shuffle_dataset = True
dataset_size = len(dataset)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))
np.random.shuffle(indices)

train_indices, val_indices,test_indices = indices[split:], indices[:int(split/5)],indices[int(split/5):split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)
test_sampler= SubsetRandomSampler(test_indices)

batch_size=32

train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,sampler=valid_sampler)
test_loader= torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)
print("stage1.................")
del xdata
del ydata
del indices
del dataset_size
del split

for x,y in train_loader:
  print(x.shape)

  break
import torch
torch.cuda.empty_cache()
import gc
gc.collect()

print("stage: Pytorch model formation done! ")

loss_fn=torch.nn.L1Loss()
#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.manual_seed(41)
criterion = nn.L1Loss()

#dataset = ConcatDataset([train_dataloader, test_dataloader])
num_epochs = 10
batch_size = 8
k = 10
# splits = KFold(n_splits = k, shuffle = True, random_state = 41)
# foldperf={}
def show_metrics(y_test, y_predict):
    from sklearn.metrics import mean_absolute_error as mae
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()
    pass


def train_epoch(model,start_index,end_index,device,dataloader,loss_fn,optimizer,epochs=1,patience_iterations=4,previous_loss_index=0,index_range=3):
    """
    model: rnn model u r going to use 
    start index: index to get piece of data/slicing start
    end index: slicinf=g index end
    device : device for pu
    dataloader: training dataloader
    lossfn: loss fn
    epochs: no of epochs for each sample
    patience_iterations: wait till no of iterations when train loss is more than val loss; part for rarly stopping, after patience iterations exceed training will stop
    previous_loss_index: index where previois  val loss is less than training loss
    index_range: range  of index where once tarining greater is less than val loss and again tarining loss is less than validation loss and now currently now training loss is greater than validation loss
    """
    train_loss=[]
    val0dation_loss=[]
    model.train().to(device)
    counter_loss=0
    count=0 ## no of times val loss is more than training loss
    previous_loss_index=previous_loss_index
    patience_iterations=patience_iterations
    index_range=index_range # difference between previous index came when val loss is than trainingb loss

    sample_dcount=0
    mylist=[]
    should_break=0
    for epoch in range(epochs):
        
        for i,(eeg_scan, age) in enumerate(dataloader,0): 
            
            sample_dcount+=1
            #print("current update is : ",sample_dcount)
            #model.train().to(device)
            eeg_scan, age0 = eeg_scan.to(device), age.to(device)
            # make 0 gradient initially for optimization
            optimizer.zero_grad()  # gradient : slope
            age=(age0-torch.mean(age0))/(torch.var(age0)).to(device)
            age=age0
            output = model(eeg_scan[:,start_index:end_index,:])
            loss = loss_fn(output,age)
            loss.backward()  #  calculating gradients ,update weights to input 
            optimizer.step()  # move in direction of gradient to get minima

            counter_loss+=loss.item()  
            #val_loss=valid_epoch(model,start_index,end_index)
          
            copy=counter_loss
            n=20
            ii=i
        
        val_loss=valid_epoch(model,start_index,end_index)
        print("epoch: ",epoch," traing loss after 1 batch is:  ",counter_loss/(ii+1),"Validation loss after 20*32 batch is:  ",val_loss)
                #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
        train_loss.append(counter_loss/(ii+1))
        val0dation_loss.append(val_loss)
        x=counter_loss
        counter_loss=0
                #sample_dcount=0
            
          #   if i%n==0 and i!=0:
          #       print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1),"Validation loss after 20*32 batch is:  ",val_loss)
          #       #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
          #       train_loss.append(counter_loss/(n+1))
          #       val0dation_loss.append(val_loss)
          #       x=counter_loss
          #       counter_loss=0
          #       sample_dcount=0
          # # try:
          #   np.mean(val0dation_loss[index_range:])>np.mean(train_loss[index_range:])
          #   should_break+=1
          # except:
          #   pass
        # if should_break>=patience_iterations:
        #   break

        # #continue
        if  val_loss>counter_loss/(ii+1):
          if len(mylist)>patience_iterations:
            break
          else:
            if len(mylist)>0:
              if (epoch-mylist[-1])>index_range:
                mylist=[epoch]
              else:
                mylist.append(epoch)
            else:
              mylist=[epoch]

    plt.plot(val0dation_loss)
    plt.plot(train_loss)
    plt.legend(["val_loss","train-loss"])
    pass

@torch.no_grad()
def valid_epoch(model,start_index,end_index):
    valid_loss= 0.0
    #model.eval()
    count=0
    for i, (x,y) in enumerate(validation_loader):
        eeg_scan, age = x.to(device), y.to(device)
        output = model(eeg_scan[:,start_index:end_index,:])
        loss = loss_fn(output,age)
        valid_loss += loss.item()
        count=i+1
    return valid_loss/count


@torch.no_grad()
def predict( model,start_index=0,end_index=-1):
    ypred=[]
    ytest=[]
    model.eval()  
    for x,y in test_loader:
        yhat = model(x)
        # retrieve numpy array
        yhat = yhat.detach().cpu().numpy()
        #check=[i[0] for i in yhat[0]]
        ypred.extend(yhat)
        #print(ypred.shape)
        ytest.extend(y.cpu().numpy())
   
    # out=[]
    # for i in ypred[0]:
    #     out.append(i.tolist()[0])
    # show_metrics(np.array(ytest),np.array(out))
    return ytest,ypred

import torch, gc
gc.collect()
torch.cuda.empty_cache()


# best accuracy for 2000 sample

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=200, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=200,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(16,8))
        self.conv2=nn.Conv2d(1,1,(32,8))
        self.conv3=nn.Conv2d(1,1,(16,4))
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(245*10, 4000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))

        h11 = Variable(torch.zeros( 1, x.shape[0],200)).to(device) #x.shape[0],
        c11 = Variable(torch.zeros(1, x.shape[0],200)).to(device) #x.shape[0],
        a11,(h11,c11)=self.lstm2(a1,(h11,c11))
        
        b=self.batchNorm(a11)
        #print(b.shape)
        c=torch.unsqueeze(b,1)
        #print(c.shape)
        out=self.conv1(c)
        out=self.conv2(out)
        out=torch.nn.MaxPool2d((8,8))(out)
        #print(out.shape)
        out=self.conv3(out)
        out=torch.nn.MaxPool2d((2,2))(out)
        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=200, hidden_size=100,num_layers=num_layers,batch_first=True).to(device)
        #self.lstm2 = nn.LSTM(input_size=20, hidden_size=200,num_layers=num_layers,batch_first=True).to(device)
        #self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        #self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        self.conv0=nn.Conv2d(1,1,(2,2))
        self.conv1=nn.Conv2d(1,1,(4,2))
        self.conv2=nn.Conv2d(1,1,(8,2))
        self.conv3=nn.Conv2d(1,1,(16,2))
        self.conv4=nn.Conv2d(1,1,(32,2))
        self.conv5=nn.Conv2d(1,1,(64,2))
        self.conv6=nn.Conv2d(1,1,(128,5))
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(2128, 1000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],100)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],100)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))

        b=self.batchNorm(a1)
        c=torch.unsqueeze(b,1)
        out=self.conv0(c)
        out=self.conv1(out)
        out=self.conv2(out)
        out=self.conv3(out)
        out=self.conv4(out)
        out=torch.nn.MaxPool2d((2,2))(out)
        out=self.conv5(out)
        out=self.conv6(out)

        out=torch.nn.MaxPool2d((2,2))(out)
        out=torch.nn.MaxPool2d((3,3))(out)
        #print(out.shape)
        #out=self.conv3(out)
        #out=torch.nn.MaxPool2d((2,2))(out)
        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        #out=self.l1(out)
        #out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4096,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4096, 200))  # sample, timeseries, len of one timeseries
print(s)
print("stage1.................")
model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=50,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])


#path="/home/adarsh/projects/rpp-doesburg/adarsh/pytorch/preprocess/model_history2"
#torch.save(model.state_dict(), path)



plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

"""## graphical checking for wavelet data"""

#plt.subplots(200,1,figsize=(25,2));
import time
wavelet_un_normalized_data="/content/gdrive/MyDrive/mitacs 2023/Wavelet_without_zscore_data_0_400_samples_0_9_.npy"
#d=np.load(wavelet_un_normalized_data,allow_pickle=True)
Wavelet_transformed_arrray=stats.zscore(d,axis=2)
data=Wavelet_transformed_arrray[17,:,:]
print(data.shape)
count=1
channel=0

for i in range(data.shape[0]):
  if i%10==0:
    channel+=1
  plt.figure(figsize=(25,5));
  if i%10==0:
    count=1
    plt.title("Channel No: {j},wavelet Function No: {i}".format(j=channel,i=count),fontsize=40);
  else:
    plt.title("Channel No: {j},wavelet Function No: {i}".format(j=channel,i=count),fontsize=40);
  count+=1
  plt.plot(data[i,:]);
  plt.show();
  time.sleep(1)
  plt.close()
  plt.clf()

"""## samples """

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size
        self.lstm1 = nn.LSTM(input_size=200, hidden_size=400,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=400, hidden_size=100,num_layers=num_layers,batch_first=True).to(device)
        

        self.n=length
        self.conv0=nn.Conv2d(1,1,(32,2))
        self.conv1=nn.Conv2d(1,1,(64,2))
        self.conv2=nn.Conv2d(1,1,(128,2))
        self.conv3=nn.Conv2d(1,1,(256,2))
        self.conv4=nn.Conv2d(1,1,(512,2))
        self.conv5=nn.Conv2d(1,1,(1024,2))
        # self.conv6=nn.Conv2d(1,1,(128,5))
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(1587, 2000)
        self.l1=nn.Linear(10000,2000)
        self.l2=nn.Linear(2000,500)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,100)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,1)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],400)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],400)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))

        h11 = Variable(torch.zeros( 1, x.shape[0],100)).to(device) #x.shape[0],
        c11 = Variable(torch.zeros(1, x.shape[0],100)).to(device) #x.shape[0],
        a11,(h111,c111)=self.lstm2(a1,(h11,c11))

        b=self.batchNorm(a11)
        c=torch.unsqueeze(b,1)
        out=self.conv0(c)
        out=self.conv1(out)
        out=self.conv2(out)
        out=self.conv3(out)
        out=torch.nn.MaxPool2d((2,2))(out)
        out=self.conv4(out)
        out=self.conv5(out)
        #print(out.shape)
        #out=torch.nn.MaxPool2d((2,2))(out)
        #out=self.conv5(out)
        #out=self.conv6(out)

        out=torch.nn.MaxPool2d((4,2))(out)
        #out=torch.nn.MaxPool2d((3,3))(out)
        #print(out.shape)
        #out=self.conv3(out)
        #out=torch.nn.MaxPool2d((2,2))(out)
        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        #out=self.l1(out)
        out=self.l2(out)
        #out=self.l3(out)
        out=self.l4(out)
        #out=self.l5(out)
        out=self.l6(out)
        #out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4096,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4096, 200))  # sample, timeseries, len of one timeseries
print(s)

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=200, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        

        self.n=length
        self.conv0=nn.Conv2d(1,1,(64,2))
        self.conv1=nn.Conv2d(1,1,(128,2))
        self.conv2=nn.Conv2d(1,1,(128,2))
        self.conv3=nn.Conv2d(1,1,(16,2))
        self.conv4=nn.Conv2d(1,1,(32,2))
        self.conv5=nn.Conv2d(1,1,(64,2))
        self.conv6=nn.Conv2d(1,1,(128,5))
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(65*480, 1000)
        self.l1=nn.Linear(1000,500)
        self.l2=nn.Linear(500,100)
        self.l3=nn.Linear(100,1)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))

        b=self.batchNorm(a1)
        c=torch.unsqueeze(b,1)
        out=self.conv0(c)
        out=self.conv1(out)
        #out=self.conv2(out)

        #out=self.conv3(out)
        #out=self.conv4(out)
        #out=torch.nn.MaxPool2d((2,2))(out)
        #out=self.conv5(out)
        #out=self.conv6(out)

        out=torch.nn.MaxPool2d((32,3))(out)
        #out=torch.nn.MaxPool2d((3,3))(out)
        #print(out.shape)
        #out=self.conv3(out)
        #out=torch.nn.MaxPool2d((2,2))(out)
        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        #out=self.l4(out)
        #out=self.l5(out)
        #out=self.l6(out)
        #out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model4=simpleLSTM(length=4096,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4096, 200))  # sample, timeseries, len of one timeseries
print(s)

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=200, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        

        self.n=length
        self.conv0=nn.Conv2d(1,1,(64,2))
        self.conv1=nn.Conv2d(1,1,(128,2))
        self.conv2=nn.Conv2d(1,1,(128,2))
        self.conv3=nn.Conv2d(1,1,(16,2))
        self.conv4=nn.Conv2d(1,1,(32,2))
        self.conv5=nn.Conv2d(1,1,(64,2))
        self.conv6=nn.Conv2d(1,1,(128,5))
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(65*480, 1000)
        self.l1=nn.Linear(1000,500)
        self.l2=nn.Linear(500,100)
        self.l3=nn.Linear(100,1)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))

        b=self.batchNorm(a1)
        c=torch.unsqueeze(b,1)
        out=self.conv0(c)
        out=self.conv1(out)
        #out=self.conv2(out)

        #out=self.conv3(out)
        #out=self.conv4(out)
        #out=torch.nn.MaxPool2d((2,2))(out)
        #out=self.conv5(out)
        #out=self.conv6(out)

        out=torch.nn.MaxPool2d((32,3))(out)
        #out=torch.nn.MaxPool2d((3,3))(out)
        #print(out.shape)
        #out=self.conv3(out)
        #out=torch.nn.MaxPool2d((2,2))(out)
        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        #out=self.l4(out)
        #out=self.l5(out)
        #out=self.l6(out)
        #out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model5=simpleLSTM(length=4096,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4096, 200))  # sample, timeseries, len of one timeseries
print(s)

"""## part2"""

model=model3
s=summary(model, (1,4096, 200))  # sample, timeseries, len of one timeseries
print(s)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)

train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=75,patience_iterations=50,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])


#path="/home/adarsh/projects/rpp-doesburg/adarsh/pytorch/preprocess/model_history2"
#torch.save(model.state_dict(), path)

import pywt
# Data Description
# Input(2000,20,4096)  # (toal_eeg_recordings,Toatl_channels, Toatal_datapoints)
# output:(2000,10,20,4096) (Samples, Wavelets_pseudo_frequency,Toatal_channels,Toatal_datapoints)

N_Samples=2000 
filter_count=10     # no of pseudo-frequencies used to filter eeg data # how many filters u need between (0,max_freq_value)
N_datapoints= 4096  #Total no of datapoints in 1 channels
sample_rate=128     # sampling rate of eeg data
channels=20

max_freq_value=55   # max pseudo-frequenciy # maximum frequency we can use to filter eeg data
freq_=np.linspace(1,max_freq_value,filter_count)

# code for logspace:-
# s1=np.logspace(1,55,20) # (base=10)**start,(base==10)**stop
# s2=np.linspace(1,55,20) # (base=1)**start,(base==10)**stop

Wavelet_processed_data = np.ndarray(shape=(N_Samples, filter_count, channels, N_datapoints), dtype = 'float32')
pseudo_frequency = sample_rate*pywt.scale2frequency('cgau8',freq_, precision=8)    #returns the pseudo-frequencies corresponding to the scales given by  wavelet "cgau8" specified by wname 

final=[]    #np.ndarray(shape=(N_Samples, filter_count*channels, N_datapoints), dtype = 'float32')
for sample in range(0,int(data.shape[0])):
  current = np.ndarray(shape=( filter_count*channels, N_datapoints), dtype = 'float32')
  for channel in range(data.shape[1]):
    present_data=data[sample,channel,:]
    coef, freqs=pywt.cwt(present_data,pseudo_frequency,'cgau8',1/sample_rate)
    current=np.concatenate((current,np.real(coef)))
  new=current[filter_count*channels:,]
  final.append(new)

d=np.array(final)

# due to memory issue i m notrying normalization as given below
# Wavelet_transformed_arrray=stats.zscore(d,axis=2)

a=4+3j
np.abs(a)

"""# Libraries"""

from google.colab import drive
drive.mount("/content/gdrive/")

import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
!pip install mne
import mne
# Ignore warnings
import warnings
from scipy import stats
!pip install torchinfo
import  torchinfo
from torchinfo import summary
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import torchsummary
from torch.autograd import Variable
import random
import tensorflow as tf
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
from keras.metrics import mean_absolute_error  as mae
from sklearn.metrics import explained_variance_score
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

"""# Week 11 report"""

#PLot for  ABSolute +Logspace Synthetic data of 0.8%Data 
X_0_8_path="/content/gdrive/MyDrive/mitacs 2023/abs_logspace_Wavelet_without_zscore_data_0_400_samples_0_8_.npy"
#X_0_8_path="/content/gdrive/MyDrive/mitacs 2023/abs_logspace_Wavelet_without_zscore_data_0_400_samples_0_8_.npy"
data=np.load(X_0_8_path,allow_pickle=True)
Wavelet_transformed_arrray=stats.zscore(data,axis=2)
del data

freq_=np.logspace(np.log10(1),np.log10(55),10)
import time
data=Wavelet_transformed_arrray[17,:,:]
print(data.shape)
count=1
channel=0
for i in range(data.shape[0]):
  if i%10==0:
    channel+=1
  plt.figure(figsize=(25,5));
  if i%10==0:
    count=1
    plt.title("Channel No: {j},wavelet Function scale value : {i}".format(j=channel,i=freq_[count-1]),fontsize=30);
  else:
    plt.title("Channel No: {j},wavelet Function scale value: {i}".format(j=channel,i=freq_[count-1]),fontsize=30);
  count+=1
  plt.plot(data[i,:]);
  plt.show();
  time.sleep(1)
  plt.close()
  plt.clf()

#PLot for  real +Logspace Synthetic data of 0.8%Data 
X_0_8_path="/content/gdrive/MyDrive/mitacs 2023/real_logspace_Wavelet_without_zscore_data_0_400_samples_0_8_.npy"
#X_0_8_path="/content/gdrive/MyDrive/mitacs 2023/abs_logspace_Wavelet_without_zscore_data_0_400_samples_0_8_.npy"
data=np.load(X_0_8_path,allow_pickle=True)
Wavelet_transformed_arrray=stats.zscore(data,axis=2)
del data

freq_=np.logspace(np.log10(1),np.log10(55),10)
import time
data=Wavelet_transformed_arrray[17,:,:]
print(data.shape)
count=1
channel=0
for i in range(data.shape[0]):
  if i%10==0:
    channel+=1
  plt.figure(figsize=(25,5));
  if i%10==0:
    count=1
    plt.title("Channel No: {j},wavelet Function scale value : {i}".format(j=channel,i=freq_[count-1]),fontsize=30);
  else:
    plt.title("Channel No: {j},wavelet Function scale value: {i}".format(j=channel,i=freq_[count-1]),fontsize=30);
  count+=1
  plt.plot(data[i,:]);
  plt.show();
  time.sleep(1)
  plt.close()
  plt.clf()

#PLot for  real  + linspace Synthetic data of 0.8%Data 
X_0_8_path="/content/gdrive/MyDrive/mitacs 2023/real_linspace_Wavelet_without_zscore_data_0_400_samples_0_8_.npy"
#X_0_8_path="/content/gdrive/MyDrive/mitacs 2023/abs_logspace_Wavelet_without_zscore_data_0_400_samples_0_8_.npy"
data=np.load(X_0_8_path,allow_pickle=True)
Wavelet_transformed_arrray=stats.zscore(data,axis=2)
del data

freq_=np.logspace(np.log10(1),np.log10(55),10)
import time
data=Wavelet_transformed_arrray[17,:,:]
print(data.shape)
count=1
channel=0
for i in range(data.shape[0]):
  if i%10==0:
    channel+=1
  plt.figure(figsize=(25,5));
  if i%10==0:
    count=1
    plt.title("Channel No: {j},wavelet Function scale value : {i}".format(j=channel,i=freq_[count-1]),fontsize=30);
  else:
    plt.title("Channel No: {j},wavelet Function scale value: {i}".format(j=channel,i=freq_[count-1]),fontsize=30);
  count+=1
  plt.plot(data[i,:]);
  plt.show();
  time.sleep(1)
  plt.close()
  plt.clf()

"""## Power Spectrum density"""

X_0_8_path="/content/gdrive/MyDrive/mitacs 2023/abs_logspace_Wavelet_without_zscore_data_0_400_samples_0_8_.npy"
#X_0_8_path="/content/gdrive/MyDrive/mitacs 2023/abs_logspace_Wavelet_without_zscore_data_0_400_samples_0_8_.npy"
data=np.load(X_0_8_path,allow_pickle=True)
Wavelet_transformed_arrray=stats.zscore(data,axis=2)
del data

from scipy import signal
# our data is :Wavelet_transformed_arrray
Wavelet_transformed_arrray.shape
unit=Wavelet_transformed_arrray[12,1,:]
print(unit.shape)

m, n = signal.welch(unit,128)


plt.semilogy(m, n) # plot on log scale
plt.figure()
plt.plot(m, n) # plot on normal scale

# plotting power spectrogram to finding at which frequency point power density is high
freq_=np.logspace(np.log10(1),np.log10(55),10)

testing_data=Wavelet_transformed_arrray[29,:,:]

for freq_i in range(len(freq_)):
  current_freq_scale_data=testing_data[freq_i,:]
  #print(current_freq_scale_data.shape)
  m, n = signal.welch(current_freq_scale_data,128,nperseg=4096,scaling="spectrum")

  mm=max(n)  #max power density 
  max_freq_index=np.where(n==mm) 
  freqm=max_freq_index[0].tolist()[0] # index of max power density

  plt.plot(m,n)  # x is frquency, vs y is power density corresponding given frequency
  plt.title("real_frequency_scale is:  {i}, calculated_max_pwer_freq is {j} ".format(i=freq_[freq_i],j=m[freqm]))
  plt.figure()

"""## MODEL Testing"""

from google.colab import drive
drive.mount("/content/gdrive/")

print("stage1................. 10:51am start")
import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
!pip install mne
import mne
# Ignore warnings
import warnings

!pip install torchinfo
from scipy import stats
import  torchinfo
from torchinfo import summary
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import torchsummary
from torch.autograd import Variable
import random
import tensorflow as tf
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
from keras.metrics import mean_absolute_error  as mae
from sklearn.metrics import explained_variance_score
print("stage: libraries importing done")


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
index=np.load("/content/gdrive/MyDrive/mitacs 2023/indexes_400.npy",allow_pickle=True)
Y_path =  '/content/gdrive/MyDrive/mitacs 2023/Synthetic_Age.npy'

wavelet_un_normalized_data="/content/gdrive/MyDrive/mitacs 2023/abs_logspace_Wavelet_without_zscore_data_0_400_samples_0_8_.npy"
d=np.load(wavelet_un_normalized_data,allow_pickle=True)
xdata=stats.zscore(d,axis=2)
del d
ydata=np.load(Y_path,allow_pickle=True)[index]


print("stage1.................")
#NPY_CustomEEGData : use it  when you load data as 
class NPY_CustomEEGData:
    def __init__(self,x,y,device=device):
      """
      inputs:
      x: npy araay of all edf outputs
      y: age of all data in  npy array in x 
      """
      self.x = torch.from_numpy(np.asarray(x ,dtype=np.float32))#.double()
      #self.y = torch.from_numpy(np.asarray(y,dtype=np.float32))#.double()
      self.y=y
      self.device=device
        
    def __len__(self):
      return len(self.x)

    def __getitem__(self, idx):
      """
      output: X and Y

      """
      # xdata=np.transpose(self.x[idx],(1,0))
      xdata=np.transpose(self.x[idx],(1,0))
      #print(xdata1.shape)
      #xdata=stats.zscore(xdata1,axis=0)
      #xdata=torch.moveaxis(self.x[idx],-1,-2)  # samples, wavelets,datapoints,chaneels
      yy=torch.from_numpy(np.asarray(self.y[idx],dtype=np.float32))#.double()
      #xdata=self.x[idx]
        
      #print(torch.any(torch.isnan(xdata)),"dataloader checking")
      return xdata.to(self.device),   yy.to(self.device) #self.y[idx].to(self.device)
dataset=NPY_CustomEEGData(np.stack(xdata,0), ydata)
xx1=dataset.__getitem__(10)



validation_split = .5
shuffle_dataset = True
dataset_size = len(dataset)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))
np.random.shuffle(indices)

train_indices, val_indices,test_indices = indices[split:], indices[:int(split/5)],indices[int(split/5):split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)
test_sampler= SubsetRandomSampler(test_indices)

batch_size=32

train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,sampler=valid_sampler)
test_loader= torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)
print("stage1.................")
del xdata
del ydata
del indices
del dataset_size
del split

for x,y in train_loader:
  print(x.shape)

  break
import torch
torch.cuda.empty_cache()
import gc
gc.collect()

print("stage: Pytorch model formation done! ")

loss_fn=torch.nn.L1Loss()
#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.manual_seed(41)
criterion = nn.L1Loss()

#dataset = ConcatDataset([train_dataloader, test_dataloader])
num_epochs = 10
batch_size = 8
k = 10
# splits = KFold(n_splits = k, shuffle = True, random_state = 41)
# foldperf={}
def show_metrics(y_test, y_predict):
    from sklearn.metrics import mean_absolute_error as mae
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()
    pass


def train_epoch(model,start_index,end_index,device,dataloader,loss_fn,optimizer,epochs=1,patience_iterations=4,previous_loss_index=0,index_range=3):
    """
    model: rnn model u r going to use 
    start index: index to get piece of data/slicing start
    end index: slicinf=g index end
    device : device for pu
    dataloader: training dataloader
    lossfn: loss fn
    epochs: no of epochs for each sample
    patience_iterations: wait till no of iterations when train loss is more than val loss; part for rarly stopping, after patience iterations exceed training will stop
    previous_loss_index: index where previois  val loss is less than training loss
    index_range: range  of index where once tarining greater is less than val loss and again tarining loss is less than validation loss and now currently now training loss is greater than validation loss
    """
    train_loss=[]
    val0dation_loss=[]
    model.train().to(device)
    counter_loss=0
    count=0 ## no of times val loss is more than training loss
    previous_loss_index=previous_loss_index
    patience_iterations=patience_iterations
    index_range=index_range # difference between previous index came when val loss is than trainingb loss

    sample_dcount=0
    mylist=[]
    should_break=0
    for epoch in range(epochs):
        
        for i,(eeg_scan, age) in enumerate(dataloader,0): 
            
            sample_dcount+=1
            #print("current update is : ",sample_dcount)
            #model.train().to(device)
            eeg_scan, age0 = eeg_scan.to(device), age.to(device)
            # make 0 gradient initially for optimization
            optimizer.zero_grad()  # gradient : slope
            age=(age0-torch.mean(age0))/(torch.var(age0)).to(device)
            age=age0
            output = model(eeg_scan[:,start_index:end_index,:])
            loss = loss_fn(output,age)
            loss.backward()  #  calculating gradients ,update weights to input 
            optimizer.step()  # move in direction of gradient to get minima

            counter_loss+=loss.item()  
            #val_loss=valid_epoch(model,start_index,end_index)
          
            copy=counter_loss
            n=20
            ii=i
        
        val_loss=valid_epoch(model,start_index,end_index)
        print("epoch: ",epoch," traing loss after 1 batch is:  ",counter_loss/(ii+1),"Validation loss after 20*32 batch is:  ",val_loss)
                #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
        train_loss.append(counter_loss/(ii+1))
        val0dation_loss.append(val_loss)
        x=counter_loss
        counter_loss=0
                #sample_dcount=0
            
          #   if i%n==0 and i!=0:
          #       print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1),"Validation loss after 20*32 batch is:  ",val_loss)
          #       #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
          #       train_loss.append(counter_loss/(n+1))
          #       val0dation_loss.append(val_loss)
          #       x=counter_loss
          #       counter_loss=0
          #       sample_dcount=0
          # # try:
          #   np.mean(val0dation_loss[index_range:])>np.mean(train_loss[index_range:])
          #   should_break+=1
          # except:
          #   pass
        # if should_break>=patience_iterations:
        #   break

        # #continue
        if  val_loss>counter_loss/(ii+1):
          if len(mylist)>patience_iterations:
            break
          else:
            if len(mylist)>0:
              if (epoch-mylist[-1])>index_range:
                mylist=[epoch]
              else:
                mylist.append(epoch)
            else:
              mylist=[epoch]

    plt.plot(val0dation_loss)
    plt.plot(train_loss)
    plt.legend(["val_loss","train-loss"])
    pass

@torch.no_grad()
def valid_epoch(model,start_index,end_index):
    valid_loss= 0.0
    #model.eval()
    count=0
    for i, (x,y) in enumerate(validation_loader):
        eeg_scan, age = x.to(device), y.to(device)
        output = model(eeg_scan[:,start_index:end_index,:])
        loss = loss_fn(output,age)
        valid_loss += loss.item()
        count=i+1
    return valid_loss/count


@torch.no_grad()
def predict( model,start_index=0,end_index=-1):
    ypred=[]
    ytest=[]
    model.eval()  
    for x,y in test_loader:
        yhat = model(x)
        # retrieve numpy array
        yhat = yhat.detach().cpu().numpy()
        #check=[i[0] for i in yhat[0]]
        ypred.extend(yhat)
        #print(ypred.shape)
        ytest.extend(y.cpu().numpy())
   
    # out=[]
    # for i in ypred[0]:
    #     out.append(i.tolist()[0])
    # show_metrics(np.array(ytest),np.array(out))
    return ytest,ypred

import torch, gc
gc.collect()
torch.cuda.empty_cache()

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=200, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=200,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(16,2))
        self.conv2=nn.Conv2d(1,1,(32,8))
        self.conv3=nn.Conv2d(1,1,(16,4))
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(245*10, 4000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))

        h11 = Variable(torch.zeros( 1, x.shape[0],200)).to(device) #x.shape[0],
        c11 = Variable(torch.zeros(1, x.shape[0],200)).to(device) #x.shape[0],
        a11,(h11,c11)=self.lstm2(a1,(h11,c11))
        
        b=self.batchNorm(a11)
        #print(b.shape)
        c=torch.unsqueeze(b,1)
        #print(c.shape)
        out=self.conv1(c)
        out=self.conv2(out)
        out=torch.nn.MaxPool2d((8,8))(out)
        #print(out.shape)
        out=self.conv3(out)
        out=torch.nn.MaxPool2d((2,2))(out)
        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4096,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4096, 200))  # sample, timeseries, len of one timeseries
print(s)
print("stage1.................")
model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=50,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=200, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=200,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(16,2))
        self.conv2=nn.Conv2d(1,1,(32,8))
        self.conv3=nn.Conv2d(1,1,(16,4))
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(245*10, 4000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))

        h11 = Variable(torch.zeros( 1, x.shape[0],200)).to(device) #x.shape[0],
        c11 = Variable(torch.zeros(1, x.shape[0],200)).to(device) #x.shape[0],
        a11,(h11,c11)=self.lstm2(a1,(h11,c11))
        
        b=self.batchNorm(a11)
        #print(b.shape)
        c=torch.unsqueeze(x,1)
        #print(c.shape)
        out=self.conv1(c)
        out=self.conv2(out)
        out=torch.nn.MaxPool2d((8,8))(out)
        #print(out.shape)
        out=self.conv3(out)
        out=torch.nn.MaxPool2d((2,2))(out)
        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4096,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4096, 200))  # sample, timeseries, len of one timeseries
print(s)
print("stage1.................")
model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=50,previous_loss_index=0,index_range=1)

plt.figure()
ytest,ypred=predict(model)

out=[]
for i in ypred:
  out.append(i)

show_metrics(np.array(ytest), np.array(out))

plt.figure()
plt.title("real age vs predicted age")
plt.plot(out,"r*")
plt.plot(ytest,"go")
plt.legend(["pred","real"])

"""# Fianl Task"""

from google.colab import drive
drive.mount("/content/gdrive/")

import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
!pip install mne
import mne
# Ignore warnings
import warnings
from scipy import stats
!pip install torchinfo
import  torchinfo
from torchinfo import summary
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import torchsummary
from torch.autograd import Variable
import random
import tensorflow as tf
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
from keras.metrics import mean_absolute_error  as mae
from sklearn.metrics import explained_variance_score
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

!pip install --quiet pytorch-lightning
!pip install --quiet tqdm

import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
from matplotlib.ticker import MaxNLocator


import pandas as pd
import numpy as np
from tqdm.notebook import tqdm

import torch
import torch.autograd as autograd
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

import pytorch_lightning as pl
#from pytorch_lightning.metrics.functional import accuracy, f1, auroc
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import TensorBoardLogger

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix

from multiprocessing import cpu_count

y_0_6_path="/content/gdrive/MyDrive/mitacs/data/Synthetic_Age_32Sec_128hz_5000sample_10channel_06_noise.npy"
ydata=np.load(y_0_6_path,allow_pickle=True)
x="/content/gdrive/MyDrive/mitacs/data/Synthetic_Data_noise_added_alpha_0.6_32Sec_128hz_5000sample_10channel.npy"
xdata=np.load(x,allow_pickle=True)

xdata.shape

plt.plot(xdata[23,3,:])
np.mean(xdata[23,3,:])

plt.plot(ydata)

"""## Pytorch code"""

test_split = .2
shuffle_dataset = True
dataset_size = len(ydata)
indices = list(range(dataset_size))
split = int(np.floor(test_split * dataset_size))
np.random.shuffle(indices)
train_indices,test_indices = indices[split:],indices[:split]


train_sequences={}
test_sequences={}

count=0
for i in train_indices:
  train_sequences[count]=(xdata[i,:,:],ydata[i])
  count+=1

count=0
for i in test_indices:
  test_sequences[count]=(xdata[i,:,:],ydata[i])
  count+=1

class SurfaceDataset(Dataset):

  def __init__(self,sequences):
    self.sequences = sequences

  def __len__(self):
    return len(self.sequences)

  def __getitem__(self,idx):
    sequence, label = self.sequences[idx]
    local=np.transpose(sequence,(1,0))
    #print(local.shape)
    return dict(
        sequence = torch.from_numpy(np.asarray(local ,dtype=np.float32)),
        labels = torch.from_numpy(np.asarray(label ,dtype=np.float32))
    )
check=SurfaceDataset(train_sequences)
check.__getitem__(10)

# class SurfaceDataset(Dataset):

#   def __init__(self,x,y):
#     self.x = x
#     self.y=y

#   def __len__(self):
#     return len(self.y)

#   def __getitem__(self,idx):
#     xsequence= self.x[idx]
#     ylabel=self.y[idx]
#     print(ylabel)
#     return dict(
#         sequence=torch.from_numpy(np.asarray(xsequence ,dtype=np.float32)),
#         label = torch.from_numpy(np.asarray(ylabel ,dtype=np.float32))
#     )
# out=SurfaceDataset(xdata,ydata)
# out.__getitem__(10)

class SurfaceDataModule(pl.LightningDataModule):

  def __init__(self, train_sequences, test_sequences, batch_size=8):
    super().__init__()
    self.batch_size = batch_size
    self.train_sequences = train_sequences
    self.test_sequences = test_sequences

  def setup(self, stage=None):
    self.train_dataset = SurfaceDataset(self.train_sequences)
    self.test_dataset = SurfaceDataset(self.test_sequences)

  def train_dataloader(self):
    return DataLoader(
      self.train_dataset,
      batch_size=self.batch_size,
      shuffle=True,
      num_workers=cpu_count()
    )

  def val_dataloader(self):
    return DataLoader(
      self.test_dataset,
      batch_size=self.batch_size,
      shuffle=False,
      num_workers=cpu_count()
    )

  def test_dataloader(self):
    return DataLoader(
      self.test_dataset,
      batch_size=self.batch_size,
      shuffle=False,
      num_workers=cpu_count()
    )

    
N_EPOCHS = 50
BATCH_SIZE = 8

data_module = SurfaceDataModule(
  train_sequences,
  test_sequences,
  batch_size=BATCH_SIZE
)

class SequenceModel(nn.Module):
  def __init__(self,n_features, n_hidden=256, n_layers=3):
    super().__init__()
    
    self.lstm = nn.LSTM(
        input_size=n_features,
        hidden_size=n_hidden,
        num_layers=n_layers,
        batch_first=True,
        dropout=0.75
    )
    self.flat=torch.nn.Flatten()
    self.l0=nn.Linear(n_hidden,1)
    

  def forward(self,x):
    #print(x.shape,"x shape")
    self.lstm.flatten_parameters()
    a,(hidden,cell) = self.lstm(x)
    out = hidden[-1]

    #out = self.flat(out)
    #print(out.shape)

    out=self.l0(out)
    out=torch.squeeze(out,1)
   
    return out

n_features=10
n_hidden=256
n_layers=3
model3=SequenceModel(n_features=n_features,n_hidden=n_hidden,n_layers=n_layers).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,1280, 10))  # sample, timeseries, len of one timeseries
print(s)

class SurfacePredictor(pl.LightningModule):

  def __init__(self,n_features:int, n_hidden: int,n_layers:int):
    super().__init__()
    self.model = SequenceModel(n_features, n_hidden,n_layers)
    self.criterion = torch.nn.L1Loss()

  def forward(self, x, labels=None):
    
    output = self.model(x)
    loss = 0
    if labels is not None:
        loss = self.criterion(output, labels)
    return loss, output

  def training_step(self, batch, batch_idx):
    x = batch["sequence"]
    labels = batch["labels"]
    loss, outputs = self(x, labels)
    #predictions = torch.argmax(outputs,dim=1)
    predictions=outputs
    #step_accuracy = accuracy(predictions, labels)

    self.log("train_loss", loss, prog_bar=True, logger=True)
    #self.log("train_accuracy", step_accuracy, prog_bar=True, logger=True)
    return {"loss": loss } # "accuracy": step_accuracy}

  def validation_step(self, batch, batch_idx):
    x = batch["sequence"]
    labels = batch["labels"]
    loss, outputs = self(x, labels)
    #predictions = torch.argmax(outputs,dim=1)
    predictions=outputs
    #step_accuracy = accuracy(predictions, labels)

    self.log("val_loss", loss, prog_bar=True, logger=True)
    #self.log("val_accuracy", step_accuracy, prog_bar=True, logger=True)
    return {"loss": loss}# "accuracy": step_accuracy}

  def test_step(self, batch, batch_idx):
    
    x = batch["sequence"]
    labels = batch["labels"]
    loss, outputs = self(x, labels)
    #predictions = torch.argmax(outputs,dim=1)
    predictions=outputs
    #step_accuracy = accuracy(predictions, labels)

    self.log("test_loss", loss, prog_bar=True, logger=True)
    #self.log("test_accuracy", step_accuracy, prog_bar=True, logger=True)
    return {"loss": loss}# "accuracy": step_accuracy}

  
  def configure_optimizers(self):
    return optim.Adam(self.parameters(), lr=0.0001)

def show_metrics(y_test, y_predict):
    from sklearn.metrics import mean_absolute_error as mae
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()
    pass

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir ./lightning_logs

"""## Model 1: 3 lstm layers , 10 sec"""

n_layers=3

model3=SequenceModel(n_features=n_features,n_hidden=n_hidden,n_layers=n_layers).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,1280, 10))  # sample, timeseries, len of one timeseries
print(s)

model = SurfacePredictor(n_features=n_features,n_hidden=n_hidden,n_layers=n_layers)

checkpoint_callback = ModelCheckpoint(
    dirpath="checkpoints",
    filename="best-checkpoint",
    save_top_k=1,
    verbose=True,
    monitor="val_loss",
    mode="min"
)

early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)
logger=True
trainer = pl.Trainer(
  logger=logger,
  #checkpoint_callback=checkpoint_callback,
  callbacks=[early_stopping_callback,checkpoint_callback],
  max_epochs=N_EPOCHS,
  #gpus=1,
  accelerator ="gpu"
  #progress_bar_refresh_rate=30
)

# note if you are running on "gpu" kindly change accelator option in to gpu in "traine"  description
trainer.fit(model, data_module)

def transform(x):
  return x.detach().tolist()
test_dataset = SurfaceDataset(test_sequences)

predict_age = []
actual_age = []

try:
  for item in tqdm(test_dataset):
    sequence = item["sequence"]
    label = item["labels"]
    _, output = trained_model(sequence.unsqueeze(dim=0))
    actual_age.append(transform(label))
    predict_age.append(transform(output)[0])
    #print(sequence.unsqueeze(dim=0).shape)
except:
  pass

show_metrics(np.array(actual_age), np.array(predict_age))

"""## Model 2: 5 lstm layers , 10 sec"""

n_layers=5

model3=SequenceModel(n_features=n_features,n_hidden=n_hidden,n_layers=n_layers).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,1280, 10))  # sample, timeseries, len of one timeseries
print(s)

model = SurfacePredictor(n_features=n_features,n_hidden=n_hidden,n_layers=n_layers)

checkpoint_callback = ModelCheckpoint(
    dirpath="checkpoints",
    filename="best-checkpoint",
    save_top_k=1,
    verbose=True,
    monitor="val_loss",
    mode="min"
)

early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)
logger=True
trainer = pl.Trainer(
  logger=logger,
  #checkpoint_callback=checkpoint_callback,
  callbacks=[early_stopping_callback,checkpoint_callback],
  max_epochs=N_EPOCHS,
  #gpus=1,
  accelerator ="gpu"
  #progress_bar_refresh_rate=30
)

# note if you are running on "gpu" kindly change accelator option in to gpu in "traine"  description
trainer.fit(model, data_module)

def transform(x):
  return x.detach().tolist()
test_dataset = SurfaceDataset(test_sequences)

predict_age = []
actual_age = []

try:
  for item in tqdm(test_dataset):
    sequence = item["sequence"]
    label = item["labels"]
    _, output = trained_model(sequence.unsqueeze(dim=0))
    actual_age.append(transform(label))
    predict_age.append(transform(output)[0])
    #print(sequence.unsqueeze(dim=0).shape)
except:
  pass

show_metrics(np.array(actual_age), np.array(predict_age))

"""## Model 3: 7 Lstm Layer, 10 sec"""

n_layers=7

model3=SequenceModel(n_features=n_features,n_hidden=n_hidden,n_layers=n_layers).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,1280, 10))  # sample, timeseries, len of one timeseries
print(s)

model = SurfacePredictor(n_features=n_features,n_hidden=n_hidden,n_layers=n_layers)

checkpoint_callback = ModelCheckpoint(
    dirpath="checkpoints",
    filename="best-checkpoint",
    save_top_k=1,
    verbose=True,
    monitor="val_loss",
    mode="min"
)

early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)
logger=True
trainer = pl.Trainer(
  logger=logger,
  #checkpoint_callback=checkpoint_callback,
  callbacks=[early_stopping_callback,checkpoint_callback],
  max_epochs=N_EPOCHS,
  #gpus=1,
  accelerator ="gpu"
  #progress_bar_refresh_rate=30
)

# note if you are running on "gpu" kindly change accelator option in to gpu in "traine"  description
trainer.fit(model, data_module)

def transform(x):
  return x.detach().tolist()
test_dataset = SurfaceDataset(test_sequences)

predict_age = []
actual_age = []

try:
  for item in tqdm(test_dataset):
    sequence = item["sequence"]
    label = item["labels"]
    _, output = trained_model(sequence.unsqueeze(dim=0))
    actual_age.append(transform(label))
    predict_age.append(transform(output)[0])
    #print(sequence.unsqueeze(dim=0).shape)
except:
  pass

show_metrics(np.array(actual_age), np.array(predict_age))

"""## model 1: 3 lstm layer, 32 sec"""

n_layers=3

model3=SequenceModel(n_features=n_features,n_hidden=n_hidden,n_layers=n_layers).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,1280, 10))  # sample, timeseries, len of one timeseries
print(s)

model = SurfacePredictor(n_features=n_features,n_hidden=n_hidden,n_layers=n_layers)

checkpoint_callback = ModelCheckpoint(
    dirpath="checkpoints",
    filename="best-checkpoint",
    save_top_k=1,
    verbose=True,
    monitor="val_loss",
    mode="min"
)

early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)
logger=True
trainer = pl.Trainer(
  logger=logger,
  #checkpoint_callback=checkpoint_callback,
  callbacks=[early_stopping_callback,checkpoint_callback],
  max_epochs=N_EPOCHS,
  #gpus=1,
  accelerator ="gpu"
  #progress_bar_refresh_rate=30
)

# note if you are running on "gpu" kindly change accelator option in to gpu in "traine"  description
trainer.fit(model, data_module)

def transform(x):
  return x.detach().tolist()
test_dataset = SurfaceDataset(test_sequences)

predict_age = []
actual_age = []

try:
  for item in tqdm(test_dataset):
    sequence = item["sequence"]
    label = item["labels"]
    _, output = trained_model(sequence.unsqueeze(dim=0))
    actual_age.append(transform(label))
    predict_age.append(transform(output)[0])
    #print(sequence.unsqueeze(dim=0).shape)
except:
  pass

show_metrics(np.array(actual_age), np.array(predict_age))

"""## Model 2: 5 lstm model, 32 sec"""

n_layers=5

model3=SequenceModel(n_features=n_features,n_hidden=n_hidden,n_layers=n_layers).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,1280, 10))  # sample, timeseries, len of one timeseries
print(s)

model = SurfacePredictor(n_features=n_features,n_hidden=n_hidden,n_layers=n_layers)

checkpoint_callback = ModelCheckpoint(
    dirpath="checkpoints",
    filename="best-checkpoint",
    save_top_k=1,
    verbose=True,
    monitor="val_loss",
    mode="min"
)

early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)
logger=True
trainer = pl.Trainer(
  logger=logger,
  #checkpoint_callback=checkpoint_callback,
  callbacks=[early_stopping_callback,checkpoint_callback],
  max_epochs=N_EPOCHS,
  #gpus=1,
  accelerator ="gpu"
  #progress_bar_refresh_rate=30
)

# note if you are running on "gpu" kindly change accelator option in to gpu in "traine"  description
trainer.fit(model, data_module)

def transform(x):
  return x.detach().tolist()
test_dataset = SurfaceDataset(test_sequences)

predict_age = []
actual_age = []

try:
  for item in tqdm(test_dataset):
    sequence = item["sequence"]
    label = item["labels"]
    _, output = trained_model(sequence.unsqueeze(dim=0))
    actual_age.append(transform(label))
    predict_age.append(transform(output)[0])
    #print(sequence.unsqueeze(dim=0).shape)
except:
  pass

show_metrics(np.array(actual_age), np.array(predict_age))

"""## model 3 7 LSTM 32 sec"""

n_layers=7

model3=SequenceModel(n_features=n_features,n_hidden=n_hidden,n_layers=n_layers).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,1280, 10))  # sample, timeseries, len of one timeseries
print(s)

model = SurfacePredictor(n_features=n_features,n_hidden=n_hidden,n_layers=n_layers)

checkpoint_callback = ModelCheckpoint(
    dirpath="checkpoints",
    filename="best-checkpoint",
    save_top_k=1,
    verbose=True,
    monitor="val_loss",
    mode="min"
)

early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)
logger=True
trainer = pl.Trainer(
  logger=logger,
  #checkpoint_callback=checkpoint_callback,
  callbacks=[early_stopping_callback,checkpoint_callback],
  max_epochs=N_EPOCHS,
  #gpus=1,
  accelerator ="gpu"
  #progress_bar_refresh_rate=30
)

# note if you are running on "gpu" kindly change accelator option in to gpu in "traine"  description
trainer.fit(model, data_module)

def transform(x):
  return x.detach().tolist()
test_dataset = SurfaceDataset(test_sequences)

predict_age = []
actual_age = []

try:
  for item in tqdm(test_dataset):
    sequence = item["sequence"]
    label = item["labels"]
    _, output = trained_model(sequence.unsqueeze(dim=0))
    actual_age.append(transform(label))
    predict_age.append(transform(output)[0])
    #print(sequence.unsqueeze(dim=0).shape)
except:
  pass

show_metrics(np.array(actual_age), np.array(predict_age))