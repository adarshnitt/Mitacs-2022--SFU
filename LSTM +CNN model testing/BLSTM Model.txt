#print(len(xtrain))
from keras.layers import Bidirectional
model2 = Sequential()
model2.add(Dense(750,input_shape=(20,350)))
model2.add(Flatten())
#model2.add(Bidirectional(LSTM(700,return_sequences=False), input_shape=(350,20))) # 10 is ouput of lstm block and input is 20 timesteps with 90000 inpiut shape  
#model2.add(Dropout(0.2))
#model2.add(tf.keras.layers.BatchNormalization())
#model2.add(LSTM(700, return_sequences=False))
#model2.add(tf.keras.layers.BatchNormalization())
#model2.add(LSTM(425, return_sequences=False))
#model2.add(Dropout(0.2))
#model2.add(tf.keras.layers.BatchNormalization())
#model.add(LSTM(300))
#model.add(Dropout(0.2))
model2.add(Dense(750, activation='relu'))
#model2.add(Dense(35, activation='relu'))
#model2.add(Dropout(0.2))
#model2.add(Dense(350, activation='relu'))
#model2.add(Dense(350, activation='relu'))
model2.add(Dense(1, activation='relu'))
model2.compile(loss="MeanAbsoluteError", optimizer='adam')

model=model2

tf.keras.optimizers.Adam(learning_rate=0.001 )#, beta_1=0.09, beta_2=0.009, epsilon=0.01, decay=0.001, amsgrad=False)
model.compile(loss="MeanAbsoluteError", optimizer='adam')
model.summary()

from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='loss', mode='auto', verbose=1,min_delta=0.001, patience=5)
hist=model.fit(xtrain,ytrain, steps_per_epoch=5,validation_split=0.3 ,epochs=50,batch_size=64,use_multiprocessing=True,callbacks=[es]  )

pred=model.predict(xtest)
ypred1=tf.reshape(pred,pred.shape[0])

show_metrics(ytest, ypred1)
model.summary()