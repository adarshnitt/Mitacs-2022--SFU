# state of art

from keras.layers import Bidirectional
model = Sequential()
model.add(Bidirectional(LSTM(50,return_sequences=True), input_shape=(350,20))) # 10 is ouput of lstm block and input is 20 timesteps with 90000 inpiut shape  
#model.add(tf.keras.layers.BatchNormalization())
#model.add(LSTM(100, return_sequences=True))

model.add(tf.keras.layers.Reshape((350,100,1)))
#model.add(Conv2D(64,2))
model.add(tf.keras.layers.MaxPool2D(2))
model.add(Conv1D(200,2))
model.add(tf.keras.layers.MaxPool2D(2))
model.add(Conv1D(100,2))
model.add(tf.keras.layers.AveragePooling2D(2))
model.add(Conv1D(50,2))

#model.add(tf.keras.layers.AveragePooling2D(3))

model.add(Flatten())
model.add(Dense(2000, activation='relu'))
# model.add(Dense(100, activation='relu'))
model.add(Dense(2000, activation='relu'))
model.add(Dense(1000, activation='relu'))
model.add(Dense(50, activation='relu'))
model.add(Dense(1, activation='relu'))

##############################################################################
model.compile(loss="MeanAbsoluteError", optimizer='adam')

tf.keras.optimizers.Adam(learning_rate=0.0001 )#, beta_1=0.09, beta_2=0.009, epsilon=0.01, decay=0.001, amsgrad=False)
model.compile(loss="MeanAbsoluteError", optimizer='adam')

from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='loss', mode='auto', verbose=0,min_delta=0.0001, patience=5)
hist=model.fit(xtrain,ytrain, steps_per_epoch=5,validation_split=0.2 ,epochs=50,batch_size=32,use_multiprocessing=True,callbacks=[es]  )

pred=model.predict(xtest)
ypred1=tf.reshape(pred,pred.shape[0])

show_metrics(ytest, ypred1)
model.summary()
plt.plot(hist.history["val_loss"])
plt.plot(hist.history["loss"])
plt.legend(["va loss"," training loss"])
