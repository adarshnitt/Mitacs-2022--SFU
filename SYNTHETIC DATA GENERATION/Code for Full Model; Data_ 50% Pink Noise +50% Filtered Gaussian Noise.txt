from google.colab import drive
drive.mount("/content/gdrive/")

import os
import torch
import pandas as pd
#from skimage import io, transform
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
!pip install mne
import mne
# Ignore warnings
import warnings

!pip install torchinfo
import  torchinfo
from torchinfo import summary
warnings.filterwarnings("ignore")
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
import torchsummary
from torch.autograd import Variable
import random
import tensorflow as tf
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset
from keras.metrics import mean_absolute_error  as mae
from sklearn.metrics import explained_variance_score

from scipy import stats

print("stage: libraries importing done")


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
X_pure="/content/gdrive/MyDrive/mitacs 2023/2000_without_alpha.npy"
Y_pure="/content/gdrive/MyDrive/mitacs 2023/Synthetic_2000_Age.npy"

X_0_9_path =  '/content/gdrive/MyDrive/data/synthetic/Synthetic_Data_noise_added_alpha_0.9_2000samples.npy'
XX_0_99_path=  "/content/gdrive/MyDrive/data/synthetic/Synthetic_Data_noise_added_alpha_0.99_2000samples.npy"
Y_path =  '/content/gdrive/MyDrive/data/synthetic/Synthetic_Age.npy'

x1=np.load(X_pure,allow_pickle=True)
xdata=stats.zscore(x1,axis=2)

ydata=np.load(Y_pure,allow_pickle=True)


#NPY_CustomEEGData : use it  when you load data as 
class NPY_CustomEEGData:
    def __init__(self,x,y,device=device):
      """
      inputs:
      x: npy araay of all edf outputs
      y: age of all data in  npy array in x 
      """
      self.x = torch.from_numpy(np.asarray(x ,dtype=np.float32))#.double()
      self.y = torch.from_numpy(np.asarray(y,dtype=np.float32))#.double()
      self.device=device
        
    def __len__(self):
      return len(self.x)

    def __getitem__(self, idx):
      """
      output: X and Y

      """

      xdata=np.transpose(self.x[idx],(1,0))
      #xdata=self.x[idx]
      return xdata.to(self.device),self.y[idx].to(self.device)
dataset=NPY_CustomEEGData(np.stack(xdata,0), ydata)
xx1=dataset.__getitem__(10)


validation_split = .5
shuffle_dataset = True
dataset_size = len(dataset)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))
np.random.shuffle(indices)

train_indices, val_indices,test_indices = indices[split:], indices[:int(split/5)],indices[int(split/5):split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)
test_sampler= SubsetRandomSampler(test_indices)

batch_size=32

train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,sampler=valid_sampler)
test_loader= torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)
del xdata
del ydata
del indices
del dataset_size
del split

for x,y in train_loader:
  print(x.shape)
  break
import torch
torch.cuda.empty_cache()
import gc
gc.collect()


print("stage: Pytorch model formation done! ")

loss_fn=torch.nn.L1Loss()
#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.manual_seed(41)
criterion = nn.L1Loss()

#dataset = ConcatDataset([train_dataloader, test_dataloader])
num_epochs = 10
batch_size = 32
k = 10
# splits = KFold(n_splits = k, shuffle = True, random_state = 41)
# foldperf={}
def show_metrics(y_test, y_predict):
    from sklearn.metrics import mean_absolute_error as mae
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()
    pass


def train_epoch(model,start_index,end_index,device,dataloader,loss_fn,optimizer,epochs=1,patience_iterations=4,previous_loss_index=0,index_range=3):
    """
    model: rnn model u r going to use 
    start index: index to get piece of data/slicing start
    end index: slicinf=g index end
    device : device for pu
    dataloader: training dataloader
    lossfn: loss fn
    epochs: no of epochs for each sample
    patience_iterations: wait till no of iterations when train loss is more than val loss; part for rarly stopping, after patience iterations exceed training will stop
    previous_loss_index: index where previois  val loss is less than training loss
    index_range: range  of index where once tarining greater is less than val loss and again tarining loss is less than validation loss and now currently now training loss is greater than validation loss
    """
    train_loss=[]
    val0dation_loss=[]
    model.train().to(device)
    counter_loss=0
    count=0 ## no of times val loss is more than training loss
    previous_loss_index=previous_loss_index
    patience_iterations=patience_iterations
    index_range=index_range # difference between previous index came when val loss is than trainingb loss

    sample_dcount=0
    mylist=[]
    should_break=0
    for epoch in range(epochs):
        
        for i,(eeg_scan, age) in enumerate(dataloader,0): 
            
            sample_dcount+=1
            #model.train().to(device)
            eeg_scan, age0 = eeg_scan.to(device), age.to(device)
            # make 0 gradient initially for optimization
            optimizer.zero_grad()  # gradient : slope
            age=(age0-torch.mean(age0))/(torch.var(age0)).to(device)
            age=age0
            output = model(eeg_scan[:,start_index:end_index,:])
            loss = loss_fn(output,age)
            loss.backward()  #  calculating gradients ,update weights to input 
            optimizer.step()  # move in direction of gradient to get minima

            counter_loss+=loss.item()  
            #val_loss=valid_epoch(model,start_index,end_index)
          
            copy=counter_loss
            n=20
            ii=i
        
        val_loss=valid_epoch(model,start_index,end_index)
        print("epoch: ",epoch," traing loss after 1 batch is:  ",counter_loss/(ii+1),"Validation loss after 20*32 batch is:  ",val_loss)
                #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
        train_loss.append(counter_loss/(ii+1))
        val0dation_loss.append(val_loss)
        x=counter_loss
        counter_loss=0
                #sample_dcount=0
            
          #   if i%n==0 and i!=0:
          #       print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1),"Validation loss after 20*32 batch is:  ",val_loss)
          #       #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
          #       train_loss.append(counter_loss/(n+1))
          #       val0dation_loss.append(val_loss)
          #       x=counter_loss
          #       counter_loss=0
          #       sample_dcount=0
          # # try:
          #   np.mean(val0dation_loss[index_range:])>np.mean(train_loss[index_range:])
          #   should_break+=1
          # except:
          #   pass
        # if should_break>=patience_iterations:
        #   break

        # #continue
        if  val_loss>counter_loss/(ii+1):
          if len(mylist)>patience_iterations:
            break
          else:
            if len(mylist)>0:
              if (epoch-mylist[-1])>index_range:
                mylist=[epoch]
              else:
                mylist.append(epoch)
            else:
              mylist=[epoch]

    plt.plot(val0dation_loss)
    plt.plot(train_loss)
    plt.legend(["val_loss","train-loss"])
    pass

@torch.no_grad()
def valid_epoch(model,start_index,end_index):
    valid_loss= 0.0
    #model.eval()
    count=0
    for i, (x,y) in enumerate(validation_loader):
        eeg_scan, age = x.to(device), y.to(device)
        output = model(eeg_scan[:,start_index:end_index,:])
        loss = loss_fn(output,age)
        valid_loss += loss.item()
        count=i+1
    return valid_loss/count


@torch.no_grad()
def predict( model,start_index=0,end_index=-1):
    ypred=[]
    ytest=[]
    model.eval()  
    for x,y in test_loader:
        yhat = model(x)
        # retrieve numpy array
        yhat = yhat.detach().cpu().numpy()
        #check=[i[0] for i in yhat[0]]
        ypred.extend(yhat)
        #print(ypred.shape)
        ytest.extend(y.cpu().numpy())
   
    # out=[]
    # for i in ypred[0]:
    #     out.append(i.tolist()[0])
    # show_metrics(np.array(ytest),np.array(out))
    return ytest,ypred

import torch, gc
gc.collect()
torch.cuda.empty_cache()

# best accuracy for 2000 sample

class simpleLSTM(nn.Module):
     # only lstm network
    def __init__(self, length,input_size, hidden_size, num_layers=1):
        super(simpleLSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm2 = nn.LSTM(input_size=20, hidden_size=2,num_layers=num_layers,batch_first=True).to(device)
        self.lstm3 = nn.LSTM(input_size=50, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)
        self.lstm4 = nn.LSTM(input_size=20, hidden_size=20,num_layers=num_layers,batch_first=True).to(device)

        self.n=length
        
        self.conv1=nn.Conv2d(1,1,(16,2))
        self.conv2=nn.Conv2d(1,1,(32,2))
        self.conv3=nn.Conv2d(1,1,(256,1))
        self.conv4=nn.Conv1d(700,400,3)
        self.conv5=nn.Conv1d(400,100,3)
        
        
        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(self.n).to(device)
        
        self.l0=nn.Linear(506*18, 4000)
        self.l1=nn.Linear(4000,2000)
        self.l2=nn.Linear(2000,1000)
        self.l3=nn.Linear(1000,500)
        self.l4=nn.Linear(500,250)
        self.l5=nn.Linear(250,100)
        self.l6=nn.Linear(100,10)
        self.l7=nn.Linear(10,1)

       
    def forward(self, x):
        #print(x.shape)
        #x=torch.unsqueeze(x[:,100:200,:],0)
        x=x[:,0:self.n,:]
        

        h1 = Variable(torch.zeros( 1, x.shape[0],20)).to(device) #x.shape[0],
        c1 = Variable(torch.zeros(1, x.shape[0],20)).to(device) #x.shape[0],
        a1,(h1,c1)=self.lstm1(x,(h1,c1))
        
        b=self.batchNorm(a1)
        #print(b.shape)
        c=torch.unsqueeze(b,1)
        #print(c.shape)
        out=self.conv1(c)
        out=self.conv2(out)
        out=torch.nn.MaxPool2d((8,1))(out)
        #print(out.shape)
        #out=self.conv3(out)

        # out=self.conv2(out)
        # out=self.conv3(out)
        # out=self.conv4(out)
        # out=self.conv5(out)
        #print(a1.shape)

        out = self.flat(out)
      
        out=self.l0(out)
        out=self.l1(out)
        out=self.l2(out)
        out=self.l3(out)
        out=self.l4(out)
        out=self.l5(out)
        out=self.l6(out)
        out=self.l7(out)

        out=torch.squeeze(out,1)
        return out

model3=simpleLSTM(length=4096,input_size=20,hidden_size=5).to(device)  # len of sample at 1 timeseries
s=summary(model3, (1,4096, 20))  # sample, timeseries, len of one timeseries
print(s)

model=model3
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0001)
train_epoch(model=model,start_index=0,end_index=4096,device=device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer,epochs=50,patience_iterations=50,previous_loss_index=0,index_range=1)

