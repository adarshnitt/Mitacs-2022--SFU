print("stage: Pytorch model formation done! ")

loss_fn=torch.nn.L1Loss()
#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.manual_seed(41)
criterion = nn.L1Loss()

#dataset = ConcatDataset([train_dataloader, test_dataloader])
num_epochs = 10
batch_size = 32
k = 10
# splits = KFold(n_splits = k, shuffle = True, random_state = 41)
# foldperf={}
def show_metrics(y_test, y_predict):
    from sklearn.metrics import mean_absolute_error as mae
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()
    pass


def train_epoch(model,start_index,end_index,device,dataloader,loss_fn,optimizer,epochs=1,patience_iterations=4,previous_loss_index=0,index_range=3):
    """
    model: rnn model u r going to use 
    start index: index to get piece of data/slicing start
    end index: slicinf=g index end
    device : device for pu
    dataloader: training dataloader
    lossfn: loss fn
    epochs: no of epochs for each sample
    patience_iterations: wait till no of iterations when train loss is more than val loss; part for rarly stopping, after patience iterations exceed training will stop
    previous_loss_index: index where previois  val loss is less than training loss
    index_range: range  of index where once tarining greater is less than val loss and again tarining loss is less than validation loss and now currently now training loss is greater than validation loss
    """
    train_loss=[]
    val0dation_loss=[]
    model.train().to(device)
    counter_loss=0
    count=0 ## no of times val loss is more than training loss
    previous_loss_index=previous_loss_index
    patience_iterations=patience_iterations
    index_range=index_range # difference between previous index came when val loss is than trainingb loss

    sample_dcount=0
    mylist=[]
    should_break=0
    for epoch in range(epochs):
        
        for i,(eeg_scan, age) in enumerate(dataloader,0): 
            
            sample_dcount+=1
            #model.train().to(device)
            eeg_scan, age0 = eeg_scan.to(device), age.to(device)
            # make 0 gradient initially for optimization
            optimizer.zero_grad()  # gradient : slope
            age=(age0-torch.mean(age0))/(torch.var(age0)).to(device)
            age=age0
            output = model(eeg_scan[:,start_index:end_index,:])
            loss = loss_fn(output,age)
            loss.backward()  #  calculating gradients ,update weights to input 
            optimizer.step()  # move in direction of gradient to get minima

            counter_loss+=loss.item()  
            #val_loss=valid_epoch(model,start_index,end_index)
          
            copy=counter_loss
            n=20
            ii=i
        
        val_loss=valid_epoch(model,start_index,end_index)
        print("epoch: ",epoch," traing loss after 1 batch is:  ",counter_loss/(ii+1),"Validation loss after 20*32 batch is:  ",val_loss)
                #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
        train_loss.append(counter_loss/(ii+1))
        val0dation_loss.append(val_loss)
        x=counter_loss
        counter_loss=0
                #sample_dcount=0
            
          #   if i%n==0 and i!=0:
          #       print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1),"Validation loss after 20*32 batch is:  ",val_loss)
          #       #print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/(n+1))
          #       train_loss.append(counter_loss/(n+1))
          #       val0dation_loss.append(val_loss)
          #       x=counter_loss
          #       counter_loss=0
          #       sample_dcount=0
          # # try:
          #   np.mean(val0dation_loss[index_range:])>np.mean(train_loss[index_range:])
          #   should_break+=1
          # except:
          #   pass
        # if should_break>=patience_iterations:
        #   break

        # #continue
        if  val_loss>counter_loss/(ii+1):
          if len(mylist)>patience_iterations:
            break
          else:
            if len(mylist)>0:
              if (epoch-mylist[-1])>index_range:
                mylist=[epoch]
              else:
                mylist.append(epoch)
            else:
              mylist=[epoch]

    plt.plot(val0dation_loss)
    plt.plot(train_loss)
    plt.legend(["val_loss","train-loss"])
    pass

@torch.no_grad()
def valid_epoch(model,start_index,end_index):
    valid_loss= 0.0
    #model.eval()
    count=0
    for i, (x,y) in enumerate(validation_loader):
        eeg_scan, age = x.to(device), y.to(device)
        output = model(eeg_scan[:,start_index:end_index,:])
        loss = loss_fn(output,age)
        valid_loss += loss.item()
        count=i+1
    return valid_loss/count


@torch.no_grad()
def predict( model,start_index=0,end_index=-1):
    ypred=[]
    ytest=[]
    model.eval()  
    for x,y in test_loader:
        yhat = model(x)
        # retrieve numpy array
        yhat = yhat.detach().cpu().numpy()
        #check=[i[0] for i in yhat[0]]
        ypred.extend(yhat)
        #print(ypred.shape)
        ytest.extend(y.cpu().numpy())
   
    # out=[]
    # for i in ypred[0]:
    #     out.append(i.tolist()[0])
    # show_metrics(np.array(ytest),np.array(out))
    return ytest,ypred

import torch, gc
gc.collect()
torch.cuda.empty_cache()