# model input n,2048,20
#cnn-1d model channel wise
class LSTM(nn.Module):
     # only lstm network
    def __init__(self, input_size, hidden_size, num_layers=1):
        super(LSTM, self).__init__()
      
        self.num_layers = num_layers
        self.input_size = input_size #  input at time-0 sequence lenth=20 units
        self.hidden_size = hidden_size

        self.conv1=nn.Conv1d(1,1,1)
        self.conv2=nn.Conv1d(1,1,1)
        self.conv3=nn.Conv1d(2,2,1)
        self.conv4=nn.Conv1d(500,500,3)
        self.conv1=nn.Conv1d(1,1,1)

        self.flat=torch.nn.Flatten()
        self.dropout=torch.nn.Dropout(0.3)
        self.relu=torch.nn.ReLU()
        self.batchNorm=torch.nn.BatchNorm1d(2048).to(device)
        
        #self.grpNorm=nn.GroupNorm(1, 2*20)
        
        self.l1=nn.Linear(2*2048, 1)
        self.l2=nn.Linear(400,1)
        self.l3=nn.Linear(2000,1)
        self.l4=nn.Linear(1000,500)
        self.l5=nn.Linear(500,10)
        self.l6=nn.Linear(10,1)
  
       
    def forward(self, x):
        
        xx1=torch.unsqueeze(x[:,1,:],1)
        xx2=torch.unsqueeze(x[:,2,:],1)
        


        #out=self.batchNorm(a)
        out1=self.conv1(xx1)
        # #out=torch.nn.MaxPool1d(2)(out)
        # #out=torch.nn.AvgPool1d(2)(out)

        out2=self.conv2(xx2)
        new = torch.cat((out1,out2),1)
        #print(new.shape)
        # #print(new.shape)
        # out=self.conv3(new)
        # out=self.relu(out)
        # #out=torch.nn.MaxPool1d(2)(out) 

        # #out=self.conv3(out)
        # #out=torch.nn.MaxPool1d(2)(out)
        
        out=nn.Flatten()(new)
        #print(out.shape,"flatten")
        #out=self.grpNorm(out)  ##out=nn.GroupNorm(1, 516)(out)  # put 516 channels in  1 groupand normalize it as one piece
        
        out=self.l1(out)

        #out=self.l2(out)
        #out=self.relu(out)
        #out=self.l3(out)
        #out=self.l4(out)
        #out=self.relu(out)
        #out=self.l5(out)
        #out=self.l6(out)
        return out

model=LSTM(2048,2048).to(device)  # len of sample at 1 timeseries
summary(model, (1,20, 2048))  # sample, timeseries, len of one timeseries





print("stage: Pytorch model formation done! ")

loss_fn=torch.nn.L1Loss()
#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.manual_seed(41)
criterion = nn.L1Loss()

#dataset = ConcatDataset([train_dataloader, test_dataloader])
num_epochs = 10
batch_size = 32
k = 10
# splits = KFold(n_splits = k, shuffle = True, random_state = 41)
# foldperf={}
def show_metrics(y_test, y_predict):
    from sklearn.metrics import mean_absolute_error as mae
    print('MAE:', round(mae(y_test, y_predict), 3))
    print('Correlation:', round(np.corrcoef(y_test, y_predict)[0,1], 3))
    print('Explained variance:', round(explained_variance_score(y_test, y_predict), 3))
    
    plt.rcParams["figure.figsize"] = (8, 8)
    plt.scatter(y_test, y_predict, color='k', alpha=0.5)
    plt.xlim([0, 100])
    plt.ylim([0, 100])
    plt.xlabel('Actual age', fontsize=14)
    plt.ylabel('Predicted age', fontsize=14)

    a, b = np.polyfit(y_test, y_predict, 1)
    plt.plot(y_test, a*y_test + b, 'r')

    plt.show()
    pass


def train_epoch(model,start_index,end_index,device,dataloader,loss_fn,optimizer,epochs=1):
    train_loss=[]
    val0dation_loss=[]
    model.train().to(device)
    for epoch in range(epochs):
        counter_loss=0
        for i,(eeg_scan, age) in enumerate(dataloader,0): # start from zero
            model.train().to(device)
            eeg_scan, age0 = eeg_scan.to(device), age.to(device)
            # make 0 gradient initially for optimization
            optimizer.zero_grad()  # gradient : slope
            #print(eeg_scan.shape)
            age=(age0-torch.mean(age0))/(torch.var(age0)).to(device)
            age=age0
            output = model(eeg_scan[:,start_index:end_index,:])
            loss = loss_fn(output,age)
            loss.backward()  #  calculating gradients ,update weights to input 
            optimizer.step()  # move in direction of gradient to get minima
            counter_loss+=loss.item()
            if i%10==0:
                val_loss=valid_epoch(start_index,end_index)
                print("epoch: ",epoch," Sample_index: ",i," traing loss after 20*32 batch is:  ",counter_loss/10,"Validation loss after 20*32 batch is:  ",val_loss)
                train_loss.append(counter_loss/10)
                val0dation_loss.append(val_loss)
                counter_loss=0
            #break
    print(loss.item())
    plt.plot(val0dation_loss)
    plt.plot(train_loss)
    plt.legend(["val-loss","train-loss"])
    pass

@torch.no_grad()
def valid_epoch(start_index,end_index):
    valid_loss= 0.0
    model.eval()
    count=0
    for i, (x,y) in enumerate(validation_loader):
        eeg_scan, age = x.to(device), y.to(device)
        output = model(eeg_scan[:,start_index:end_index,:])
        loss = loss_fn(output,age)
        valid_loss += loss.item()
        #break
        count=i+1
    return valid_loss/count


@torch.no_grad()
def predict( model,start_index,end_index):
    ypred=[]
    ytest=[]
    model.eval()  
    for x,y in test_loader:
        yhat = model(x[:,start_index:end_index,:] )
        # retrieve numpy array
        yhat = yhat.detach().cpu().numpy()
     #   check=[i[0] for i in yhat[0]]
        ypred.extend(yhat)
        #print(ypred.shape)
        ytest.extend(y.cpu().numpy())
        #break
   
    # out=[]
    # for i in ypred[0]:
    #     out.append(i.tolist()[0])
    # show_metrics(np.array(ytest),np.array(out))
    return ytest,ypred
    #pass

train_epoch(model,0, 2048,device,dataloader=train_loader,loss_fn=loss_fn,optimizer=optimizer)